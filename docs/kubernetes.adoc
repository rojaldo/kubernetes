= Temario: Curso de Kubernetes
:toc: left
:icons: font
:source-highlighter: highlight.js
:toclevels: 3
:diagram-plantuml-format: png

== Módulo 1: Introducción a Kubernetes

=== Fundamentos de orquestación de contenedores
La orquestación de contenedores es el proceso de automatizar la implementación, gestión, escalado y networking de contenedores. Permite administrar múltiples contenedores distribuidos en diferentes hosts, facilitando la alta disponibilidad, el balanceo de carga y la recuperación ante fallos.

=== Historia y evolución de Kubernetes
Kubernetes fue desarrollado originalmente por Google, basado en su experiencia con Borg y Omega, y donado a la Cloud Native Computing Foundation (CNCF) en 2015. Desde entonces, se ha convertido en el estándar de facto para la orquestación de contenedores, evolucionando rápidamente gracias a una comunidad activa y un ecosistema robusto.

=== Arquitectura y componentes principales
Kubernetes se compone de un plano de control (control plane) y nodos de trabajo (worker nodes). 
.Los componentes principales incluyen:
- *API Server*: punto de entrada para todas las operaciones.
- *etcd*: almacén de datos clave-valor distribuido.
- *Controller Manager*: gestiona los controladores que regulan el estado del clúster.
- *Scheduler*: asigna pods a los nodos disponibles.
- *Kubelet*: agente que corre en cada nodo y gestiona los pods.
- *Kube Proxy*: gestiona la red y el acceso a los servicios.

.Diagrama de arquitectura de Kubernetes con todas sus entidades
[plantuml , diagram-plantuml]
----
@startuml
package "Control Plane" {
  [API Server] as api
  [etcd] as etcd
  [Controller Manager] as cm
  [Scheduler] as sch
}
package "Worker Nodes" {
  [Kubelet] as kubelet
  [Kube Proxy] as proxy
  [Pods] as pods
}
api -down-> etcd
api -down-> cm
api -down-> sch
cm -down-> kubelet
sch -down-> kubelet
kubelet -down-> pods
proxy -down-> pods
@enduml
----

=== Ventajas y casos de uso
.Kubernetes ofrece:
- Escalabilidad automática de aplicaciones.
- Alta disponibilidad y tolerancia a fallos.
- Despliegues y actualizaciones sin tiempo de inactividad.
- Gestión eficiente de recursos.
Casos de uso comunes incluyen microservicios, aplicaciones cloud-native, procesamiento de datos y entornos de desarrollo y pruebas.

=== Diferencias con Docker Swarm y otras plataformas
- *Kubernetes* es más complejo pero ofrece mayor flexibilidad, escalabilidad y un ecosistema más amplio.
- *Docker Swarm* es más sencillo de configurar, pero menos potente y con menor adopción en producción.
- Otras plataformas como *Mesos* o *Nomad* tienen enfoques distintos, pero Kubernetes se ha consolidado como el estándar por su comunidad, soporte y características avanzadas.

== Módulo 2: Configuración del Entorno

=== Instalación de Kubernetes (local y cloud)
Para entornos locales, puedes instalar Kubernetes usando herramientas como Minikube, Kind o K3s. En la nube, los principales proveedores ofrecen servicios gestionados como Google Kubernetes Engine (GKE), Amazon EKS y Azure AKS. La instalación local es ideal para pruebas y desarrollo, mientras que la instalación en la nube es recomendada para producción.

=== Minikube, Kind y K3s para desarrollo
- *Minikube*: Permite crear un clúster de Kubernetes en una máquina local usando una máquina virtual o contenedor.
- *Kind (Kubernetes IN Docker)*: Ejecuta clústeres de Kubernetes usando contenedores Docker, ideal para pruebas rápidas y CI.
- *K3s*: Una distribución ligera de Kubernetes, fácil de instalar y optimizada para IoT y edge computing.

=== Configuración de kubectl
`kubectl` es la herramienta de línea de comandos para interactuar con Kubernetes. Para configurarla:

.Instala `kubectl` en tu máquina local:
[source,bash]
----
# Instala kubectl
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
# Verifica la instalación
kubectl version --client
# Configura el acceso al clúster
kubectl config set-cluster <nombre-del-cluster> --server=<url-del-api-server>
kubectl config set-credentials <nombre-del-usuario> --token
kubectl config set-context <nombre-del-contexto> --cluster=<nombre-del-cluster> --user=<nombre-del-usuario>
kubectl config use-context <nombre-del-contexto>
----

.Verifica la conexión ejecutando:
[source,bash]
----
kubectl cluster-info
----

=== Primeros comandos básicos
Algunos comandos esenciales para comenzar:
.Ver nodos del clúster:
[source,bash]
----
kubectl get nodes
----
.Ver pods en todos los namespaces:
[source,bash]
----
kubectl get pods --all-namespaces
----
.Crear un pod de ejemplo:
[source,bash]
----
kubectl run nginx --image=nginx
----
.Eliminar un pod:
[source,bash]
----
kubectl delete pod nginx
----

=== Explorando el dashboard de Kubernetes
El Dashboard es una interfaz web para gestionar y visualizar recursos del clúster.
.Instala el dashboard:
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
----
.Accede al dashboard ejecutando:
[source,bash]
----
kubectl proxy
----
.Abre en tu navegador: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
.Autentícate usando un token de acceso generado para tu usuario o cuenta de servicio.

== Módulo 3: Objetos Básicos de Kubernetes

=== Pods: unidad básica de despliegue
Un *Pod* es la unidad más pequeña de despliegue en Kubernetes. Un pod puede contener uno o varios contenedores que comparten red, almacenamiento y especificaciones de configuración. Los pods se utilizan para ejecutar aplicaciones o servicios y son efímeros: si fallan, Kubernetes puede recrearlos automáticamente.

==== Ciclo de vida de un pod
Un pod puede ser creado, programado en un nodo, ejecutado y finalmente terminado o eliminado. Kubernetes gestiona automáticamente la recreación de pods en caso de fallos, según las políticas definidas en los controladores como Deployments o ReplicaSets.

==== Estados de un pod
.Los principales estados de un pod son:
- *Pending*: El pod ha sido aceptado por el clúster, pero uno o más de sus contenedores aún no han sido creados.
- *Running*: Todos los contenedores del pod han sido creados y al menos uno está en ejecución.
- *Succeeded*: Todos los contenedores han terminado correctamente y no serán reiniciados.
- *Failed*: Todos los contenedores han terminado, pero al menos uno falló.
- *Unknown*: El estado del pod no pudo ser determinado, generalmente por problemas de comunicación con el nodo.

==== Gestión de pods
La gestión de pods se realiza principalmente mediante controladores como Deployments, ReplicaSets o Jobs, que permiten definir el estado deseado y delegar en Kubernetes la creación, actualización y eliminación de pods. Los pods individuales pueden ser gestionados con `kubectl`:

.Crear un pod:
[source,bash]
----
kubectl run nginx --image=nginx
----

.Listar pods:
[source,bash]
----
kubectl get pods
----

.Eliminar un pod:
[source,bash]
----
kubectl delete pod <nombre-del-pod>
----

.Ver detalles de un pod:
[source,bash]
----
kubectl describe pod <nombre-del-pod>
----

.Ver logs de un pod:
[source,bash]
----
kubectl logs <nombre-del-pod>
----

.Ejecutar un comando en un pod:
[source,bash]
----
kubectl exec -it <nombre-del-pod> -- /bin/bash
----

.Escalar un pod:
[source,bash]
----
kubectl scale --replicas=3 deployment/<nombre-del-deployment>
----

.Actualizar un pod:
[source,bash]
----
kubectl set image deployment/<nombre-del-deployment> <nombre-del-contenedor>=<nueva-imagen>
----

.Reiniciar un pod:
[source,bash]
----
kubectl rollout restart deployment/<nombre-del-deployment>
----

.Ver el historial de cambios de un pod:
[source,bash]
----
kubectl rollout history deployment/<nombre-del-deployment>
----

.Deshacer un cambio en un pod:
[source,bash]
----
kubectl rollout undo deployment/<nombre-del-deployment>
----

.Ver el estado de un pod:
[source,bash]
----
kubectl get pod <nombre-del-pod> -o wide
----

.Ver eventos relacionados con un pod:
[source,bash]
----
kubectl get events --field-selector involvedObject.name=<nombre-del-pod>
----

.Ver la configuración de un pod:
[source,bash]
----
kubectl get pod <nombre-del-pod> -o yaml
----


==== Labels y Annotations
*Labels* son pares clave-valor que se asignan a los objetos de Kubernetes para identificarlos, agruparlos y seleccionarlos lógicamente. Se utilizan para operaciones como filtrado, organización, despliegue selectivo y políticas de red. Ejemplo de uso de labels en un manifiesto:

.Un ejemplo de label en un pod:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
  labels:
    app: web
    entorno: produccion
----

*Annotations* permiten adjuntar metadatos adicionales a los objetos, como información de despliegue, enlaces, descripciones o configuraciones externas. A diferencia de los labels, no se usan para selección o agrupación, sino para almacenar datos auxiliares. Ejemplo de uso de annotations:

.Un ejemplo de annotation en un pod:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
  annotations:
    descripcion: "Pod de ejemplo para mostrar annotations"
    contacto: "devops@empresa.com"
----

Las labels y annotations son fundamentales para la gestión avanzada de recursos, automatización y observabilidad en Kubernetes.

=== ReplicaSets y controladores de replicación
Un *ReplicaSet* es un objeto de Kubernetes que asegura que un número específico de réplicas de un pod estén corriendo en todo momento. Si un pod falla o es eliminado, el ReplicaSet crea uno nuevo para mantener el estado deseado. Los ReplicaSets reemplazaron a los controladores de replicación tradicionales, ofreciendo mayor flexibilidad y soporte para selectores avanzados.

Los controladores de replicación fueron el primer mecanismo para garantizar la disponibilidad de pods, pero han sido reemplazados por ReplicaSets debido a sus limitaciones en la selección de pods. Actualmente, los ReplicaSets suelen ser gestionados indirectamente a través de Deployments.

.Un ReplicaSet consta de estos apartados:
. `apiVersion`: Versión de la API de Kubernetes utilizada.
. `kind`: Tipo de recurso, en este caso `ReplicaSet`.
. `metadata`: Información descriptiva del objeto (nombre, etiquetas, anotaciones).
. `spec`: Especificaciones del ReplicaSet, que incluye:
.. `replicas`: Número de réplicas deseadas.
.. `selector`: Selector de etiquetas para identificar los pods gestionados.
.. `template`: Plantilla de los pods a crear, que contiene:
... `metadata`: Etiquetas y metadatos de los pods.
... `spec`: Especificación de los contenedores y configuración de los pods.

.Ejemplo básico de ReplicaSet:
[source,yaml]
----
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: ejemplo-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      containers:
      - name: nginx
        image: nginx:latest
----

Los ReplicaSets son útiles para mantener la alta disponibilidad y escalabilidad de las aplicaciones, pero en la práctica se gestionan casi siempre mediante Deployments, que facilitan actualizaciones y rollbacks.

==== Ciclo de vida de un ReplicaSet
El ciclo de vida de un ReplicaSet comienza con su creación a través de un manifiesto YAML o mediante un Deployment. Una vez creado, el ReplicaSet observa continuamente el estado de los pods asociados y asegura que el número de réplicas especificado se mantenga. Si un pod falla o es eliminado, el ReplicaSet crea uno nuevo automáticamente. Si se actualiza el manifiesto (por ejemplo, cambiando el número de réplicas), el ReplicaSet ajusta la cantidad de pods en consecuencia. El ciclo termina cuando el ReplicaSet es eliminado, lo que provoca la eliminación de todos los pods gestionados por él, a menos que tengan un propietario adicional como un Deployment.

==== Estados de un ReplicaSet
Los ReplicaSets no tienen estados tan detallados como los pods, pero su estado se puede observar a través de los siguientes indicadores:

- *Current Replicas*: Número actual de pods gestionados por el ReplicaSet.
- *Desired Replicas*: Número de réplicas especificadas en la configuración.
- *Available Replicas*: Número de pods disponibles y listos para recibir tráfico.
- *Ready Replicas*: Número de pods que han pasado las comprobaciones de estado y están listos para su uso.

Un ReplicaSet se considera saludable cuando el número de pods actuales y disponibles coincide con el número deseado. Puedes consultar el estado de un ReplicaSet con:

[source,bash]
----
kubectl get replicaset
kubectl describe replicaset <nombre-del-replicaset>
----

==== Gestión de ReplicaSets
La gestión de ReplicaSets se realiza principalmente mediante el uso de manifiestos YAML y la herramienta `kubectl`. Las operaciones más comunes incluyen la creación, actualización, escalado y eliminación de ReplicaSets.

.Crear un ReplicaSet:
[source,bash]
----
kubectl apply -f replicaset.yaml
----
.Listar ReplicaSets:
[source,bash]
----
kubectl get replicasets
----
.Escalar un ReplicaSet (cambiar el número de réplicas):
[source,bash]
----
kubectl scale replicaset <nombre-del-replicaset> --replicas=5
----
.Actualizar un ReplicaSet:
[source,bash]
----
kubectl apply -f replicaset.yaml
----
.Ver detalles de un ReplicaSet:
[source,bash]
----
kubectl describe replicaset <nombre-del-replicaset>
----

.Eliminar un ReplicaSet:
[source,bash]
----
kubectl delete replicaset <nombre-del-replicaset>
----

En la práctica, los ReplicaSets suelen ser gestionados indirectamente a través de Deployments, que facilitan la actualización y el control del ciclo de vida de las aplicaciones.

=== Deployments: gestión de actualizaciones
Un *Deployment* gestiona la creación y actualización de ReplicaSets y, por ende, de pods. Permite realizar despliegues declarativos, actualizaciones progresivas (rolling updates) y retrocesos (rollbacks) en caso de fallos. Es el recurso recomendado para gestionar aplicaciones sin tiempo de inactividad.

==== Ciclo de vida de un Deployment
El ciclo de vida de un Deployment inicia con la creación del recurso mediante un manifiesto YAML o un comando `kubectl`. Al crearse, el Deployment genera un ReplicaSet que, a su vez, crea los pods necesarios según la configuración deseada.

Cuando se actualiza el Deployment (por ejemplo, cambiando la imagen del contenedor o el número de réplicas), Kubernetes realiza una actualización progresiva (rolling update), reemplazando gradualmente los pods antiguos por nuevos para evitar tiempo de inactividad. Si ocurre un error durante la actualización, el Deployment puede realizar un rollback automático o manual a una versión anterior.

El ciclo de vida termina cuando el Deployment es eliminado, lo que provoca la eliminación de los ReplicaSets y pods gestionados por él, a menos que estos tengan otros propietarios.

.Principales etapas:
- Creación del Deployment y su ReplicaSet asociado.
- Escalado automático o manual de réplicas.
- Actualizaciones progresivas y rollbacks.
- Eliminación del Deployment y recursos asociados.

==== Estados de un Deployment
El estado de un Deployment se determina por el estado de sus ReplicaSets y pods gestionados. 

.Los principales indicadores y condiciones que puedes observar son:
- *Available Replicas*: Número de pods disponibles y listos para recibir tráfico.
- *Updated Replicas*: Número de pods actualizados con la última especificación del Deployment.
- *Ready Replicas*: Número de pods que han pasado las comprobaciones de estado y están listos para su uso.
- *Unavailable Replicas*: Número de pods que no están disponibles.
- *Conditions*: El Deployment puede mostrar condiciones como `Progressing` (en proceso de actualización), `Available` (disponible) o `ReplicaFailure` (fallo en la creación de réplicas).

Un Deployment se considera saludable cuando el número de pods actualizados, disponibles y listos coincide con el número deseado de réplicas y no hay condiciones de error.

Puedes consultar el estado de un Deployment con:

[source,bash]
----
kubectl get deployments
kubectl describe deployment <nombre-del-deployment>
----

==== Gestión de Deployments
La gestión de Deployments se realiza principalmente mediante manifiestos YAML y la herramienta `kubectl`. Permite crear, actualizar, escalar, reiniciar, hacer rollback y eliminar aplicaciones de forma declarativa y controlada.

.Crear un Deployment:
[source,bash]
----
kubectl apply -f deployment.yaml
----
.Listar Deployments:
[source,bash]
----
kubectl get deployments
----
.Escalar un Deployment (cambiar el número de réplicas):
[source,bash]
----
kubectl scale deployment <nombre-del-deployment> --replicas=5
----
.Actualizar la imagen de un Deployment:
[source,bash]
----
kubectl set image deployment/<nombre-del-deployment> <nombre-del-contenedor>=<nueva-imagen>
----
.Reiniciar un Deployment:
[source,bash]
----
kubectl rollout restart deployment/<nombre-del-deployment>
----
.Ver el historial de cambios:
[source,bash]
----
kubectl rollout history deployment/<nombre-del-deployment>
----
.Hacer rollback a una versión anterior:
[source,bash]
----
kubectl rollout undo deployment/<nombre-del-deployment>
----
.Eliminar un Deployment:
[source,bash]
----
kubectl delete deployment <nombre-del-deployment>
----

Los Deployments facilitan la gestión del ciclo de vida de las aplicaciones, permitiendo actualizaciones sin tiempo de inactividad y un control detallado sobre el estado de los despliegues.

=== StatefulSets: gestión de aplicaciones con estado

Un *StatefulSet* es un recurso de Kubernetes diseñado para gestionar aplicaciones con estado, es decir, aquellas que requieren identidad persistente, almacenamiento estable y un orden específico en el despliegue y actualización de los pods. A diferencia de los Deployments o ReplicaSets, que están pensados para aplicaciones sin estado (stateless), los StatefulSets proporcionan garantías adicionales necesarias para bases de datos, sistemas distribuidos y servicios que dependen de la persistencia de datos y la identidad de red.

==== Características principales de los StatefulSets

- **Identidad estable**: Cada pod gestionado por un StatefulSet tiene un nombre único y predecible (por ejemplo, `miapp-0`, `miapp-1`, etc.), que se mantiene incluso si el pod es eliminado y recreado.
- **Almacenamiento persistente**: Los StatefulSets pueden asociar un PersistentVolumeClaim (PVC) único a cada pod, asegurando que los datos persisten aunque el pod se reinicie o se reprograme en otro nodo.
- **Orden de despliegue y actualización**: Los pods se crean, actualizan y eliminan en un orden controlado (de menor a mayor índice), lo que es esencial para aplicaciones que requieren inicialización secuencial o dependencias entre instancias.
- **Headless Service**: Para exponer los pods de un StatefulSet y permitir la comunicación directa entre ellos, se utiliza un Service de tipo headless (`clusterIP: None`), que asigna un DNS único a cada pod.

==== Casos de uso típicos

- Bases de datos distribuidas (MongoDB, Cassandra, MySQL Galera, etc.)
- Sistemas de mensajería (Kafka, RabbitMQ)
- Aplicaciones que requieren almacenamiento persistente y orden en el despliegue

==== Ejemplo de manifiesto YAML para StatefulSet

A continuación se muestra un ejemplo básico de StatefulSet para una base de datos MongoDB con 3 réplicas y almacenamiento persistente:

.Definición del servicio headless y StatefulSet:
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mongo
spec:
  clusterIP: None
  selector:
    app: mongo
  ports:
    - port: 27017
      name: mongo

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: mongo
  replicas: 3
  selector:
    matchLabels:
      app: mongo
  template:
    metadata:
      labels:
        app: mongo
    spec:
      containers:
        - name: mongo
          image: mongo:5.0
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: datos
              mountPath: /data/db
  volumeClaimTemplates:
    - metadata:
        name: datos
      spec:
        accessModes: [ "ReadWriteOnce" ]
        resources:
          requests:
            storage: 5Gi
----

.Para ejecutar este manifiesto, guarda el contenido en un archivo llamado `mongo-statefulset.yaml` y ejecuta:
[source,bash]
----
kubectl apply -f mongo-statefulset.yaml
----

.En este ejemplo:
- Se crea un Service headless llamado `mongo` para exponer los pods.
- El StatefulSet crea 3 pods (`mongo-0`, `mongo-1`, `mongo-2`), cada uno con su propio volumen persistente.
- Si un pod se elimina, Kubernetes lo recrea con el mismo nombre y volumen.

==== Comandos útiles para gestionar StatefulSets

.Crear el StatefulSet:
[source,bash]
----
kubectl apply -f mongo-statefulset.yaml
----

.Listar los pods creados:
[source,bash]
----
kubectl get pods -l app=mongo
----

.Ver los volúmenes persistentes asociados:
[source,bash]
----
kubectl get pvc -l app=mongo
----

.Escalar el StatefulSet:
[source,bash]
----
kubectl scale statefulset mongo --replicas=5
----

.Eliminar el StatefulSet (los PVC pueden permanecer según la política de retención):
[source,bash]
----
kubectl delete statefulset mongo
----

==== Buenas prácticas con StatefulSets

- Usa StatefulSets solo cuando la aplicación requiera identidad persistente, almacenamiento estable y orden en el despliegue.
- Define un Service headless para garantizar la resolución DNS individual de los pods.
- Configura correctamente los PersistentVolumeClaims para asegurar la persistencia de datos.
- Supervisa el estado de los pods y los volúmenes asociados.
- Considera la política de retención de volúmenes (`Retain` o `Delete`) según la criticidad de los datos.

==== Resumen gráfico

[plantuml, statefulset-diagram, png]
----
@startuml
node "Kubernetes Cluster" {
  [mongo-0] --> [PVC datos-mongo-0]
  [mongo-1] --> [PVC datos-mongo-1]
  [mongo-2] --> [PVC datos-mongo-2]
}
cloud "Headless Service (mongo)"
[Usuario] --> "Headless Service (mongo)" : DNS mongo-0.mongo, mongo-1.mongo, ...
@enduml
----

=== DaemonSets: ejecución de pods en todos los nodos

Un *DaemonSet* es un recurso de Kubernetes que garantiza que una copia de un pod específico se ejecute en cada nodo del clúster (o en un subconjunto de nodos, según la configuración). Es ideal para desplegar agentes de monitorización, logging, proxies, o cualquier servicio que deba estar presente en todos los nodos.

==== Características principales de los DaemonSets

- **Despliegue automático en todos los nodos**: Cada vez que se añade un nodo al clúster, el DaemonSet programa automáticamente un pod en ese nodo.
- **Gestión de nodos selectivos**: Puedes limitar la ejecución a ciertos nodos usando `nodeSelector`, `affinity` o `tolerations`.
- **Actualización controlada**: Permite actualizar los pods de forma progresiva usando la estrategia `RollingUpdate`.
- **Eliminación automática**: Cuando un nodo es eliminado del clúster, el pod correspondiente también se elimina.

==== Casos de uso típicos

- Agentes de monitorización (Prometheus Node Exporter, Datadog Agent, etc.).
- Recolectores de logs (Fluentd, Filebeat).
- Proxies de red o almacenamiento.
- Herramientas de seguridad y escaneo de nodos.

==== Ejemplo de manifiesto YAML para DaemonSet

A continuación, un ejemplo básico de DaemonSet que despliega Prometheus Node Exporter en todos los nodos:

[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  labels:
    app: node-exporter
spec:
  selector:
    matchLabels:
      app: node-exporter
  template:
    metadata:
      labels:
        app: node-exporter
    spec:
      containers:
        - name: node-exporter
          image: prom/node-exporter:v1.7.0
          ports:
            - containerPort: 9100
          resources:
            limits:
              memory: "128Mi"
              cpu: "100m" 
          volumeMounts:
            - name: proc
              mountPath: /host/proc
              readOnly: true
            - name: sys
              mountPath: /host/sys
              readOnly: true
      volumes:
        - name: proc
          hostPath:
            path: /proc
        - name: sys
          hostPath:
            path: /sys
----

==== Comandos útiles para gestionar DaemonSets

.Crear el DaemonSet:
[source,bash]
----
kubectl apply -f node-exporter-daemonset.yaml
----

.Listar los pods creados por el DaemonSet:
[source,bash]
----
kubectl get pods -l app=node-exporter -o wide
----

.Ver en qué nodos está corriendo cada pod:
[source,bash]
----
kubectl get pods -o wide
----

.Actualizar el DaemonSet (por ejemplo, nueva imagen):
[source,bash]
----
kubectl set image daemonset/node-exporter node-exporter=prom/node-exporter:v1.8.0
----

.Eliminar el DaemonSet:
[source,bash]
----
kubectl delete daemonset node-exporter
----

==== Buenas prácticas con DaemonSets

* Usa `nodeSelector`, `affinity` o `tolerations` para limitar la ejecución a nodos específicos si no necesitas que el DaemonSet se ejecute en todos los nodos.
** `nodeSelector` permite seleccionar nodos basados en etiquetas.
** `affinity` ofrece una selección más avanzada, permitiendo definir reglas complejas.
** `tolerations` permite que los pods se ejecuten en nodos con taints específicos.
* Configura `hostPath` con precaución, ya que puede afectar la seguridad y la portabilidad de los pods.
* Define recursos (`requests` y `limits`) para evitar sobrecargar los nodos.
** `requests` especifica la cantidad mínima de recursos que el pod necesita.
** `limits` define la cantidad máxima de recursos que el pod puede consumir.
* Supervisa el estado de los pods del DaemonSet para detectar fallos o nodos no cubiertos.
* Utiliza la estrategia `RollingUpdate` para actualizaciones seguras y progresivas.
* Documenta el propósito de cada DaemonSet y los recursos que gestiona.

==== Resumen gráfico

[plantuml, daemonset-diagram, png]
----
@startuml
node "Nodo 1" {
  [Pod DaemonSet]
}
node "Nodo 2" {
  [Pod DaemonSet]
}
node "Nodo 3" {
  [Pod DaemonSet]
}
cloud "Kubernetes Cluster"
cloud "DaemonSet Controller"
"DaemonSet Controller" --> [Pod DaemonSet] : Programa pods en cada nodo
@enduml
----

=== Jobs y CronJobs: ejecución de tareas puntuales y programadas

Un *Job* en Kubernetes es un recurso que permite ejecutar tareas puntuales o por lotes, garantizando que una o varias ejecuciones finalicen correctamente. Un *CronJob* extiende este concepto permitiendo programar la ejecución periódica de Jobs según una expresión cron, similar a los cron jobs de sistemas Unix.

==== Características principales de los Jobs

- **Ejecución garantizada**: Un Job asegura que una tarea se ejecute hasta completarse con éxito, recreando pods si es necesario.
- **Control de concurrencia**: Puedes definir cuántos pods se ejecutan en paralelo (`parallelism`) y cuántas ejecuciones deben completarse (`completions`).
- **Reintentos automáticos**: Si un pod falla, el Job puede reintentarlo hasta alcanzar el límite definido por `backoffLimit`.
- **Indexed Jobs**: Permiten asignar un índice único a cada pod, útil para tareas distribuidas o procesamiento de lotes.

==== Ejemplo básico de Job

A continuación, un manifiesto YAML de un Job que imprime "Hola Kubernetes" y termina:

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: hola-job
spec:
  template:
    spec:
      containers:
        - name: hola
          image: busybox
          command: ["echo", "Hola Kubernetes"]
      restartPolicy: Never
----

.Crear el Job:
[source,bash]
----
kubectl apply -f hola-job.yaml
----

.Verificar el estado y logs:
[source,bash]
----
kubectl get jobs
kubectl get pods --selector=job-name=hola-job
kubectl logs <nombre-del-pod>
----

==== Ejemplo de Job con múltiples ejecuciones y concurrencia

Puedes definir Jobs que ejecuten varias tareas en paralelo:

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-job
spec:
  completions: 5
  parallelism: 2
  template:
    spec:
      containers:
        - name: worker
          image: busybox
          command: ["sh", "-c", "echo Trabajo $((RANDOM % 100))"]
      restartPolicy: Never
----

- `completions: 5`: El Job se considera exitoso cuando 5 ejecuciones han terminado correctamente.
- `parallelism: 2`: Hasta 2 pods pueden ejecutarse en paralelo.

==== Indexed Jobs

Permiten que cada pod reciba un índice único a través de la variable de entorno `JOB_COMPLETION_INDEX`, útil para procesamiento distribuido:

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: indexed-job
spec:
  completions: 3
  parallelism: 3
  completionMode: Indexed
  template:
    spec:
      containers:
        - name: worker
          image: busybox
          command: ["sh", "-c", "echo Soy el índice $JOB_COMPLETION_INDEX"]
      restartPolicy: Never
----

==== Características principales de los CronJobs

- **Programación periódica**: Ejecutan Jobs automáticamente según una expresión cron (`schedule`).
- **Control de concurrencia**: Puedes definir si se permiten ejecuciones solapadas (`concurrencyPolicy`).
- **Historial de ejecuciones**: Controla cuántos Jobs exitosos o fallidos se mantienen (`successfulJobsHistoryLimit`, `failedJobsHistoryLimit`).
- **Soporte para zonas horarias**: Desde Kubernetes 1.24, puedes especificar la zona horaria (`timeZone`).

==== Ejemplo básico de CronJob

Un CronJob que imprime la fecha y hora cada minuto:

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: fecha-cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: fecha
              image: busybox
              command: ["date"]
          restartPolicy: OnFailure
----

En el ejemplo anterior:

- `spec`: Especificaciones del CronJob, que incluye:
.. `schedule`: Expresión cron que define la programación.
.. `jobTemplate`: Plantilla del Job que se ejecutará según la programación.

.El objeto schedule define la frecuencia de ejecución:
- `*/1 * * * *`: Cada minuto.
- `0 * * * *`: Cada hora.
- `0 0 * * *`: Cada día a medianoche.
- `0 0 * * 0`: Cada domingo a medianoche.
- `0 0 1 * *`: El primer día de cada mes a medianoche.
- `0 0 1 1 *`: El primer día de enero a medianoche.
- `0 0 * * 1`: Cada lunes a medianoche.
- `0 0 * * 1-5`: Cada lunes a viernes a medianoche.
- `0 0 * * 1-5,6`: Cada lunes a viernes y sábado a medianoche.

La codificación del elemento schedule es la siguiente:
- `*`: Cada unidad de tiempo.
- `*/n`: Cada n unidades de tiempo.
- `n`: Cada n unidades de tiempo.
- `n1,n2`: En los minutos, horas, días del mes, meses o días de la semana.
- `n1-n2`: Desde n1 hasta n2 unidades de tiempo.
- `?`: No especifica un valor.
- `L`: Último día del mes o último día de la semana.
- `W`: Día de la semana más cercano a una fecha específica.
- `#`: Día de la semana específico (por ejemplo, `2#1` significa el primer lunes del mes).
- `C`: Indica la zona horaria (desde Kubernetes 1.24).
- `TZ`: Indica la zona horaria (desde Kubernetes 1.24).
- `timeZone`: Indica la zona horaria (desde Kubernetes 1.24).
- `@yearly`, `@annually`: Cada año.
- `@monthly`: Cada mes.
- `@weekly`: Cada semana.
- `@daily`: Cada día.
- `@hourly`: Cada hora.


.Crear el CronJob:
[source,bash]
----
kubectl apply -f fecha-cronjob.yaml
----

.Verificar ejecuciones:
[source,bash]
----
kubectl get cronjobs
kubectl get jobs
kubectl get pods --selector=job-name=<nombre-del-job>
kubectl logs <nombre-del-pod>
----

==== Ejemplo de CronJob con backup de base de datos

Un CronJob que ejecuta un script de backup cada hora y almacena el resultado en un volumen persistente:

[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-db
spec:
  schedule: "0 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: backup
              image: mysql:8
              command: ["sh", "-c", "mysqldump -h db -u root -p$MYSQL_ROOT_PASSWORD basededatos > /backup/backup.sql"]
              env:
                - name: MYSQL_ROOT_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: db-secret
                      key: password
              volumeMounts:
                - name: backup-vol
                  mountPath: /backup
          restartPolicy: OnFailure
          volumes:
            - name: backup-vol
              persistentVolumeClaim:
                claimName: backup-pvc
----

==== Buenas prácticas con Jobs y CronJobs

* Usa restartPolicy: `Never` o `OnFailure` según el caso.
** `Never`: No reinicia el pod si falla.
** `OnFailure`: Reinicia el pod si falla, útil para tareas que pueden ser reintentadas.
* Controla el número de ejecuciones y la concurrencia para evitar sobrecargar el clúster.
* Elimina Jobs y pods antiguos para liberar recursos, ajustando los límites de historial.
* Supervisa el estado y los logs de los Jobs para detectar fallos o ejecuciones incompletas.
* Usa variables de entorno y Secrets para gestionar credenciales de forma segura.
* Documenta la programación y el propósito de cada CronJob.

==== Resumen gráfico

[plantuml, jobs-cronjobs, png]
----
@startuml
actor "Usuario/Administrador"
cloud "Kubernetes Cluster"
"Usuario/Administrador" --> "Kubernetes Cluster" : Crea Job/CronJob
"Kubernetes Cluster" --> [Job] : Ejecuta pods hasta completarse
"Kubernetes Cluster" --> [CronJob] : Programa Jobs periódicamente
[Job] --> [Pod(s)] : Lanza pods para la tarea
[CronJob] --> [Job] : Crea Jobs según schedule
@enduml
----

=== Volúmenes y almacenamiento persistente

El almacenamiento persistente es fundamental en Kubernetes para garantizar que los datos generados y utilizados por los pods sobrevivan a reinicios, reprogramaciones o eliminaciones de los mismos. A diferencia del almacenamiento efímero (almacenamiento local del pod), el almacenamiento persistente asegura la durabilidad y disponibilidad de los datos más allá del ciclo de vida de los contenedores.

==== Conceptos clave

- *Volumen (Volume)*: Abstracción que permite a los contenedores de un pod acceder a un sistema de archivos compartido. Puede ser efímero (por ejemplo, `emptyDir`) o persistente (por ejemplo, asociado a un PersistentVolume).
- *PersistentVolume (PV)*: Recurso del clúster que representa una porción de almacenamiento físico (local, en red o en la nube) gestionada por el administrador o aprovisionada dinámicamente.
- *PersistentVolumeClaim (PVC)*: Solicitud de almacenamiento realizada por un usuario o aplicación, especificando el tamaño y las características requeridas.
- *StorageClass*: Define las características y el tipo de almacenamiento dinámico que puede ser aprovisionado para los PVCs.

==== Tipos de volúmenes

- `emptyDir`: Directorio temporal que existe mientras el pod está en ejecución.
- `hostPath`: Monta un directorio del nodo anfitrión en el pod.
- `configMap` y `secret`: Permiten montar datos de configuración o secretos como archivos.
- `persistentVolumeClaim`: Permite montar almacenamiento persistente gestionado por PV y PVC.
- Otros: `nfs`, `awsElasticBlockStore`, `gcePersistentDisk`, `azureDisk`, etc.

==== Ejemplo básico de uso de volúmenes

.Montar un volumen `emptyDir` en un pod:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-con-emptydir
spec:
  containers:
    - name: app
      image: busybox
      command: ["sh", "-c", "echo 'Hola Volumen' > /datos/mensaje.txt && sleep 3600"]
      volumeMounts:
        - name: datos
          mountPath: /datos
  volumes:
    - name: datos
      emptyDir: {}
----

.Los datos en `/datos` se eliminan al borrar el pod.

==== Ejemplo de almacenamiento persistente con PV y PVC

.Definir un PersistentVolume:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-ejemplo
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/datos
----

.Crear un PersistentVolumeClaim:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-ejemplo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
----

.Usar el PVC en un pod:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-con-pvc
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - name: datos
          mountPath: /usr/share/nginx/html
  volumes:
    - name: datos
      persistentVolumeClaim:
        claimName: pvc-ejemplo
----

==== Aprovisionamiento dinámico con StorageClass

Las StorageClasses permiten que los PVCs soliciten almacenamiento dinámicamente, sin necesidad de definir PVs manualmente.

.Ejemplo de StorageClass:
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mi-storageclass
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Delete
----

.Ejemplo de PVC usando StorageClass:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-con-storageclass
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: mi-storageclass
----

==== Buenas prácticas

- Utiliza StorageClasses para aprovisionamiento dinámico y flexibilidad.
- Define políticas de retención adecuadas (`Delete` o `Retain`) según la criticidad de los datos.
- Supervisa el uso de almacenamiento y ajusta cuotas o tamaños según el crecimiento de la aplicación.
- Documenta el uso de volúmenes y su relación con los pods y aplicaciones.
- Elimina PVCs y PVs que ya no sean necesarios para liberar recursos.

==== Comandos útiles

.Crear y listar volúmenes:
[source,bash]
----
kubectl apply -f pv-ejemplo.yaml
kubectl apply -f pvc-ejemplo.yaml
kubectl get pv
kubectl get pvc
----

.Ver detalles y estado:
[source,bash]
----
kubectl describe pv pv-ejemplo
kubectl describe pvc pvc-ejemplo
----

.Eliminar recursos:
[source,bash]
----
kubectl delete pod pod-con-pvc
kubectl delete pvc pvc-ejemplo
kubectl delete pv pv-ejemplo
----

==== Resumen gráfico

[plantuml, volumes-pv-pvc, png]
----
@startuml
actor "Usuario/App"
cloud "Kubernetes Cluster"
rectangle "PersistentVolume (PV)" as PV
rectangle "PersistentVolumeClaim (PVC)" as PVC
rectangle "Pod" as P
"Usuario/App" --> PVC : Solicita almacenamiento
PVC --> PV : Se enlaza automáticamente
P --> PVC : Monta el volumen
PV --> "Almacenamiento físico" : Proporciona datos
@enduml
----

=== Services: comunicación entre componentes
Un *Service* expone uno o varios pods bajo una dirección IP y un nombre DNS estable dentro del clúster. Facilita la comunicación interna y externa entre componentes, gestionando el balanceo de carga y el descubrimiento de servicios. Tipos comunes: ClusterIP, NodePort, LoadBalancer e Ingress.

==== Tipos de servicios
.Kubernetes ofrece varios tipos de servicios para exponer aplicaciones y gestionar la comunicación entre pods y con el exterior:

- *ClusterIP*: Es el tipo por defecto. Expone el servicio en una IP interna accesible solo dentro del clúster. Ideal para comunicación interna entre pods.
- *NodePort*: Expone el servicio en un puerto específico de cada nodo del clúster, permitiendo el acceso externo a través de la IP del nodo y el puerto asignado.
- *LoadBalancer*: Proporciona una IP externa mediante un balanceador de carga (usualmente en entornos cloud), permitiendo el acceso desde fuera del clúster.
- *ExternalName*: Asocia el servicio a un nombre DNS externo, redirigiendo el tráfico a recursos fuera del clúster.
- *Headless Service*: Cuando se define con `clusterIP: None`, no asigna una IP virtual y permite el descubrimiento directo de los pods, útil para bases de datos o aplicaciones stateful.

==== Ciclo de vida de un Service
El ciclo de vida de un Service en Kubernetes comienza con su creación mediante un manifiesto YAML o un comando `kubectl`. Una vez creado, el Service asigna una IP virtual y, según el tipo, puede asignar un puerto en los nodos o solicitar un balanceador de carga externo.

El Service monitoriza continuamente los endpoints (pods) que coinciden con su selector de labels, actualizando automáticamente la lista de pods disponibles para recibir tráfico. Si los pods asociados cambian (por ejemplo, por escalado o actualización), el Service ajusta sus endpoints sin necesidad de ser recreado.

El ciclo de vida termina cuando el Service es eliminado, liberando la IP y los recursos asociados. Durante todo su ciclo, el Service garantiza la conectividad y el balanceo de carga entre los pods y, si corresponde, con el exterior del clúster.

==== Características principales de los Services

- *IP virtual*: Cada Service tiene una IP virtual que actúa como punto de acceso para los pods asociados, permitiendo la comunicación sin necesidad de conocer las IPs individuales de los pods.
- *Selector*: Los Services utilizan labels para seleccionar los pods que deben recibir el tráfico. Esto permite agrupar y gestionar dinámicamente los pods asociados.
- *Endpoints*: Los Services mantienen una lista de endpoints (pods) que coinciden con su selector, actualizándola automáticamente a medida que los pods cambian.
- *ClusterIP/External IP*: Indica si el Service tiene asignada una IP interna o externa.
- *Ports*: Puertos expuestos por el Service y su correspondencia con los pods.
- *Session Affinity*: Permite mantener la sesión del cliente en el mismo pod durante un tiempo determinado, útil para aplicaciones que requieren persistencia de sesión.
- *Health Checks*: Comprobaciones de estado que determinan si los pods asociados están disponibles y listos para recibir tráfico.
- *Load Balancing*: Distribución del tráfico entre los pods asociados al Service, garantizando alta disponibilidad y rendimiento.
- *DNS interno*: Kubernetes asigna un nombre DNS al Service, permitiendo que los pods se comuniquen usando ese nombre en lugar de la IP.
- *Annotations*: Metadatos adicionales que pueden ser utilizados por controladores de servicios o herramientas externas para configurar características avanzadas.
- *Labels*: Etiquetas que permiten identificar y seleccionar los pods asociados al Service.
- *Type*: Tipo de Service (ClusterIP, NodePort, LoadBalancer, etc.) que determina cómo se expone el servicio y cómo se accede a él.

.Un ejemplo de un Service con gran parte de estas características sería:
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio
  labels:
    app: mi-app
    entorno: produccion
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
spec:
  type: LoadBalancer
  selector:
    app: mi-app
    version: v1
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 8080
    - name: https
      protocol: TCP
      port: 443
      targetPort: 8443
  sessionAffinity: ClientIP
  externalTrafficPolicy: Local
----

.En este ejemplo:
- El Service se llama `mi-servicio` y expone dos puertos (HTTP y HTTPS).
- Usa el tipo `LoadBalancer` para exponer la aplicación al exterior (en cloud o con MetalLB).
- Selecciona pods con las etiquetas `app: mi-app` y `version: v1`.
- Incluye labels y annotations útiles para monitorización y organización.
- Habilita afinidad de sesión (`sessionAffinity: ClientIP`) para mantener la sesión del cliente en el mismo pod.
- Usa `externalTrafficPolicy: Local` para preservar la IP de origen del cliente.

.Para lanzar el Service, guarda el contenido en un archivo llamado `mi-servicio.yaml` y ejecuta:
[source,bash]
----
kubectl apply -f mi-servicio.yaml
----

.Puedes consultar y probar el Service con:
[source,bash]
----
kubectl get service mi-servicio
kubectl describe service mi-servicio
----

==== Gestión de Services
La gestión de Services en Kubernetes se realiza principalmente mediante manifiestos YAML y la herramienta `kubectl`. Permite crear, listar, actualizar, eliminar y exponer servicios de manera flexible para facilitar la comunicación entre pods y con el exterior.

.Crear un Service:
[source,bash]
----
kubectl apply -f service.yaml
----
.Listar Services:
[source,bash]
----
kubectl get services
----
.Ver detalles de un Service:
[source,bash]
----
kubectl describe service <nombre-del-service>
----
.Actualizar un Service:
[source,bash]
----
kubectl apply -f service.yaml
----
.Eliminar un Service:
[source,bash]
----
kubectl delete service <nombre-del-service>
----
.Exponer un Deployment como Service:
[source,bash]
----
kubectl expose deployment <nombre-del-deployment> --port=80 --target-port=8080 --type=NodePort
----

La gestión adecuada de los Services es fundamental para garantizar la conectividad, el balanceo de carga y la exposición segura de las aplicaciones dentro y fuera del clúster.

==== Endpoints y descubrimiento de servicios
Un *Endpoint* en Kubernetes representa la lista de direcciones IP y puertos de los pods que están asociados a un Service. Cuando un Service se crea, Kubernetes genera automáticamente un objeto Endpoint que se actualiza dinámicamente conforme los pods coinciden o dejan de coincidir con el selector del Service.

.El descubrimiento de servicios se realiza de dos formas principales:
- **Variables de entorno**: Kubernetes inyecta variables de entorno en los pods con la información de los servicios disponibles en el mismo namespace.
- **DNS interno**: Kubernetes crea automáticamente registros DNS para cada Service, permitiendo que los pods resuelvan el nombre del servicio (por ejemplo, `mi-servicio.mi-namespace.svc.cluster.local`) y se comuniquen usando ese nombre.

Puedes consultar los endpoints de un Service con:

[source,bash]
----
kubectl get endpoints
kubectl describe service <nombre-del-service>
----

El mecanismo de endpoints y el DNS interno facilitan el descubrimiento y la comunicación entre microservicios dentro del clúster, permitiendo arquitecturas dinámicas y escalables.

==== Proxies y balanceo de carga
En Kubernetes, el balanceo de carga y el proxy de red son fundamentales para distribuir el tráfico entre los pods y garantizar la alta disponibilidad de las aplicaciones.

- *kube-proxy*: Es el componente encargado de implementar las reglas de red en cada nodo, actuando como proxy para el tráfico dirigido a los Services. Puede funcionar en modo iptables o IPVS, redirigiendo el tráfico a los pods disponibles según el selector del Service.
- *Balanceo de carga interno*: Todos los Services de tipo ClusterIP y NodePort utilizan kube-proxy para balancear el tráfico entre los pods asociados.
- *Balanceo de carga externo*: Los Services de tipo LoadBalancer solicitan un balanceador de carga externo (por ejemplo, de un proveedor cloud) para exponer la aplicación fuera del clúster y distribuir el tráfico entrante entre los nodos y pods.
- *Ingress*: Para tráfico HTTP/HTTPS, los recursos Ingress y sus controladores permiten balancear y enrutar el tráfico a diferentes servicios internos, aplicando reglas avanzadas como rutas, TLS y autenticación.

El uso de proxies y balanceadores de carga en Kubernetes permite escalar aplicaciones de forma eficiente y garantizar la disponibilidad y el rendimiento ante cambios en el clúster.

==== DNS interno de Kubernetes
Kubernetes incluye un servicio de DNS interno que resuelve automáticamente los nombres de los servicios y pods dentro del clúster. Cuando se crea un Service, Kubernetes genera un registro DNS con el formato `<nombre-del-service>.<namespace>.svc.cluster.local`, permitiendo que cualquier pod pueda comunicarse con el servicio usando ese nombre.

El DNS interno es gestionado por el componente *CoreDNS* (o kube-dns en versiones antiguas), que se despliega como un conjunto de pods dentro del clúster. CoreDNS responde a las consultas DNS de los pods y resuelve tanto servicios como pods (si está habilitado el subdominio `pod`).

.Ejemplo de resolución DNS de un servicio:
.Un Service llamado `miapp` en el namespace `produccion` será accesible desde cualquier pod como:
----
miapp.produccion.svc.cluster.local
----

.Ventajas del DNS interno:
- Facilita el descubrimiento y la comunicación entre servicios sin necesidad de conocer direcciones IP.
- Permite cambiar la infraestructura subyacente sin modificar la configuración de las aplicaciones.
- Soporta subdominios y nombres cortos para facilitar la usabilidad.

.Puedes comprobar la resolución DNS desde un pod ejecutando:
[source,bash]
----
nslookup <nombre-del-service>
# o
dig <nombre-del-service>.<namespace>.svc.cluster.local
----

El DNS interno es esencial para arquitecturas de microservicios y despliegues dinámicos en Kubernetes.

==== Exposición de servicios al exterior
Para exponer aplicaciones fuera del clúster de Kubernetes, existen varias opciones según el tipo de acceso y el entorno:

- *NodePort*: Expone el servicio en un puerto específico de cada nodo, permitiendo el acceso externo a través de la IP del nodo y el puerto asignado. Es sencillo, pero no recomendado para producción debido a limitaciones de seguridad y escalabilidad.
- *LoadBalancer*: Solicita un balanceador de carga externo (proporcionado por el proveedor cloud) que asigna una IP pública y distribuye el tráfico entrante entre los nodos y pods. Es la opción más común en entornos cloud.
- *Ingress*: Permite exponer múltiples servicios HTTP/HTTPS bajo un mismo punto de entrada, gestionando rutas, certificados TLS y reglas avanzadas mediante un controlador Ingress. Es ideal para aplicaciones web y microservicios.
- *ExternalName*: Redirige el tráfico a un nombre DNS externo, útil para integrar servicios fuera del clúster.

.Ejemplo de exposición usando NodePort:
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio
spec:
  type: NodePort
  selector:
    app: mi-app
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080
----

.Ejemplo de exposición usando LoadBalancer:
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio
spec:
  type: LoadBalancer
  selector:
    app: mi-app
  ports:
    - port: 80
      targetPort: 8080
----

.Ejemplo de exposición usando Ingress:
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mi-ingress
spec:
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mi-servicio
                port:
                  number: 80
----

.Ejemplo de exposición usando ExternalName:
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio-external
spec:
  type: ExternalName
  externalName: ejemplo.com
----

==== Ingress: gestión de tráfico HTTP
Un *Ingress* es un recurso de Kubernetes que gestiona el acceso externo HTTP y HTTPS a los servicios del clúster. Permite definir reglas de enrutamiento basadas en rutas, dominios y subdominios, así como gestionar certificados TLS para tráfico seguro.

El Ingress actúa como punto de entrada único para múltiples servicios, facilitando la publicación de aplicaciones web y microservicios bajo diferentes rutas o dominios. 

.Las reglas de Ingress permiten:
- Redirigir tráfico a diferentes servicios según la URL o el host.
- Aplicar certificados TLS para HTTPS.
- Configurar redirecciones, autenticación y otras políticas avanzadas (según el controlador Ingress utilizado).

.Ejemplo básico de recurso Ingress:
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ejemplo-ingress
spec:
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mi-servicio
                port:
                  number: 80
----

Para que los recursos Ingress funcionen, es necesario desplegar un *Ingress Controller* en el clúster (por ejemplo, NGINX Ingress Controller, Traefik, etc.), que se encargará de procesar las reglas y enrutar el tráfico adecuadamente.

==== Ingress Controllers
Un *Ingress Controller* es el componente encargado de implementar las reglas definidas en los recursos Ingress y enrutar el tráfico HTTP/HTTPS externo hacia los servicios internos del clúster. Sin un Ingress Controller desplegado, los recursos Ingress no tendrán efecto.

.Existen varios controladores populares, entre ellos:
- *NGINX Ingress Controller*: El más utilizado, flexible y ampliamente soportado.
- *Traefik*: Sencillo de configurar, con soporte para múltiples protocolos y características avanzadas.
- *HAProxy Ingress*: Basado en HAProxy, ideal para escenarios de alto rendimiento.
- *Controladores específicos de proveedores cloud*: Como AWS ALB Ingress Controller, GKE Ingress, etc.

.Ejemplo de despliegue de NGINX Ingress Controller:
[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml
----

Una vez desplegado el Ingress Controller, los recursos Ingress definidos en el clúster comenzarán a funcionar y a enrutar el tráfico según las reglas especificadas.

Es importante elegir el Ingress Controller que mejor se adapte a las necesidades de la aplicación y del entorno de despliegue.

==== Configuración de Ingress
La configuración de un recurso Ingress en Kubernetes se realiza mediante un manifiesto YAML donde se definen las reglas de enrutamiento, los hosts, los paths y, opcionalmente, la configuración de TLS para HTTPS.

.Pasos básicos para configurar un Ingress:
1. Asegúrate de tener un Ingress Controller desplegado en el clúster (por ejemplo, NGINX Ingress Controller).
2. Define el recurso Ingress especificando los hosts, paths y servicios de destino.
3. (Opcional) Configura certificados TLS para habilitar HTTPS.

.Ejemplo de configuración de Ingress con TLS:
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mi-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  tls:
    - hosts:
        - miapp.ejemplo.com
      secretName: miapp-tls-secret
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mi-servicio
                port:
                  number: 80
----

.Puntos clave:
- Las *annotations* permiten personalizar el comportamiento del Ingress Controller (por ejemplo, reescritura de URLs, autenticación, rate limiting, etc.).
- El bloque `tls` habilita HTTPS usando un secreto de tipo TLS previamente creado en el mismo namespace.
- Puedes definir múltiples reglas para enrutar tráfico a diferentes servicios según el host o la ruta.

Consulta la documentación del Ingress Controller que utilices para conocer todas las opciones de configuración disponibles.

==== Buenas prácticas para servicios
.Algunas recomendaciones incluyen:
- Utiliza *selectors* claros y consistentes en los Services para evitar conflictos y facilitar el mantenimiento.
- Prefiere *ClusterIP* para comunicación interna y expón servicios al exterior solo cuando sea necesario (NodePort, LoadBalancer o Ingress).
- Asigna *labels* y *annotations* descriptivas a los Services para mejorar la gestión y la observabilidad.
- Limita el uso de *NodePort* en producción; opta por *LoadBalancer* o *Ingress* para mayor seguridad y escalabilidad.
- Documenta los puertos y protocolos utilizados por cada Service.
- Revisa y restringe los endpoints expuestos para minimizar la superficie de ataque.
- Usa *ExternalName* solo cuando sea imprescindible integrar servicios externos.
- Versiona y revisa los manifiestos de Service junto con el resto de la infraestructura como código.
- Supervisa el estado y los endpoints de los Services para detectar problemas de conectividad o balanceo.
- Elimina los Services que ya no se utilicen para evitar confusiones y posibles riesgos de seguridad.

=== Namespaces: organización lógica
Los *Namespaces* en Kubernetes permiten dividir los recursos del clúster en espacios lógicos aislados. Son útiles para separar entornos (desarrollo, pruebas, producción), equipos o proyectos dentro de un mismo clúster, facilitando la gestión de permisos, recursos y políticas.

.Características principales:
- Aíslan recursos como pods, servicios, ConfigMaps y Secrets.
- Permiten aplicar cuotas de recursos y límites por namespace.
- Facilitan la gestión de RBAC (roles y permisos) por entorno o equipo.
- Los objetos sin namespace explícito se crean en el namespace `default`.

.Comandos útiles:
- Listar namespaces:
+
[source,bash]
----
kubectl get namespaces
----
- Crear un namespace:
+
[source,bash]
----
kubectl create namespace mi-namespace
----
- Ejecutar comandos en un namespace específico:
+
[source,bash]
----
kubectl get pods -n mi-namespace
----
- Definir el namespace en un manifiesto YAML:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
  namespace: mi-namespace
spec:
  containers:
    - name: nginx
      image: nginx
----

.Buenas prácticas:
- Utiliza namespaces para separar entornos y equipos.
- Aplica políticas de seguridad y cuotas de recursos por namespace.
- Nombra los namespaces de forma clara y consistente.
- Supervisa el uso de recursos y la actividad en cada namespace.

El uso adecuado de namespaces mejora la organización, seguridad y escalabilidad en clústeres Kubernetes multiusuario o multiproyecto.


== Módulo 4: Configuración de Aplicaciones
La configuración de aplicaciones en Kubernetes es fundamental para gestionar la información sensible y las variables de entorno necesarias para el correcto funcionamiento de los pods. Kubernetes ofrece varias formas de manejar configuraciones, como ConfigMaps y Secrets, así como la posibilidad de inyectar variables de entorno y montar volúmenes.

=== ConfigMaps y secretos
*ConfigMaps* permiten almacenar datos de configuración no sensibles en pares clave-valor, facilitando la separación de la configuración del código de la aplicación. Se pueden montar como archivos o inyectar como variables de entorno en los pods.

*Secrets* almacenan información sensible como contraseñas, tokens o claves, codificada en base64. Al igual que los ConfigMaps, pueden ser montados como archivos o variables de entorno, pero con mayor control de acceso y restricciones de visibilidad.

.Ejemplo de ConfigMap:
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: mi-configmap
data:
  APP_ENV: produccion
  APP_DEBUG: "false"
----

.Ejemplo de Secret:
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: mi-secret
type: Opaque
data:
  PASSWORD: c2VjcmV0MTIz # "secret123" en base64
----

.Usos comunes:
- Inyectar variables de entorno en los contenedores.
- Montar archivos de configuración en rutas específicas del pod.
- Separar información sensible (Secrets) de la configuración general (ConfigMaps).

.Buenas prácticas:
- Usa ConfigMaps para datos no sensibles y Secrets para información confidencial.
- Limita el acceso a los Secrets mediante RBAC.
- No almacenes información sensible en ConfigMaps.
- Versiona y documenta los cambios en la configuración.

=== Variables de entorno
Las variables de entorno permiten pasar información de configuración a los contenedores de forma dinámica y flexible. En Kubernetes, puedes definir variables de entorno directamente en el manifiesto del pod o referenciar valores almacenados en ConfigMaps y Secrets.

.Ejemplo de variables de entorno en un pod:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
spec:
  containers:
    - name: app
      image: mi-app:latest
      env:
        - name: APP_ENV
          value: "produccion"
        - name: APP_DEBUG
          value: "false"
----

.Ejemplo de variables de entorno desde ConfigMap y Secret:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
spec:
  containers:
    - name: app
      image: mi-app:latest
      env:
        - name: APP_ENV
          valueFrom:
            configMapKeyRef:
              name: mi-configmap
              key: APP_ENV
        - name: PASSWORD
          valueFrom:
            secretKeyRef:
              name: mi-secret
              key: PASSWORD
----

.Buenas prácticas:
- Usa variables de entorno para valores que cambian entre entornos o despliegues.
- Prefiere referenciar ConfigMaps y Secrets para separar la configuración del código.
- No almacenes información sensible directamente en variables de entorno; utiliza Secrets.
- Documenta las variables de entorno requeridas por cada aplicación.

=== Gestión de configuraciones
La gestión de configuraciones en Kubernetes se basa en el uso de ConfigMaps, Secrets y variables de entorno para desacoplar la configuración del código de la aplicación. Esto permite modificar el comportamiento de las aplicaciones sin necesidad de reconstruir imágenes ni redeplegar contenedores.

.Puntos clave:
- Utiliza ConfigMaps para almacenar parámetros de configuración no sensibles.
- Usa Secrets para información confidencial como contraseñas, tokens o claves.
- Inyecta la configuración en los pods mediante variables de entorno o montando volúmenes.
- Actualiza ConfigMaps y Secrets con `kubectl apply` o `kubectl edit` para reflejar cambios en la configuración.
- Supervisa y versiona los cambios en la configuración para facilitar el rollback y la trazabilidad.

.Ejemplo de actualización de un ConfigMap:
[source,bash]
----
kubectl edit configmap mi-configmap
----

.Ejemplo de recarga de configuración en un Deployment:
[source,bash]
----
kubectl rollout restart deployment <nombre-del-deployment>
----

.Buenas prácticas:
- Mantén la configuración fuera de la imagen del contenedor.
- Versiona los archivos de configuración junto con el código fuente.
- Limita el acceso a los Secrets mediante políticas de RBAC.
- Documenta las variables y parámetros de configuración requeridos por cada aplicación.

=== Montaje de volúmenes de configuración
Tanto *ConfigMaps* como *Secrets* pueden montarse como volúmenes dentro de los pods, permitiendo que los datos de configuración se expongan como archivos en el sistema de archivos del contenedor. Esto es útil para aplicaciones que requieren archivos de configuración en rutas específicas o para cargar certificados, claves y otros datos sensibles.

.Ejemplo de montaje de ConfigMap como volumen:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
spec:
  containers:
    - name: app
      image: mi-app:latest
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config
  volumes:
    - name: config-vol
      configMap:
        name: mi-configmap
----

.Ejemplo de montaje de Secret como volumen:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: ejemplo-pod
spec:
  containers:
    - name: app
      image: mi-app:latest
      volumeMounts:
        - name: secret-vol
          mountPath: /etc/secret
          readOnly: true
  volumes:
    - name: secret-vol
      secret:
        secretName: mi-secret
----

.Puntos clave:
- Los archivos generados por ConfigMaps y Secrets montados como volúmenes se actualizan automáticamente si el recurso cambia (según la implementación de la aplicación).
- Es recomendable montar los Secrets como solo lectura (`readOnly: true`) para mayor seguridad.
- Puedes especificar subclaves o rutas personalizadas usando la propiedad `items` en la definición del volumen.

El montaje de volúmenes de configuración facilita la gestión centralizada y segura de parámetros y datos sensibles en aplicaciones desplegadas en Kubernetes.

=== Buenas prácticas para configuraciones
.Algunas recomendaciones incluyen:
- Separa la configuración del código fuente utilizando ConfigMaps y Secrets.
- Usa Secrets exclusivamente para información sensible y limita su acceso mediante RBAC.
- Versiona y documenta los cambios en los archivos de configuración.
- Evita almacenar datos sensibles en ConfigMaps.
- Valida y prueba la configuración en entornos de desarrollo antes de aplicarla en producción.
- Utiliza variables de entorno para valores que cambian entre entornos o despliegues.
- Elimina configuraciones obsoletas para evitar confusiones y riesgos de seguridad.
- Limita el acceso a los archivos de configuración montados como volúmenes, especialmente los Secrets, usando `readOnly: true`.
- Supervisa y revisa periódicamente los recursos de configuración para detectar posibles errores o fugas de información.
- Mantén la configuración centralizada y gestionada como parte de la infraestructura como código.

== Módulo 5: Almacenamiento en Kubernetes
=== Conceptos de almacenamiento persistente
El almacenamiento persistente en Kubernetes permite que los datos generados y utilizados por los pods sobrevivan a reinicios, reprogramaciones o eliminaciones de los mismos. A diferencia del almacenamiento efímero (almacenamiento local del pod), el almacenamiento persistente asegura la durabilidad y disponibilidad de los datos más allá del ciclo de vida de los contenedores.

.Principales conceptos:
- *Persistencia*: Los datos almacenados no se pierden aunque el pod sea destruido o recreado.
- *Desacoplamiento*: El almacenamiento está desacoplado del ciclo de vida de los pods, permitiendo que diferentes pods accedan a los mismos datos.
- *Portabilidad*: Kubernetes abstrae el almacenamiento subyacente, permitiendo usar soluciones locales, en red o en la nube (NFS, iSCSI, EBS, GCE Persistent Disk, etc.).

El almacenamiento persistente es fundamental para aplicaciones que requieren guardar información de manera fiable, como bases de datos, sistemas de archivos compartidos o aplicaciones stateful.

En Kubernetes, la gestión del almacenamiento persistente se realiza principalmente a través de los recursos *PersistentVolume* (PV) y *PersistentVolumeClaim* (PVC), que permiten solicitar, aprovisionar y consumir almacenamiento de forma dinámica y flexible.

=== PersistentVolumes y PersistentVolumeClaims
Un *PersistentVolume* (PV) es un recurso del clúster que representa una porción de almacenamiento físico (local, en red o en la nube) gestionada por el administrador o aprovisionada dinámicamente. Un *PersistentVolumeClaim* (PVC) es una solicitud de almacenamiento realizada por un usuario o aplicación, especificando el tamaño y las características requeridas.

El ciclo de uso es el siguiente:
1. El administrador define uno o varios PVs, o se configuran StorageClasses para aprovisionamiento dinámico.
2. El usuario crea un PVC solicitando almacenamiento.
3. Kubernetes enlaza automáticamente el PVC con un PV disponible que cumpla los requisitos.
4. El pod consume el almacenamiento reclamado a través del PVC.

.Ejemplo de PersistentVolume:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-ejemplo
spec:
  capacity:
    storage: 5Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /mnt/datos
----

.Ejemplo de PersistentVolumeClaim:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-ejemplo
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
----

.Ejemplo de uso de un PVC en un pod:
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: pod-con-pvc
spec:
  containers:
    - name: app
      image: nginx
      volumeMounts:
        - name: datos
          mountPath: /usr/share/nginx/html
  volumes:
    - name: datos
      persistentVolumeClaim:
        claimName: pvc-ejemplo
----

.Puntos clave:
- Los PV y PVC desacoplan la gestión del almacenamiento del ciclo de vida de los pods.
- El acceso puede ser *ReadWriteOnce* (un nodo), *ReadOnlyMany* o *ReadWriteMany* (varios nodos, según el backend).
- El almacenamiento puede ser aprovisionado estática o dinámicamente usando StorageClasses.

=== StorageClasses
Una *StorageClass* en Kubernetes define las características y el tipo de almacenamiento dinámico que puede ser aprovisionado para los PersistentVolumeClaims (PVC). Permite abstraer detalles como el tipo de disco, el rendimiento, la replicación o el proveedor de almacenamiento (local, NFS, EBS, GCE, etc.).

Cuando un usuario crea un PVC y especifica una StorageClass, Kubernetes aprovisiona automáticamente un PersistentVolume (PV) con las características definidas en esa clase.

.Características principales:
- Permite aprovisionamiento dinámico de volúmenes.
- Define parámetros como tipo de almacenamiento, políticas de retención y proveedor.
- Facilita la gestión de diferentes tipos de almacenamiento en un mismo clúster.

.Ejemplo de StorageClass para volúmenes estándar:
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mi-storageclass
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
reclaimPolicy: Delete
mountOptions:
  - debug
----

.Ejemplo de PVC usando una StorageClass:
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-con-storageclass
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: mi-storageclass
----

.Puntos clave:
- El parámetro `provisioner` indica el plugin de almacenamiento a utilizar.
- `parameters` define opciones específicas del backend (tipo de disco, IOPS, etc.).
- `reclaimPolicy` controla qué ocurre con el volumen cuando el PVC es eliminado (`Delete` o `Retain`).
- Si no se especifica una StorageClass, se usará la clase por defecto del clúster si existe.

El uso de StorageClasses permite a los administradores ofrecer diferentes calidades y tipos de almacenamiento a los usuarios de forma sencilla y flexible.

=== Administración de volúmenes
La administración de volúmenes en Kubernetes implica la creación, asignación, monitoreo y eliminación de recursos de almacenamiento persistente para los pods. Los volúmenes pueden ser efímeros (vinculados al ciclo de vida del pod) o persistentes (gestionados mediante PV y PVC).

.Pasos comunes en la administración de volúmenes:
- Crear PersistentVolumes (PV) y PersistentVolumeClaims (PVC) según las necesidades de la aplicación.
- Asignar PVCs a los pods mediante la sección `volumes` en los manifiestos.
- Monitorear el estado de los volúmenes y su uso con:
+
[source,bash]
----
kubectl get pv
kubectl get pvc
kubectl describe pv <nombre-del-pv>
kubectl describe pvc <nombre-del-pvc>
----
- Eliminar PVCs y PVs cuando ya no sean necesarios, teniendo en cuenta la política de retención (`reclaimPolicy`) para liberar o conservar los datos.

.Buenas prácticas:
- Utiliza StorageClasses para aprovisionamiento dinámico y flexibilidad.
- Define políticas de retención adecuadas (`Delete` o `Retain`) según la criticidad de los datos.
- Supervisa el uso de almacenamiento y ajusta cuotas o tamaños según el crecimiento de la aplicación.
- Documenta el uso de volúmenes y su relación con los pods y aplicaciones.

La administración eficiente de volúmenes es clave para garantizar la disponibilidad, durabilidad y rendimiento de las aplicaciones stateful en Kubernetes.

=== Soluciones de almacenamiento para Kubernetes
Kubernetes soporta una amplia variedad de soluciones de almacenamiento, tanto locales como en red y en la nube, para satisfacer las necesidades de aplicaciones stateful y persistentes.

.Algunas de las soluciones más comunes incluyen:
- *NFS (Network File System)*: Proporciona almacenamiento compartido en red, ideal para entornos de desarrollo y pruebas.
- *iSCSI*: Permite conectar volúmenes de almacenamiento en red como si fueran discos locales.
- *Ceph*: Plataforma de almacenamiento distribuido que ofrece bloques, archivos y objetos, muy utilizada en entornos empresariales.
- *GlusterFS*: Sistema de archivos distribuido y escalable, adecuado para grandes volúmenes de datos.
- *Longhorn*: Solución nativa de almacenamiento distribuido para Kubernetes, fácil de instalar y administrar.
- *OpenEBS*: Proporciona almacenamiento local y distribuido basado en contenedores, pensado para Kubernetes.
- *Portworx*: Plataforma avanzada de almacenamiento empresarial para Kubernetes, con soporte para replicación, snapshots y cifrado.
- *Soluciones cloud*: 
  - *AWS EBS (Elastic Block Store)*
  - *Google Persistent Disk*
  - *Azure Disk/Files*
  - *Amazon EFS (Elastic File System)*
  - *Azure NetApp Files*

.Puntos clave al elegir una solución:
- Compatibilidad con el entorno (on-premise, cloud, híbrido).
- Soporte para aprovisionamiento dinámico mediante StorageClasses.
- Características como replicación, snapshots, cifrado y rendimiento.
- Facilidad de integración y administración dentro del clúster.

La elección de la solución de almacenamiento adecuada depende de los requisitos de la aplicación, el entorno de despliegue y las necesidades de rendimiento y disponibilidad.

== Módulo 6: Redes en Kubernetes
=== Modelo de red de Kubernetes

El modelo de red de Kubernetes es fundamental para el funcionamiento de los clústeres, ya que permite la comunicación entre los diferentes componentes y aplicaciones desplegadas. A continuación se explica en detalle cómo funciona este modelo:

==== Principios básicos

Kubernetes define un modelo de red plano, donde:

- **Todos los pods pueden comunicarse entre sí sin NAT**: No importa en qué nodo estén, todos los pods tienen una dirección IP única y pueden comunicarse directamente.
- **Los nodos pueden comunicarse con cualquier pod**: Los nodos del clúster pueden acceder a cualquier pod usando su IP.
- **La comunicación es bidireccional**: No hay restricciones de firewall internas por defecto, aunque se pueden aplicar políticas de red.

==== Componentes principales

- **Pod Network**: Cada pod recibe una IP única dentro de un rango definido para el clúster. Esta red es gestionada por un complemento de red (CNI, Container Network Interface).
- **Service Network**: Los servicios de Kubernetes exponen aplicaciones a través de una IP virtual y un nombre DNS, permitiendo el balanceo de carga interno.
- **Node Network**: Los nodos del clúster también tienen sus propias IPs y pueden comunicarse entre sí y con los pods.

==== Plugins de red (CNI)

.Kubernetes no implementa la red por sí mismo, sino que utiliza plugins CNI como:
- **Calico**
- **Flannel**
- **Weave**
- **Cilium**

==== Ejemplo de comunicación

Para ilustrar cómo funciona la comunicación en el modelo de red de Kubernetes, consideremos el siguiente escenario:

===== Escenario

Supongamos que tenemos dos pods desplegados en diferentes nodos del clúster:

- `Pod A` en el `Nodo 1`
- `Pod B` en el `Nodo 2`

Ambos pods forman parte de la misma red definida por el complemento CNI (por ejemplo, Calico o Flannel).

===== Archivos de configuración YAML

A continuación se muestran los archivos `.yml` necesarios para desplegar ambos pods y un servicio para exponer `Pod B`:

.Ejemplo de configuración para `Pod A`:
[source,yaml]
----
# pod-a.yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  labels:
    app: demo
spec:
  containers:
    - name: app
      image: busybox
      command: ["sleep", "3600"]
----

.Ejemplo de configuración para `Pod B`:
[source,yaml]
----
# pod-b.yml
apiVersion: v1
kind: Pod
metadata:
  name: pod-b
  labels:
    app: demo
spec:
  containers:
    - name: app
      image: busybox
      command: ["sleep", "3600"]
----

.Ejemplo de configuración para el servicio `service-b`: que expone `Pod B`:
[source,yaml]
----
# service-b.yml
apiVersion: v1
kind: Service
metadata:
  name: service-b
spec:
  selector:
    app: demo
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
----

===== Flujo de comunicación

1. **Asignación de IPs**: Al crearse, cada pod recibe una dirección IP única dentro del rango de red del clúster.
2. **Solicitud de comunicación**: `Pod A` puede enviar datos a `Pod B` usando la IP de este último o a través del servicio `service-b`.
3. **Enrutamiento transparente**: El complemento de red configura las rutas necesarias en ambos nodos para que el tráfico pueda fluir directamente entre los pods, sin necesidad de NAT.
4. **Recepción de datos**: `Pod B` recibe la solicitud directamente desde `Pod A`, como si estuvieran en la misma red local, aunque estén en nodos distintos.

===== Diagrama de ejemplo

[plantuml, pod-communication, png]
----
@startuml
node "Nodo 1" {
  [Pod A]
}
node "Nodo 2" {
  [Pod B]
}
[Pod A] --> [Pod B] : Comunicación directa (sin NAT)
@enduml
----

===== Puntos clave

- No importa en qué nodo esté cada pod; la red de Kubernetes garantiza la conectividad directa.
- El tráfico entre pods no pasa por NAT ni por proxies intermedios, a menos que se apliquen políticas de red o servicios específicos.
- Este modelo facilita la escalabilidad y la simplicidad en la comunicación entre microservicios.


==== Políticas de red

Las políticas de red en Kubernetes permiten controlar el tráfico de red entre los pods dentro de un clúster. Son una herramienta fundamental para mejorar la seguridad, ya que definen qué conexiones están permitidas y cuáles no, basándose en reglas declarativas.

===== ¿Qué es una política de red?

Una política de red (`NetworkPolicy`) es un recurso de Kubernetes que especifica cómo los pods pueden comunicarse entre sí y con otros recursos de red. Por defecto, sin políticas de red, todo el tráfico entre pods está permitido.

===== ¿Cómo funcionan?

Las políticas de red funcionan seleccionando pods mediante etiquetas y definiendo reglas de entrada (ingress) y salida (egress). Estas reglas pueden restringir el tráfico según:

- El origen o destino (otros pods, namespaces, IPs).
- El puerto y el protocolo.
- Las etiquetas de los pods.

Para que las políticas de red tengan efecto, el clúster debe usar un plugin de red compatible con NetworkPolicy (por ejemplo, Calico, Cilium, Weave).

===== Ejemplo de política de red

El siguiente ejemplo solo permite el tráfico de entrada al pod `app=backend` desde pods con la etiqueta `app=frontend` en el mismo namespace:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
spec:
  podSelector:
    matchLabels:
      app: backend
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: frontend
----

===== Tipos de reglas

- **Ingress**: Controla el tráfico entrante a los pods seleccionados.
- **Egress**: Controla el tráfico saliente desde los pods seleccionados.

===== Buenas prácticas

- Aplicar el principio de menor privilegio: solo permitir el tráfico necesario.
- Probar las políticas en entornos de desarrollo antes de aplicarlas en producción.
- Documentar las reglas y mantenerlas actualizadas.

===== Resumen gráfico

[plantuml, network-policy, png]
----
@startuml
node "Namespace" {
  [Pod frontend] -down- [Pod backend]
}
[Pod frontend] --> [Pod backend] : Permitido
actor "Otro Pod" as Otro
Otro -x [Pod backend] : Denegado
@enduml
----

==== Resumen gráfico

[plantuml, network-model, png]
----
@startuml
node "Nodo A" {
  [Pod 1] - [Pod 2]
}
node "Nodo B" {
  [Pod 3]
}
[Pod 1] ..> [Pod 3] : Comunicación directa
@enduml
----

=== Proxies y balanceadores de carga

En Kubernetes, los proxies y balanceadores de carga son componentes esenciales para gestionar el tráfico de red hacia y entre las aplicaciones desplegadas en el clúster. A continuación se explica en detalle su funcionamiento y sus diferentes tipos:

==== ¿Qué es un proxy?

Un proxy es un intermediario que recibe solicitudes de red y las reenvía a uno o varios servidores de destino. En Kubernetes, los proxies ayudan a dirigir el tráfico hacia los pods correctos, facilitando la escalabilidad y la tolerancia a fallos.

==== ¿Qué es un balanceador de carga?

Un balanceador de carga distribuye el tráfico de red entre varios servidores o pods, asegurando que ninguno se sobrecargue y que las solicitudes se atiendan de manera eficiente. Esto mejora la disponibilidad y el rendimiento de las aplicaciones.

==== Tipos de proxies y balanceadores en Kubernetes

- **kube-proxy**: Es el componente nativo de Kubernetes que implementa la lógica de red para los servicios. kube-proxy puede funcionar en modo iptables, IPVS o userspace, y se encarga de reenviar el tráfico a los pods correctos según las reglas de los servicios.
- **Service de tipo ClusterIP**: Crea una IP virtual interna para acceder a un conjunto de pods. kube-proxy distribuye el tráfico entre los pods asociados.
- **Service de tipo NodePort**: Expone un servicio en un puerto específico de cada nodo del clúster, permitiendo el acceso externo a través de la IP del nodo y el puerto asignado.
- **Service de tipo LoadBalancer**: Integra un balanceador de carga externo (por ejemplo, de un proveedor cloud) que distribuye el tráfico entrante entre los nodos y, a su vez, entre los pods.
- **Ingress**: Es un recurso que gestiona el acceso HTTP/HTTPS externo al clúster. Utiliza controladores (Ingress Controllers) como NGINX o Traefik, que actúan como proxies inversos y balanceadores de carga a nivel de aplicación.

==== Ejemplo de flujo de tráfico

1. Un usuario externo accede a una aplicación a través de un balanceador de carga (LoadBalancer o Ingress).
2. El balanceador de carga recibe la solicitud y la dirige a uno de los nodos del clúster.
3. kube-proxy en el nodo recibe la solicitud y la reenvía a uno de los pods disponibles según la configuración del servicio.

==== Ventajas de usar proxies y balanceadores

- **Alta disponibilidad**: Si un pod falla, el tráfico se redirige automáticamente a otros pods sanos.
- **Escalabilidad**: Permite añadir o quitar pods sin interrumpir el servicio.
- **Flexibilidad**: Se pueden definir reglas avanzadas de enrutamiento y seguridad.

==== Resumen gráfico

[plantuml, format="png", id="myId"]
----
@startuml
actor Usuario
cloud "LoadBalancer / Ingress" as LB
node "Nodo 1" {
  [Pod A]
}
node "Nodo 2" {
  [Pod B]
}
Usuario --> LB : Solicitud HTTP
LB --> [Pod A] : Tráfico balanceado
LB --> [Pod B] : Tráfico balanceado
@enduml
----

=== Políticas de red en Kubernetes

Las políticas de red (`NetworkPolicy`) en Kubernetes son un mecanismo fundamental para controlar el flujo de tráfico entre los pods y otros recursos de red dentro del clúster. Permiten definir reglas declarativas que especifican qué conexiones están permitidas y cuáles no, mejorando así la seguridad y el aislamiento de las aplicaciones.

==== ¿Qué es una NetworkPolicy?

Una NetworkPolicy es un recurso de Kubernetes que define cómo los pods pueden comunicarse entre sí y con otros endpoints (otros pods, namespaces, direcciones IP externas, etc.). Por defecto, si no hay ninguna política aplicada, todo el tráfico entre pods está permitido. Al aplicar una NetworkPolicy, solo el tráfico explícitamente permitido por las reglas será aceptado; el resto será bloqueado.

==== ¿Cómo funcionan las NetworkPolicies?

Las NetworkPolicies funcionan seleccionando pods mediante etiquetas (`labels`) y definiendo reglas de entrada (`ingress`) y salida (`egress`). Estas reglas pueden restringir el tráfico según:

- El origen o destino (otros pods, namespaces, IPs).
- El puerto y el protocolo.
- Las etiquetas de los pods.

Para que las NetworkPolicies tengan efecto, el clúster debe usar un plugin de red compatible con este recurso, como Calico, Cilium o Weave.

==== Ejemplo didáctico: Permitir solo tráfico de frontend a backend

Supongamos que tienes dos aplicaciones: un frontend y un backend, cada una en su propio pod. Solo quieres permitir que el frontend se comunique con el backend, y bloquear el resto del tráfico.

.Primero, define los pods con sus etiquetas:

[source,yaml]
----
# pod-frontend.yml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    app: frontend
spec:
  containers:
    - name: app
      image: nginx
      ports:
        - containerPort: 80
----
[source,yaml]
----
# pod-backend.yml
apiVersion: v1
kind: Pod
metadata:
  name: backend
  labels:
    app: backend
spec:
  containers:
    - name: app
      image: nginx
      ports:
        - containerPort: 80
----

.Ahora, crea la NetworkPolicy para permitir solo el tráfico de los pods con la etiqueta `app: frontend` hacia los pods con la etiqueta `app: backend`:

[source,yaml]
----
# networkpolicy-backend.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 80
----

.Con esta configuración:
- Solo los pods con la etiqueta `app: frontend` podrán acceder al puerto 80 de los pods con la etiqueta `app: backend`.
- Todo el tráfico de entrada desde otros pods o fuentes será bloqueado.

==== Ejemplo avanzado: Permitir tráfico de salida solo a Internet

Supón que quieres que tus pods solo puedan acceder a Internet (por ejemplo, para actualizaciones o APIs externas), pero no a otros pods del clúster.

[source,yaml]
----
# networkpolicy-egress-internet.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-to-internet
spec:
  podSelector: {} # Aplica a todos los pods del namespace
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
----

.Esta política:
- Permite a todos los pods del namespace realizar conexiones de salida (egress) solo a cualquier dirección IP (Internet) en los puertos 80 y 443.
- Bloquea el tráfico de salida a otros pods del clúster, a menos que se definan reglas adicionales.

==== Buenas prácticas

- **Principio de menor privilegio**: Solo permite el tráfico estrictamente necesario.
- **Prueba en entornos de desarrollo** antes de aplicar en producción.
- **Documenta** cada política y su propósito.
- **Combina políticas** para cubrir diferentes escenarios de seguridad (por ejemplo, tráfico interno y externo).

==== Diagrama explicativo

[plantuml, network-policy-example, png]
----
@startuml
node "Namespace" {
  [Pod frontend] -down- [Pod backend]
}
[Pod frontend] --> [Pod backend] : Permitido
actor "Otro Pod" as Otro
Otro -x [Pod backend] : Denegado
@enduml
----

=== Ingress Controllers

Un *Ingress Controller* es un componente esencial en Kubernetes que implementa las reglas definidas en los recursos Ingress y enruta el tráfico HTTP/HTTPS externo hacia los servicios internos del clúster. Sin un Ingress Controller desplegado, los recursos Ingress no tendrán efecto y el tráfico externo no podrá ser gestionado mediante reglas avanzadas de enrutamiento.

==== ¿Qué es un Ingress Controller?

El recurso Ingress en Kubernetes solo define las reglas de enrutamiento, pero necesita un controlador que las interprete y actúe como proxy inverso y balanceador de carga. El Ingress Controller escucha los cambios en los recursos Ingress y configura automáticamente el proxy para enrutar el tráfico según las reglas especificadas.

==== Tipos de Ingress Controllers

.Existen múltiples implementaciones de Ingress Controller, cada una con características y ventajas particulares:

- **NGINX Ingress Controller**: El más popular y ampliamente soportado, ideal para la mayoría de escenarios.
- **Traefik**: Sencillo de configurar, con soporte para múltiples protocolos y funcionalidades avanzadas.
- **HAProxy Ingress**: Basado en HAProxy, recomendado para entornos de alto rendimiento.
- **Controladores cloud**: AWS ALB Ingress Controller, GKE Ingress Controller, Azure Application Gateway, etc.

La elección depende de los requisitos de la aplicación, el entorno y las características deseadas (TLS, autenticación, reescritura de URLs, etc.).

==== Ejemplo: Despliegue de NGINX Ingress Controller

Para instalar el NGINX Ingress Controller en un clúster Kubernetes, puedes aplicar el manifiesto oficial:

[source,bash]
----
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.10.1/deploy/static/provider/cloud/deploy.yaml
----

Esto desplegará los recursos necesarios (Deployment, Service, RBAC, ConfigMap, etc.) en el namespace `ingress-nginx`.

.Verifica que el controlador está en funcionamiento:
[source,bash]
----
kubectl get pods -n ingress-nginx
kubectl get services -n ingress-nginx
----

El Service de tipo LoadBalancer o NodePort expone el Ingress Controller al exterior.

==== Ejemplo de recurso Ingress

Una vez desplegado el Ingress Controller, puedes definir un recurso Ingress para enrutar tráfico HTTP/HTTPS a tus servicios internos:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ejemplo-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mi-servicio
                port:
                  number: 80
----

Opcionalmente, puedes añadir soporte para TLS:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ejemplo-ingress-tls
spec:
  tls:
    - hosts:
        - miapp.ejemplo.com
      secretName: miapp-tls-secret
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mi-servicio
                port:
                  number: 80
----

Para que funcione HTTPS, debes crear un Secret de tipo TLS con tu certificado y clave privada:

[source,bash]
----
kubectl create secret tls miapp-tls-secret --cert=certificado.crt --key=clave.key
----

==== Funcionamiento general

1. El usuario accede a `miapp.ejemplo.com`.
2. El Ingress Controller recibe la petición en el puerto 80 o 443.
3. El controlador consulta las reglas Ingress y enruta la petición al Service correspondiente.
4. El Service balancea el tráfico entre los pods disponibles.

==== Buenas prácticas

- Despliega el Ingress Controller en un namespace dedicado.
- Usa anotaciones para personalizar el comportamiento (reescritura de URLs, autenticación, rate limiting, etc.).
- Protege los endpoints de administración del Ingress Controller.
- Usa certificados TLS válidos para producción.
- Supervisa el estado del Ingress Controller y revisa los logs ante problemas de enrutamiento.

==== Resumen gráfico

[plantuml, ingress-controller, png]
----
@startuml
actor Usuario
cloud "Internet"
cloud "Ingress Controller"
node "Kubernetes Cluster" {
  [Service A] --> [Pod A1]
  [Service A] --> [Pod A2]
}
Usuario --> "Internet" --> "Ingress Controller" : Solicitud HTTP/HTTPS
"Ingress Controller" --> [Service A] : Enrutamiento según reglas
@enduml
----

=== Service Mesh

Un *Service Mesh* es una capa de infraestructura dedicada a gestionar la comunicación entre servicios (microservicios) dentro de un clúster de Kubernetes. Proporciona funcionalidades avanzadas como balanceo de carga, descubrimiento de servicios, seguridad, observabilidad, control de tráfico y recuperación ante fallos, todo ello sin modificar el código de las aplicaciones.

==== ¿Por qué usar un Service Mesh?

En arquitecturas de microservicios, la complejidad de la comunicación entre servicios crece rápidamente. Un Service Mesh ayuda a resolver retos como:

- **Balanceo de carga inteligente** entre instancias de servicios.
- **Encriptación de tráfico interno** (mTLS) para mayor seguridad.
- **Control de tráfico**: redirección, canary releases, circuit breaking, retries, timeouts.
- **Observabilidad**: métricas, trazas distribuidas y logs detallados de las llamadas entre servicios.
- **Gestión de políticas**: control de acceso, autenticación y autorización entre servicios.

==== Arquitectura de un Service Mesh

La mayoría de los Service Mesh modernos, como Istio o Linkerd, utilizan el patrón *sidecar proxy*. Esto significa que junto a cada pod de la aplicación se despliega un contenedor adicional (sidecar) que intercepta y gestiona todo el tráfico de red del pod.

[plantuml, service-mesh-architecture, png]
----
@startuml
node "Nodo Kubernetes" {
  [Servicio A] - [Sidecar Proxy A]
  [Servicio B] - [Sidecar Proxy B]
}
[Sidecar Proxy A] <--> [Sidecar Proxy B] : Comunicación gestionada
cloud "Control Plane"
[Sidecar Proxy A] ..> "Control Plane" : Configuración y métricas
[Sidecar Proxy B] ..> "Control Plane"
@enduml
----

- **Data Plane**: Los sidecars gestionan el tráfico entre servicios.
- **Control Plane**: Un componente centralizado (por ejemplo, Istio Pilot) gestiona la configuración y políticas de los sidecars.

==== Ejemplo: Despliegue básico de Istio

A continuación se muestra cómo instalar Istio, uno de los Service Mesh más populares, y cómo inyectar el sidecar en un deployment.

.Instalar Istio en el clúster:
[source,bash]
----
curl -L https://istio.io/downloadIstio | sh -
cd istio-*
export PATH=$PWD/bin:$PATH
istioctl install --set profile=demo -y
kubectl label namespace default istio-injection=enabled
----

.Ejemplo de deployment con sidecar automático:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app-demo
spec:
  replicas: 2
  selector:
    matchLabels:
      app: demo
  template:
    metadata:
      labels:
        app: demo
    spec:
      containers:
        - name: app
          image: nginx
          ports:
            - containerPort: 80
----

Cuando el namespace tiene la etiqueta `istio-injection=enabled`, Istio inyecta automáticamente el sidecar Envoy en cada pod.

.Ejemplo de configuración de política de tráfico (VirtualService):
[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: demo-route
spec:
  hosts:
    - app-demo
  http:
    - route:
        - destination:
            host: app-demo
            subset: v1
----

.Ejemplo de política de seguridad (PeerAuthentication):
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: default
spec:
  mtls:
    mode: STRICT
----

==== Funcionalidades clave de un Service Mesh

- **mTLS (Mutual TLS)**: Cifra el tráfico entre servicios y autentica los extremos.
- **Control de tráfico**: Permite implementar blue/green, canary releases, A/B testing, etc.
- **Circuit breaking y retries**: Mejora la resiliencia ante fallos.
- **Rate limiting y quotas**: Controla el uso de recursos y protege servicios críticos.
- **Trazabilidad y métricas**: Exporta métricas a Prometheus y trazas a Jaeger/Zipkin.

==== Ejemplo de observabilidad con Istio

Istio integra herramientas como Prometheus, Grafana y Kiali para monitorizar el tráfico y visualizar la topología de servicios.

.Desplegar addons de observabilidad:
[source,bash]
----
kubectl apply -f samples/addons
----

.Acceder a Kiali (dashboard de topología y métricas):
[source,bash]
----
kubectl port-forward svc/kiali -n istio-system 20001:20001
# Luego abre http://localhost:20001 en tu navegador
----

==== Otros Service Mesh populares

- **Linkerd**: Enfocado en simplicidad y bajo consumo de recursos.
- **Consul Connect**: De HashiCorp, integra descubrimiento de servicios y mTLS.
- **AWS App Mesh**: Service Mesh gestionado para AWS.

==== Buenas prácticas

- Evalúa la complejidad y el valor añadido antes de adoptar un Service Mesh.
- Empieza con un entorno de pruebas y migra gradualmente los servicios.
- Supervisa el impacto en el rendimiento y ajusta la configuración según las necesidades.
- Documenta las políticas y flujos de tráfico implementados.


=== Exposición de servicios al exterior

Exponer servicios al exterior en Kubernetes es fundamental para que las aplicaciones sean accesibles fuera del clúster. Kubernetes ofrece varias estrategias para lograrlo, cada una adecuada para diferentes escenarios y necesidades de seguridad, escalabilidad y facilidad de gestión.

==== Opciones principales para exponer servicios

- **NodePort**: Expone el servicio en un puerto específico de cada nodo del clúster. Permite el acceso externo a través de la IP del nodo y el puerto asignado. Es sencillo, pero tiene limitaciones de seguridad y escalabilidad.
- **LoadBalancer**: Solicita un balanceador de carga externo (usualmente proporcionado por el proveedor cloud) que asigna una IP pública y distribuye el tráfico entrante entre los nodos y pods. Es la opción más común en entornos cloud.
- **Ingress**: Permite exponer múltiples servicios HTTP/HTTPS bajo un mismo punto de entrada, gestionando rutas, certificados TLS y reglas avanzadas mediante un Ingress Controller. Es ideal para aplicaciones web y microservicios.
- **ExternalName**: Redirige el tráfico a un nombre DNS externo, útil para integrar servicios fuera del clúster.

==== Ejemplo de Service tipo NodePort

Este tipo de servicio expone la aplicación en un puerto alto (por defecto entre 30000 y 32767) de cada nodo del clúster.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio-nodeport
spec:
  type: NodePort
  selector:
    app: mi-app
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080
----

- Acceso: `http://<IP-del-nodo>:30080`
- Útil para pruebas o entornos pequeños.

==== Ejemplo de Service tipo LoadBalancer

En entornos cloud, este tipo de servicio crea automáticamente un balanceador de carga externo y asigna una IP pública.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio-lb
spec:
  type: LoadBalancer
  selector:
    app: mi-app
  ports:
    - port: 80
      targetPort: 8080
----

- Acceso: `http://<IP-pública-asignada>`
- Requiere soporte del proveedor cloud.

==== Ejemplo de exposición usando Ingress

Ingress permite exponer varios servicios HTTP/HTTPS bajo un mismo dominio o IP, gestionando rutas y certificados TLS.

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mi-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: mi-servicio-lb
                port:
                  number: 80
  tls:
    - hosts:
        - miapp.ejemplo.com
      secretName: miapp-tls-secret
----

Para habilitar HTTPS, crea un Secret TLS:

[source,bash]
----
kubectl create secret tls miapp-tls-secret --cert=certificado.crt --key=clave.key
----

Recuerda que para que Ingress funcione, necesitas desplegar un Ingress Controller (por ejemplo, NGINX Ingress Controller).

==== Ejemplo de Service tipo ExternalName

Este tipo de servicio actúa como un alias DNS para servicios externos al clúster.

[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: mi-servicio-external
spec:
  type: ExternalName
  externalName: ejemplo.com
----

- Acceso: Los pods que consulten `mi-servicio-external` serán redirigidos a `ejemplo.com`.

==== Buenas prácticas

- Prefiere LoadBalancer o Ingress para producción, ya que ofrecen mayor flexibilidad y seguridad.
- Usa NodePort solo para pruebas o entornos controlados.
- Protege los servicios expuestos con TLS y autenticación cuando sea necesario.
- Limita la exposición de servicios solo a los estrictamente necesarios.
- Documenta los endpoints y puertos expuestos para facilitar la gestión y el mantenimiento.

==== Resumen gráfico

[plantuml, expose-services, png]
----
@startuml
actor Usuario
cloud "Internet"
cloud "LoadBalancer / Ingress"
node "Kubernetes Cluster" {
  [Service NodePort] --> [Pod A]
  [Service LoadBalancer] --> [Pod B]
  [Service Ingress] --> [Pod C]
}
Usuario --> "Internet" --> "LoadBalancer / Ingress"
"LoadBalancer / Ingress" --> [Service NodePort]
"LoadBalancer / Ingress" --> [Service LoadBalancer]
"LoadBalancer / Ingress" --> [Service Ingress]
@enduml
----

== Módulo 7: Escalabilidad y Alta Disponibilidad

La escalabilidad y la alta disponibilidad son pilares fundamentales en Kubernetes para garantizar que las aplicaciones puedan crecer según la demanda y mantenerse operativas ante fallos. Este módulo aborda los mecanismos y mejores prácticas para lograr estos objetivos.

=== Escalado horizontal de pods (HPA)

El *Horizontal Pod Autoscaler* (HPA) es un recurso de Kubernetes que ajusta automáticamente el número de réplicas de un Deployment, ReplicaSet o StatefulSet en función de métricas como el uso de CPU, memoria u otras métricas personalizadas. Esto permite que las aplicaciones escalen dinámicamente según la demanda, mejorando la eficiencia y la disponibilidad.

==== ¿Cómo funciona el HPA?

El HPA monitoriza las métricas especificadas (por ejemplo, el uso promedio de CPU) y, si se supera un umbral definido, incrementa el número de pods. Si la carga disminuye, reduce el número de réplicas para optimizar el uso de recursos.

Para que el HPA funcione, es necesario tener instalado el *Metrics Server* en el clúster, ya que este componente recopila y expone las métricas de los pods.

==== Ejemplo paso a paso

.1. Instala Metrics Server (si no está instalado):
[source,bash]
----
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
----

.2. Define un Deployment con recursos solicitados:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: webapp
          image: nginx
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
----

.3. Crea un recurso HPA para escalar según el uso de CPU:
[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: webapp-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: webapp
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
----

Este HPA mantendrá entre 2 y 10 réplicas del Deployment `webapp`, ajustando el número de pods para mantener el uso promedio de CPU cerca del 50%.

.4. Verifica el estado del HPA:
[source,bash]
----
kubectl get hpa
kubectl describe hpa webapp-hpa
----

.5. (Opcional) Genera carga para observar el escalado:
[source,bash]
----
kubectl run -i --tty load-generator --rm --image=busybox -- /bin/sh
# Dentro del pod, ejecuta:
while true; do wget -q -O- http://webapp; done
----

==== Buenas prácticas

- Define correctamente los recursos (`requests` y `limits`) en los contenedores para que el HPA funcione de forma eficiente.
- Ajusta los valores de `minReplicas` y `maxReplicas` según la capacidad del clúster y la criticidad de la aplicación.
- Supervisa el comportamiento del HPA y revisa las métricas para evitar escalados innecesarios o insuficientes.
- Considera el uso de métricas personalizadas (por ejemplo, latencia, número de peticiones) para escenarios avanzados.

=== Escalado vertical

El *Vertical Pod Autoscaler* (VPA) es un recurso de Kubernetes que ajusta automáticamente los recursos asignados (CPU y memoria) a los pods en función de su uso real. Es especialmente útil para aplicaciones que no pueden escalar horizontalmente o que requieren ajustes dinámicos de recursos para optimizar el rendimiento y la eficiencia.

==== ¿Cómo funciona el VPA?

El VPA monitoriza el consumo de recursos de los pods y recomienda o aplica automáticamente nuevos valores de `requests` y `limits` para CPU y memoria. Cuando detecta que un pod necesita más (o menos) recursos, puede reiniciar el pod con los valores actualizados, asegurando que la aplicación tenga siempre los recursos adecuados.

.Existen tres modos de funcionamiento:
- `Off`: Solo recomienda recursos, pero no los aplica.
- `Auto`: Aplica automáticamente los cambios y reinicia los pods si es necesario.
- `Initial`: Solo ajusta los recursos al crear el pod, pero no los modifica después.

==== Ejemplo de despliegue de VPA

.1. Instala el VPA en el clúster (si no está instalado):

[source,bash]
----
kubectl apply -f https://github.com/kubernetes/autoscaler/releases/latest/download/vertical-pod-autoscaler.yaml
----

.2. Define un Deployment para la aplicación:

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: webapp
          image: nginx
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
----

.3. Crea un recurso VPA asociado al Deployment:

[source,yaml]
----
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: webapp-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: webapp
  updatePolicy:
    updateMode: "Auto"
----

Con esta configuración, el VPA monitoriza el uso de recursos de los pods del Deployment `webapp` y ajusta automáticamente los valores de CPU y memoria según la demanda.

==== Comandos útiles

- Ver recomendaciones del VPA:
+
[source,bash]
----
kubectl describe vpa webapp-vpa
----

- Ver los recursos actuales de los pods:
+
[source,bash]
----
kubectl get pods -o jsonpath="{range .items[*]}{.metadata.name}{'\t'}{.spec.containers[*].resources}{'\n'}{end}"
----

==== Buenas prácticas

.Las siguientes son algunas recomendaciones para el uso efectivo de VPA:
- Usa VPA para aplicaciones que no pueden escalar horizontalmente o que requieren ajustes finos de recursos.
- No combines HPA y VPA en modo `Auto` sobre el mismo recurso, ya que pueden entrar en conflicto (usa HPA para réplicas y VPA para recursos, pero con precaución).
- Supervisa el comportamiento tras aplicar VPA, ya que los pods pueden ser reiniciados automáticamente.
- Ajusta los límites mínimos y máximos de recursos si es necesario para evitar asignaciones excesivas o insuficientes.

=== Actualizaciones sin tiempo de inactividad

Kubernetes permite realizar actualizaciones de aplicaciones sin interrumpir el servicio mediante el mecanismo de *rolling updates* gestionado por los Deployments. Esto es fundamental para mantener la disponibilidad y la experiencia del usuario durante despliegues de nuevas versiones, correcciones o cambios de configuración.

==== ¿Cómo funcionan las actualizaciones sin downtime?

Cuando actualizas un Deployment (por ejemplo, cambiando la imagen del contenedor), Kubernetes crea gradualmente nuevos pods con la nueva versión y elimina los antiguos solo cuando los nuevos están listos. Este proceso se controla mediante los parámetros `maxUnavailable` y `maxSurge`, que definen cuántos pods pueden estar fuera de servicio y cuántos nuevos pueden crearse por encima del número deseado de réplicas.

==== Ejemplo de Deployment con estrategia de rolling update

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: webapp
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: webapp
          image: nginx:1.25
          ports:
            - containerPort: 80
----

- `maxUnavailable: 1` permite que, como máximo, un pod no esté disponible durante la actualización.
- `maxSurge: 1` permite crear un pod adicional por encima del número de réplicas durante el despliegue.

==== Proceso de actualización paso a paso

1. Modifica el manifiesto del Deployment (por ejemplo, cambia la versión de la imagen).
2. Aplica el cambio:
+
[source,bash]
----
kubectl apply -f deployment.yaml
----
3. Kubernetes inicia la actualización, creando nuevos pods y eliminando los antiguos de forma progresiva.
4. Puedes monitorizar el progreso con:
+
[source,bash]
----
kubectl rollout status deployment/webapp
----
5. Si ocurre un error, puedes hacer rollback a la versión anterior:
+
[source,bash]
----
kubectl rollout undo deployment/webapp
----

==== Buenas prácticas

- Define probes de *readiness* y *liveness* en los contenedores para asegurar que solo los pods listos reciban tráfico.
- Usa `maxUnavailable` y `maxSurge` según la criticidad de la aplicación y la capacidad del clúster.
- Supervisa el despliegue y revisa los logs para detectar posibles problemas.
- Documenta el proceso de actualización y los parámetros utilizados.

==== Ejemplo de probes en el Deployment

[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: webapp
    spec:
      containers:
        - name: webapp
          image: nginx:1.25
          ports:
            - containerPort: 80
          readinessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 5
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /
              port: 80
            initialDelaySeconds: 15
            periodSeconds: 20
----

=== Estrategias de despliegue avanzadas

Las estrategias de despliegue avanzadas en Kubernetes permiten minimizar el impacto de los cambios en producción, reducir riesgos y facilitar la validación progresiva de nuevas versiones. Las más utilizadas son *Blue/Green*, *Canary* y *A/B Testing*. Todas ellas se pueden implementar combinando Deployments, Services, etiquetas y, en muchos casos, recursos Ingress o Service Mesh.

==== Blue/Green Deployment

En esta estrategia, existen dos entornos idénticos: uno activo (Blue) y otro inactivo (Green). El entorno Green se actualiza con la nueva versión y, tras las pruebas, el tráfico se redirige completamente del entorno Blue al Green.

.Ejemplo de implementación con Deployments y Services:
[source,yaml]
----
# Deployment Blue (versión actual)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: blue
  template:
    metadata:
      labels:
        app: webapp
        version: blue
    spec:
      containers:
        - name: webapp
          image: mi-app:v1
----
[source,yaml]
----
# Deployment Green (nueva versión)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: green
  template:
    metadata:
      labels:
        app: webapp
        version: green
    spec:
      containers:
        - name: webapp
          image: mi-app:v2
----
[source,yaml]
----
# Service apuntando a la versión activa
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
    version: blue # Cambia a 'green' para hacer el switch
  ports:
    - port: 80
      targetPort: 80
----

.Para cambiar el tráfico a la nueva versión, solo actualiza el selector del Service a `version: green`.

==== Canary Deployment

Permite liberar la nueva versión a un pequeño porcentaje de usuarios/pods y, si no hay problemas, aumentar progresivamente el tráfico.

.Ejemplo de implementación:
[source,yaml]
----
# Deployment estable (90% del tráfico)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-stable
spec:
  replicas: 9
  selector:
    matchLabels:
      app: webapp
      track: stable
  template:
    metadata:
      labels:
        app: webapp
        track: stable
    spec:
      containers:
        - name: webapp
          image: mi-app:v1
----
[source,yaml]
----
# Deployment canary (10% del tráfico)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
      track: canary
  template:
    metadata:
      labels:
        app: webapp
        track: canary
    spec:
      containers:
        - name: webapp
          image: mi-app:v2
----
[source,yaml]
----
# Service balanceando entre ambas versiones
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
    - port: 80
      targetPort: 80
----

.El Service balanceará el tráfico entre los pods de ambas versiones según el número de réplicas. Puedes ajustar la proporción modificando el número de réplicas de cada Deployment.

.Para un control más granular (por ejemplo, por porcentaje real de tráfico HTTP), utiliza un Ingress Controller avanzado (NGINX, Traefik) o un Service Mesh como Istio.

==== A/B Testing

Similar al canary, pero el enrutamiento se basa en características del usuario, cabeceras HTTP, cookies, etc. Requiere un Ingress Controller avanzado o Service Mesh.

.Ejemplo de regla de Ingress para A/B Testing con NGINX:
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ab-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-by-header: "X-User-Type"
    nginx.ingress.kubernetes.io/canary-by-header-value: "beta"
spec:
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: webapp-canary
                port:
                  number: 80
----

.Los usuarios que envíen la cabecera `X-User-Type: beta` serán dirigidos a la versión canary.

==== Buenas prácticas

- Automatiza los despliegues y cambios de tráfico usando herramientas de CI/CD.
- Supervisa métricas y logs durante el despliegue para detectar problemas rápidamente.
- Define criterios claros de rollback y monitorea la experiencia del usuario.
- Documenta cada estrategia y los pasos para revertir cambios si es necesario.

==== Resumen gráfico

[plantuml, advanced-deploy-strategies, png]
----
@startuml
actor Usuario
cloud "Internet"
cloud "Ingress/Service Mesh"
node "Kubernetes Cluster" {
  [Pods v1] as v1
  [Pods v2] as v2
}
Usuario --> "Internet" --> "Ingress/Service Mesh"
"Ingress/Service Mesh" --> v1 : Tráfico estable/canary
"Ingress/Service Mesh" --> v2 : Tráfico canary/green
@enduml
----

=== Autoscaling basado en métricas

El autoscaling basado en métricas permite que Kubernetes ajuste automáticamente los recursos de las aplicaciones en función de métricas observadas, como el uso de CPU, memoria o métricas personalizadas (por ejemplo, número de peticiones, latencia, etc.). Esto garantiza que las aplicaciones puedan responder a cambios en la demanda de forma eficiente y automática.

==== Tipos de autoscaling en Kubernetes

- **Horizontal Pod Autoscaler (HPA)**: Escala el número de pods de un Deployment, ReplicaSet o StatefulSet según métricas como CPU, memoria o métricas personalizadas.
- **Vertical Pod Autoscaler (VPA)**: Ajusta los recursos (CPU y memoria) asignados a los pods en función de su uso real.
- **Cluster Autoscaler**: Escala el número de nodos del clúster según la demanda de recursos.

==== Ejemplo de HPA basado en CPU y memoria

Para usar HPA con métricas de CPU y memoria, asegúrate de tener instalado el *Metrics Server*.

.Ejemplo de Deployment:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mi-app
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mi-app
  template:
    metadata:
      labels:
        app: mi-app
    spec:
      containers:
        - name: mi-app
          image: nginx
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "256Mi"
----

.Ejemplo de HPA usando CPU y memoria:
[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mi-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mi-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 60
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70
----

Este HPA escalará el número de pods para mantener el uso promedio de CPU en 60% y de memoria en 70%.

==== Autoscaling con métricas personalizadas

Puedes escalar en función de métricas personalizadas (por ejemplo, número de peticiones HTTP, latencia, etc.) usando el *Custom Metrics API* y herramientas como Prometheus Adapter.

.Ejemplo de HPA con métrica personalizada:
[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: mi-app-hpa-custom
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: mi-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Pods
      pods:
        metric:
          name: http_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"
----

En este ejemplo, el HPA escalará los pods para mantener un promedio de 100 peticiones HTTP por segundo por pod.

==== Cluster Autoscaler

El *Cluster Autoscaler* ajusta automáticamente el número de nodos en el clúster según la demanda de recursos. Es especialmente útil en entornos cloud, donde se pueden añadir o eliminar nodos dinámicamente.

.Para instalar el Cluster Autoscaler en GKE, EKS o AKS, consulta la documentación oficial de tu proveedor cloud.

==== Buenas prácticas

- Define correctamente los recursos (`requests` y `limits`) en los contenedores para que el autoscaling funcione de forma eficiente.
- Supervisa el comportamiento del autoscaling y ajusta los umbrales según la carga real de la aplicación.
- Usa métricas personalizadas para escenarios avanzados y ajusta el HPA según las necesidades del negocio.
- Considera el uso combinado de HPA, VPA y Cluster Autoscaler para una escalabilidad completa, pero revisa posibles conflictos.

== Módulo 8: Seguridad en Kubernetes

La seguridad es un aspecto crítico en cualquier entorno de Kubernetes, ya que los clústeres suelen ejecutar aplicaciones sensibles y exponer servicios tanto internos como externos. Este módulo aborda los principales mecanismos y buenas prácticas para proteger los recursos, controlar el acceso y minimizar los riesgos.

=== RBAC (Control de Acceso Basado en Roles)

El *Role-Based Access Control* (RBAC) es el mecanismo principal de Kubernetes para gestionar los permisos y el acceso a los recursos del clúster. Permite definir de forma granular quién puede realizar qué acciones sobre qué recursos, mejorando la seguridad y el control operativo.

==== Conceptos clave

- **Role**: Define un conjunto de permisos (acciones sobre recursos) dentro de un namespace específico.
- **ClusterRole**: Similar a Role, pero los permisos aplican a todos los namespaces o a recursos globales del clúster.
- **RoleBinding**: Asocia un Role a un usuario, grupo o ServiceAccount dentro de un namespace.
- **ClusterRoleBinding**: Asocia un ClusterRole a usuarios, grupos o ServiceAccounts a nivel de clúster.

==== Ejemplo básico de Role y RoleBinding

Supón que quieres permitir que un usuario o ServiceAccount solo pueda listar y ver pods en el namespace `produccion`.

.Role:
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: produccion
  name: lector-pods
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
----

.RoleBinding:
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vincular-lector
  namespace: produccion
subjects:
  - kind: User
    name: juan
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: lector-pods
  apiGroup: rbac.authorization.k8s.io
----

==== Ejemplo de ClusterRole y ClusterRoleBinding

Para otorgar permisos de solo lectura sobre todos los pods del clúster a un ServiceAccount:

.ClusterRole:
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-lector-pods
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list"]
----

.ClusterRoleBinding:
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-vincular-lector
subjects:
  - kind: ServiceAccount
    name: lector
    namespace: produccion
roleRef:
  kind: ClusterRole
  name: cluster-lector-pods
  apiGroup: rbac.authorization.k8s.io
----

==== Buenas prácticas

.Algunas recomendaciones para implementar RBAC de forma efectiva:
- Aplica el principio de menor privilegio: otorga solo los permisos estrictamente necesarios.
- Usa ServiceAccounts para aplicaciones y automatizaciones, no usuarios personales.
- Prefiere Role/RoleBinding para permisos en un namespace y ClusterRole/ClusterRoleBinding solo cuando sea necesario.
- Revisa y audita periódicamente los permisos asignados.
- Documenta los roles y bindings creados para facilitar el mantenimiento y la seguridad.

=== Service Accounts

Las *Service Accounts* en Kubernetes son identidades especiales que utilizan los pods y procesos dentro del clúster para autenticarse y acceder a la API de Kubernetes. A diferencia de los usuarios humanos, las Service Accounts están pensadas para aplicaciones, controladores y scripts automatizados que necesitan interactuar con recursos del clúster.

==== ¿Para qué se usan las Service Accounts?

- Permitir que los pods accedan a la API de Kubernetes de forma segura y controlada.
- Asociar permisos específicos a aplicaciones mediante RBAC.
- Separar los permisos de las aplicaciones de los usuarios humanos.
- Facilitar la rotación y gestión de credenciales de acceso.

Por defecto, cada namespace tiene una Service Account llamada `default`, pero es recomendable crear cuentas específicas para cada aplicación o componente.

==== Ejemplo de creación y uso de una Service Account

.1. Crea una Service Account personalizada:
[source,yaml]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mi-app-sa
  namespace: produccion
----

.2. Asocia la Service Account a un pod o deployment:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mi-app
  namespace: produccion
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mi-app
  template:
    metadata:
      labels:
        app: mi-app
    spec:
      serviceAccountName: mi-app-sa
      containers:
        - name: app
          image: nginx
----

.3. Otorga permisos a la Service Account usando RBAC:
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: lector-configmaps
  namespace: produccion
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vincular-lector-configmaps
  namespace: produccion
subjects:
  - kind: ServiceAccount
    name: mi-app-sa
    namespace: produccion
roleRef:
  kind: Role
  name: lector-configmaps
  apiGroup: rbac.authorization.k8s.io
----

Con esta configuración, los pods del deployment `mi-app` podrán listar y obtener ConfigMaps en el namespace `produccion`, pero no tendrán permisos adicionales.

==== Buenas prácticas

.Estas son algunas recomendaciones para gestionar Service Accounts de forma segura:
- Crea una Service Account específica para cada aplicación o componente que requiera acceso a la API.
- Usa RBAC para limitar los permisos de cada Service Account al mínimo necesario (principio de menor privilegio).
- No uses la Service Account `default` para aplicaciones en producción.
- Rota y gestiona los tokens de acceso de las Service Accounts según las políticas de seguridad de tu organización.
- Supervisa el uso de Service Accounts y revisa los permisos asignados periódicamente.


=== Network Policies

Las *Network Policies* en Kubernetes son recursos que permiten controlar el tráfico de red entre pods y otros recursos dentro del clúster. Son esenciales para mejorar la seguridad, ya que definen de manera declarativa qué conexiones están permitidas y cuáles no, limitando la superficie de ataque y el movimiento lateral en caso de compromisos.

==== ¿Qué es una NetworkPolicy?

Una NetworkPolicy es un objeto de Kubernetes que especifica cómo los pods pueden comunicarse entre sí y con otros endpoints (otros pods, namespaces, direcciones IP externas, etc.). Por defecto, si no hay ninguna política aplicada, todo el tráfico entre pods está permitido. Al aplicar una NetworkPolicy, solo el tráfico explícitamente permitido por las reglas será aceptado; el resto será bloqueado.

==== ¿Cómo funcionan las NetworkPolicies?

Las NetworkPolicies funcionan seleccionando pods mediante etiquetas (`labels`) y definiendo reglas de entrada (`ingress`) y salida (`egress`). Estas reglas pueden restringir el tráfico según:

- El origen o destino (otros pods, namespaces, IPs).
- El puerto y el protocolo.
- Las etiquetas de los pods.

Para que las NetworkPolicies tengan efecto, el clúster debe usar un plugin de red compatible con este recurso, como Calico, Cilium o Weave.

==== Ejemplo básico: Permitir solo tráfico de frontend a backend

Supón que tienes dos aplicaciones: un frontend y un backend, cada una en su propio pod. Solo quieres permitir que el frontend se comunique con el backend, y bloquear el resto del tráfico.

.Primero, define los pods con sus etiquetas:
[source,yaml]
----
# pod-frontend.yml
apiVersion: v1
kind: Pod
metadata:
  name: frontend
  labels:
    app: frontend
spec:
  containers:
    - name: app
      image: nginx
      ports:
        - containerPort: 80
----
[source,yaml]
----
# pod-backend.yml
apiVersion: v1
kind: Pod
metadata:
  name: backend
  labels:
    app: backend
spec:
  containers:
    - name: app
      image: nginx
      ports:
        - containerPort: 80
----

.Ahora, crea la NetworkPolicy para permitir solo el tráfico de los pods con la etiqueta `app: frontend` hacia los pods con la etiqueta `app: backend`:
[source,yaml]
----
# networkpolicy-backend.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-frontend-to-backend
spec:
  podSelector:
    matchLabels:
      app: backend
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: frontend
      ports:
        - protocol: TCP
          port: 80
----

.Con esta configuración:
- Solo los pods con la etiqueta `app: frontend` podrán acceder al puerto 80 de los pods con la etiqueta `app: backend`.
- Todo el tráfico de entrada desde otros pods o fuentes será bloqueado.

==== Ejemplo avanzado: Permitir tráfico de salida solo a Internet

Supón que quieres que tus pods solo puedan acceder a Internet (por ejemplo, para actualizaciones o APIs externas), pero no a otros pods del clúster.

[source,yaml]
----
# networkpolicy-egress-internet.yml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-egress-to-internet
spec:
  podSelector: {} # Aplica a todos los pods del namespace
  policyTypes:
    - Egress
  egress:
    - to:
        - ipBlock:
            cidr: 0.0.0.0/0
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 80
----

.Esta política:
- Permite a todos los pods del namespace realizar conexiones de salida (egress) solo a cualquier dirección IP (Internet) en los puertos 80 y 443.
- Bloquea el tráfico de salida a otros pods del clúster, a menos que se definan reglas adicionales.

==== Tipos de reglas

- **Ingress**: Controla el tráfico entrante a los pods seleccionados.
- **Egress**: Controla el tráfico saliente desde los pods seleccionados.

==== Buenas prácticas

- Aplica el principio de menor privilegio: solo permite el tráfico necesario.
- Prueba las políticas en entornos de desarrollo antes de aplicarlas en producción.
- Documenta las reglas y mantenlas actualizadas.
- Supervisa el tráfico y revisa los logs para detectar intentos de acceso no autorizados.

==== Diagrama explicativo

[plantuml, network-policy-example, png]
----
@startuml
node "Namespace" {
  [Pod frontend] -down- [Pod backend]
}
[Pod frontend] --> [Pod backend] : Permitido
actor "Otro Pod" as Otro
Otro -x [Pod backend] : Denegado
@enduml
----

=== Pod Security Policies

Las *Pod Security Policies* (PSP) fueron un mecanismo de Kubernetes para controlar la seguridad a nivel de pod, definiendo qué características de seguridad están permitidas al crear pods en el clúster. Aunque PSP está oficialmente deprecated desde Kubernetes 1.21 y eliminado en 1.25, es importante conocer su funcionamiento y las alternativas modernas.

==== ¿Qué es una Pod Security Policy?

Una PSP es un recurso que especifica un conjunto de condiciones que los pods deben cumplir para ser aceptados por el clúster. Permite restringir aspectos como:

- Uso de privilegios (privileged)
- Montaje de volúmenes hostPath
- Usuarios y grupos de ejecución (runAsUser, fsGroup)
- Capabilities de Linux permitidas o bloqueadas
- Acceso a la red, puertos, seccomp, AppArmor, etc.

==== Ejemplo de Pod Security Policy

El siguiente ejemplo solo permite pods que no sean privilegiados, no usen hostPath y ejecuten como un usuario no root:

[source,yaml]
----
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: ejemplo-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
----

Para que una PSP tenga efecto, debe asociarse a usuarios o ServiceAccounts mediante RBAC:

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: usar-psp
  namespace: produccion
rules:
  - apiGroups: ['policy']
    resources: ['podsecuritypolicies']
    verbs: ['use']
    resourceNames: ['ejemplo-psp']
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: vincular-psp
  namespace: produccion
subjects:
  - kind: ServiceAccount
    name: mi-app-sa
    namespace: produccion
roleRef:
  kind: Role
  name: usar-psp
  apiGroup: rbac.authorization.k8s.io
----

==== Limitaciones y estado actual

- PSP está deprecated y eliminado en versiones recientes de Kubernetes.
- Su configuración era compleja y difícil de mantener en clústeres grandes.
- No cubría todos los escenarios de seguridad y podía ser bypassed accidentalmente.

==== Alternativas modernas: Pod Security Admission (PSA)

Kubernetes recomienda usar el *Pod Security Admission* (PSA), disponible desde la versión 1.22, que implementa tres niveles de políticas predefinidas:

- **Privileged**: Sin restricciones (para pods de sistema o administración).
- **Baseline**: Permite buenas prácticas mínimas de seguridad.
- **Restricted**: Aplica las restricciones más estrictas (no root, sin privilegios, sin hostPath, etc.).

La configuración se realiza mediante labels en los namespaces:

[source,bash]
----
kubectl label namespace produccion pod-security.kubernetes.io/enforce=restricted
kubectl label namespace produccion pod-security.kubernetes.io/audit=baseline
kubectl label namespace produccion pod-security.kubernetes.io/warn=restricted
----

==== Buenas prácticas

.Las siguientes son algunas recomendaciones para implementar políticas de seguridad efectivas:
- Usa PSA en lugar de PSP en clústeres modernos.
- Aplica el nivel `restricted` en producción para máxima seguridad.
- Documenta y revisa las políticas aplicadas en cada namespace.
- Complementa con herramientas como OPA Gatekeeper, Kyverno o Kubewarden para políticas personalizadas.

=== Escaneo de vulnerabilidades en contenedores

El escaneo de vulnerabilidades en contenedores es una práctica esencial para garantizar la seguridad de las aplicaciones desplegadas en Kubernetes. Consiste en analizar las imágenes de contenedor en busca de vulnerabilidades conocidas en los sistemas operativos, librerías y dependencias incluidas en la imagen.

==== ¿Por qué es importante el escaneo de vulnerabilidades?

- Las imágenes de contenedor pueden contener vulnerabilidades heredadas de sus bases o dependencias.
- El escaneo ayuda a identificar y corregir problemas antes de que lleguen a producción.
- Permite cumplir con normativas y estándares de seguridad (como CIS, NIST, PCI-DSS).
- Reduce el riesgo de explotación de vulnerabilidades conocidas.

==== Herramientas populares de escaneo

- **Trivy**: Sencilla, rápida y soporta múltiples fuentes de vulnerabilidades.
- **Clair**: Integrable con registries y plataformas CI/CD.
- **Anchore Engine**: Ofrece análisis profundo y políticas personalizadas.
- **Grype**: Escáner de vulnerabilidades de Syft, fácil de integrar.
- **Aqua Microscanner** y **Snyk**: Soluciones comerciales y SaaS con integración CI/CD.

==== Ejemplo de escaneo con Trivy

.Trivy es una de las herramientas más populares y fáciles de usar para escanear imágenes de contenedor.

.Instala Trivy:
[source,bash]
----
sudo apt-get install trivy
# o usando Homebrew:
brew install aquasecurity/trivy/trivy
----

.Escanea una imagen local o remota:
[source,bash]
----
trivy image nginx:latest
----

.El resultado mostrará las vulnerabilidades encontradas, su severidad y recomendaciones de actualización.

==== Integración en pipelines CI/CD

El escaneo de vulnerabilidades debe integrarse en el ciclo de vida de desarrollo y despliegue:

- Añade pasos de escaneo en pipelines de CI (GitLab CI, GitHub Actions, Jenkins, etc.).
- Falla el pipeline si se detectan vulnerabilidades críticas o altas.
- Genera reportes automáticos y alertas para el equipo de desarrollo.

.Ejemplo de paso en GitLab CI:
[source,yaml]
----
scan:
  image: aquasec/trivy:latest
  script:
    - trivy image $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
----

==== Escaneo continuo en Kubernetes

Además del escaneo previo al despliegue, es recomendable realizar escaneo continuo de las imágenes en uso en el clúster:

- Usa herramientas como **Kube-bench** para auditar la configuración del clúster.
- Implementa soluciones como **Trivy Operator** o **Starboard** para escanear imágenes de pods en ejecución y generar reportes como recursos de Kubernetes.
- Configura alertas para nuevas vulnerabilidades descubiertas en imágenes ya desplegadas.

==== Buenas prácticas

- Escanea todas las imágenes antes de subirlas al registry y antes de desplegarlas.
- Mantén las imágenes y dependencias actualizadas.
- Usa imágenes oficiales y minimizadas (distroless, Alpine, etc.).
- Elimina dependencias y paquetes innecesarios de las imágenes.
- Automatiza el escaneo y la gestión de vulnerabilidades en el pipeline de CI/CD.
- Documenta y sigue un proceso de remediación para vulnerabilidades detectadas.

== Módulo 9: Monitorización y Observabilidad

La monitorización y la observabilidad son fundamentales para operar aplicaciones y clústeres Kubernetes en producción. Permiten detectar problemas, analizar el comportamiento de los sistemas y anticipar incidencias antes de que afecten a los usuarios. Este módulo cubre las herramientas y prácticas clave para lograr una observabilidad completa en Kubernetes.

=== Stack de monitorización (Prometheus)

Prometheus es la herramienta estándar para la monitorización de clústeres Kubernetes. Permite recolectar, almacenar y consultar métricas de los nodos, pods, servicios y aplicaciones, facilitando la detección de problemas y el análisis de tendencias.

==== ¿Cómo funciona Prometheus en Kubernetes?

- Prometheus se despliega como un pod o conjunto de pods dentro del clúster.
- Descubre automáticamente los endpoints de métricas de los componentes de Kubernetes y de las aplicaciones instrumentadas.
- Almacena las métricas en una base de datos de series temporales.
- Permite definir alertas y consultas usando PromQL.

==== Despliegue de Prometheus con Helm

La forma más sencilla y recomendada de instalar Prometheus en Kubernetes es usando el chart oficial de Helm, que incluye Prometheus, Alertmanager, node-exporter y kube-state-metrics.

[source,bash]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack
----

Esto desplegará todo el stack de monitorización en el namespace por defecto. Puedes personalizar la instalación usando valores de Helm.

==== Exponer la interfaz web de Prometheus

Para acceder a la interfaz web de Prometheus desde tu máquina local:

[source,bash]
----
kubectl port-forward svc/prometheus-kube-prometheus-prometheus 9090:9090
# Luego abre http://localhost:9090 en tu navegador
----

==== Monitorización de aplicaciones personalizadas

Para que Prometheus recolecte métricas de tus aplicaciones, estas deben exponer un endpoint `/metrics` en formato Prometheus. Puedes usar librerías como `prometheus-client` en Python, Go, Java, etc.

.Ejemplo de ServiceMonitor para una aplicación:
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: mi-app-monitor
  labels:
    release: prometheus
spec:
  selector:
    matchLabels:
      app: mi-app
  endpoints:
    - port: http
      path: /metrics
      interval: 15s
----

Asegúrate de que el Service de tu aplicación tenga el label `app: mi-app` y exponga el puerto adecuado.

==== Alertas con Alertmanager

Prometheus puede enviar alertas a través de Alertmanager cuando se cumplen ciertas condiciones (por ejemplo, alto uso de CPU, pods caídos, etc.).

.Ejemplo de regla de alerta:
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: high-cpu-usage
  labels:
    release: prometheus
spec:
  groups:
    - name: example.rules
      rules:
        - alert: HighCpuUsage
          expr: sum(rate(container_cpu_usage_seconds_total{image!=""}[5m])) by (pod) > 0.8
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Alto uso de CPU en el pod"
            description: "El pod {{ $labels.pod }} está usando más del 80% de CPU durante 5 minutos."
----

Alertmanager puede integrarse con email, Slack, PagerDuty y otros sistemas de notificación.

==== Buenas prácticas

- Instrumenta tus aplicaciones para exponer métricas relevantes.
- Usa ServiceMonitors para descubrir y recolectar métricas de servicios personalizados.
- Define alertas proactivas para anticipar problemas.
- Protege el acceso a la interfaz de Prometheus y Alertmanager.
- Almacena las métricas en un almacenamiento persistente si necesitas retención a largo plazo.

=== Visualización con Grafana

*Grafana* es la herramienta más popular para la visualización de métricas y datos de monitorización en Kubernetes. Permite crear dashboards interactivos y personalizables a partir de las métricas recolectadas por Prometheus u otras fuentes, facilitando el análisis visual del estado y rendimiento del clúster y las aplicaciones.

==== ¿Cómo funciona Grafana en Kubernetes?

- Grafana se despliega como un pod dentro del clúster, generalmente junto con Prometheus.
- Se conecta a Prometheus (u otras fuentes de datos) para consultar y graficar métricas.
- Permite crear dashboards personalizados para visualizar el uso de CPU, memoria, estado de pods, tráfico de red, latencia, etc.
- Soporta alertas visuales y notificaciones integradas.

==== Despliegue de Grafana con Helm

La forma más sencilla de instalar Grafana en Kubernetes es usando el mismo chart de Helm que instala Prometheus (kube-prometheus-stack), ya que incluye Grafana preconfigurado.

[source,bash]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack
----

Esto desplegará Grafana junto con Prometheus y otros componentes en el clúster.

==== Acceso a la interfaz de Grafana

Para acceder a la interfaz web de Grafana desde tu máquina local:

[source,bash]
----
kubectl port-forward svc/prometheus-grafana 3000:80
# Luego abre http://localhost:3000 en tu navegador
----

El usuario y contraseña por defecto suelen ser `admin` / `prom-operator` (puedes consultarlo en los secrets del namespace).

==== Dashboards recomendados

- *Kubernetes / Compute Resources / Cluster*: Uso de CPU y memoria por nodo y por pod.
- *Kubernetes / Networking / Namespace (Pods)*: Tráfico de red por namespace y pod.
- *Kubernetes / Resources / Persistent Volumes*: Estado y uso de volúmenes persistentes.
- *Custom dashboards*: Puedes crear dashboards personalizados para tus aplicaciones, servicios o métricas específicas.

Puedes importar dashboards desde https://grafana.com/grafana/dashboards/ usando el ID correspondiente.

==== Ejemplo de configuración de datasource en Grafana

Si despliegas Grafana por separado, debes configurar Prometheus como fuente de datos:

.1. Accede a Grafana y ve a *Configuration > Data Sources*.
.2. Añade una nueva fuente de tipo *Prometheus*.
.3. Usa la URL interna del servicio Prometheus, por ejemplo: `http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090`.

==== Buenas prácticas

- Protege el acceso a Grafana con autenticación y, si es posible, Single Sign-On (SSO).
- Usa dashboards compartidos para equipos de desarrollo, operaciones y negocio.
- Automatiza la provisión de dashboards usando ConfigMaps o el API de Grafana.
- Configura alertas visuales y notificaciones para eventos críticos.
- Versiona y documenta los dashboards más importantes.


=== Visualización con Grafana

*Grafana* es la herramienta más popular para la visualización de métricas y datos de monitorización en Kubernetes. Permite crear dashboards interactivos y personalizables a partir de las métricas recolectadas por Prometheus u otras fuentes, facilitando el análisis visual del estado y rendimiento del clúster y las aplicaciones.

==== ¿Cómo funciona Grafana en Kubernetes?

- Grafana se despliega como un pod dentro del clúster, generalmente junto con Prometheus.
- Se conecta a Prometheus (u otras fuentes de datos) para consultar y graficar métricas.
- Permite crear dashboards personalizados para visualizar el uso de CPU, memoria, estado de pods, tráfico de red, latencia, etc.
- Soporta alertas visuales y notificaciones integradas.

==== Despliegue de Grafana con Helm

La forma más sencilla de instalar Grafana en Kubernetes es usando el mismo chart de Helm que instala Prometheus (kube-prometheus-stack), ya que incluye Grafana preconfigurado.

[source,bash]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
helm install prometheus prometheus-community/kube-prometheus-stack
----

Esto desplegará Grafana junto con Prometheus y otros componentes en el clúster.

==== Acceso a la interfaz de Grafana

Para acceder a la interfaz web de Grafana desde tu máquina local:

[source,bash]
----
kubectl port-forward svc/prometheus-grafana 3000:80
# Luego abre http://localhost:3000 en tu navegador
----

El usuario y contraseña por defecto suelen ser `admin` / `prom-operator` (puedes consultarlo en los secrets del namespace).

==== Dashboards recomendados

- *Kubernetes / Compute Resources / Cluster*: Uso de CPU y memoria por nodo y por pod.
- *Kubernetes / Networking / Namespace (Pods)*: Tráfico de red por namespace y pod.
- *Kubernetes / Resources / Persistent Volumes*: Estado y uso de volúmenes persistentes.
- *Custom dashboards*: Puedes crear dashboards personalizados para tus aplicaciones, servicios o métricas específicas.

Puedes importar dashboards desde https://grafana.com/grafana/dashboards/ usando el ID correspondiente.

==== Ejemplo de configuración de datasource en Grafana

Si despliegas Grafana por separado, debes configurar Prometheus como fuente de datos:

.1. Accede a Grafana y ve a *Configuration > Data Sources*.
.2. Añade una nueva fuente de tipo *Prometheus*.
.3. Usa la URL interna del servicio Prometheus, por ejemplo: `http://prometheus-kube-prometheus-prometheus.monitoring.svc.cluster.local:9090`.

==== Buenas prácticas

- Protege el acceso a Grafana con autenticación y, si es posible, Single Sign-On (SSO).
- Usa dashboards compartidos para equipos de desarrollo, operaciones y negocio.
- Automatiza la provisión de dashboards usando ConfigMaps o el API de Grafana.
- Configura alertas visuales y notificaciones para eventos críticos.
- Versiona y documenta los dashboards más importantes.

=== Trazabilidad con Jaeger/Zipkin

La trazabilidad (tracing) es un pilar fundamental de la observabilidad en arquitecturas de microservicios. Permite seguir el recorrido de una petición a través de los diferentes servicios y componentes, identificando cuellos de botella, errores y latencias. En Kubernetes, las herramientas más populares para trazabilidad distribuida son *Jaeger* y *Zipkin*.

==== ¿Por qué es importante la trazabilidad?

- Permite visualizar el flujo de las peticiones entre microservicios.
- Ayuda a identificar servicios lentos o problemáticos.
- Facilita el análisis de dependencias y la detección de errores en entornos complejos.
- Es clave para el troubleshooting y la optimización de aplicaciones distribuidas.

==== Jaeger y Zipkin: características principales

- **Jaeger**: Proyecto de la CNCF, soporta almacenamiento escalable, visualización avanzada y múltiples backends. Se integra fácilmente con Istio, OpenTelemetry y otros frameworks.
- **Zipkin**: Ligero y sencillo, ideal para entornos pequeños o pruebas rápidas. Compatible con OpenTracing y OpenTelemetry.

Ambas herramientas recolectan y almacenan *spans* (fragmentos de trazas) generados por las aplicaciones instrumentadas.

==== Despliegue de Jaeger en Kubernetes

La forma más sencilla de desplegar Jaeger es usando el operador oficial o Helm.

[source,bash]
----
kubectl create namespace observabilidad
kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.56.0/jaeger-operator.yaml -n observabilidad
----

.Crear una instancia de Jaeger:
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger-demo
  namespace: observabilidad
spec:
  strategy: allInOne
----

.Acceso a la interfaz web de Jaeger:
[source,bash]
----
kubectl port-forward service/jaeger-demo-query -n observabilidad 16686:16686
# Luego abre http://localhost:16686 en tu navegador
----

==== Despliegue de Zipkin en Kubernetes

Puedes desplegar Zipkin usando un manifiesto oficial o Helm:

[source,bash]
----
kubectl create deployment zipkin --image=openzipkin/zipkin
kubectl expose deployment zipkin --port=9411
kubectl port-forward svc/zipkin 9411:9411
# Accede a http://localhost:9411
----

==== Instrumentación de aplicaciones

Para aprovechar Jaeger o Zipkin, tus aplicaciones deben estar instrumentadas para generar trazas. Puedes usar librerías de OpenTelemetry, OpenTracing o clientes específicos para Jaeger/Zipkin en tu lenguaje de programación.

.Ejemplo básico en Python con OpenTelemetry:
[source,python]
----
from opentelemetry import trace
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

trace.set_tracer_provider(TracerProvider())
jaeger_exporter = JaegerExporter(agent_host_name="jaeger-demo-agent", agent_port=6831)
trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(jaeger_exporter))

tracer = trace.get_tracer(__name__)
with tracer.start_as_current_span("mi-operacion"):
    # Lógica de la aplicación
    pass
----

En otros lenguajes (Java, Go, Node.js, etc.) existen librerías equivalentes.

==== Integración con Service Mesh (Istio)

Si usas Istio, la inyección automática de sidecars Envoy permite recolectar trazas de todo el tráfico entre servicios sin modificar el código de las aplicaciones. Solo debes habilitar la integración con Jaeger o Zipkin en la configuración de Istio.

[source,bash]
----
istioctl install --set values.tracing.enabled=true
----

==== Buenas prácticas

- Instrumenta todos los servicios críticos para obtener trazas completas.
- Usa headers estándar (`traceparent`, `b3`) para propagar el contexto de trazas entre servicios.
- Almacena las trazas el tiempo suficiente para análisis y troubleshooting, pero define políticas de retención.
- Integra la trazabilidad con tus dashboards de observabilidad (Grafana, Kiali, etc.).
- Supervisa el impacto en el rendimiento y ajusta el muestreo de trazas según la carga.

==== Resumen

La trazabilidad distribuida con Jaeger o Zipkin permite visualizar y analizar el flujo de peticiones en arquitecturas de microservicios sobre Kubernetes. Su integración facilita la detección de problemas, la optimización de la experiencia de usuario y la mejora continua de las aplicaciones.

=== Alertas y notificaciones

Las alertas y notificaciones son componentes clave de la observabilidad en Kubernetes, ya que permiten detectar y responder rápidamente a incidentes, anomalías o degradaciones en el clúster y las aplicaciones. Un sistema de alertas bien configurado ayuda a minimizar el tiempo de inactividad y a mejorar la fiabilidad operativa.

==== ¿Cómo funcionan las alertas en Kubernetes?

Las alertas suelen configurarse sobre métricas recolectadas por Prometheus. Cuando una métrica supera un umbral definido (por ejemplo, uso de CPU alto, pods no disponibles, errores en aplicaciones), Prometheus genera una alerta que es gestionada por Alertmanager.

Alertmanager es el componente encargado de:
- Agrupar, silenciar y deduplicar alertas.
- Enviar notificaciones a canales como email, Slack, Microsoft Teams, PagerDuty, Opsgenie, Webhooks, etc.
- Aplicar reglas de enrutamiento y escalado de alertas.

==== Ejemplo de regla de alerta en Prometheus

[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: reglas-alerta
  labels:
    release: prometheus
spec:
  groups:
    - name: reglas.ejemplo
      rules:
        - alert: PodDown
          expr: kube_pod_status_phase{phase="Failed"} > 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "Pod fallido detectado"
            description: "Uno o más pods están en estado Failed por más de 2 minutos."
        - alert: HighMemoryUsage
          expr: sum(container_memory_usage_bytes{container!=""}) by (pod) > 500000000
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Alto uso de memoria"
            description: "El pod {{ $labels.pod }} está usando más de 500MB de memoria durante 5 minutos."
----

==== Configuración básica de Alertmanager

Alertmanager se despliega junto con Prometheus (por ejemplo, usando kube-prometheus-stack) y se configura mediante un archivo YAML.

.Ejemplo de configuración para enviar alertas por email y Slack:
[source,yaml]
----
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'alertas@miempresa.com'
  smtp_auth_username: 'alertas@miempresa.com'
  smtp_auth_password: 'contraseña'

route:
  receiver: 'equipo-devops'

receivers:
  - name: 'equipo-devops'
    email_configs:
      - to: 'devops@miempresa.com'
    slack_configs:
      - api_url: 'https://hooks.slack.com/services/XXX/YYY/ZZZ'
        channel: '#alertas-k8s'
----

Puedes montar este archivo como ConfigMap en el pod de Alertmanager.

==== Acceso y pruebas de Alertmanager

Para acceder a la interfaz web de Alertmanager:

[source,bash]
----
kubectl port-forward svc/prometheus-kube-prometheus-alertmanager 9093:9093
# Luego abre http://localhost:9093 en tu navegador
----

Para probar una alerta, puedes forzar una condición o usar la API de Alertmanager para enviar alertas manualmente.

==== Buenas prácticas

.Aquí hay algunas recomendaciones para configurar alertas efectivas en Kubernetes:
- Define alertas para condiciones críticas (caída de pods, uso excesivo de recursos, errores de aplicaciones, etc.).
- Ajusta los umbrales y tiempos (`for:`) para evitar alertas falsas o ruido excesivo.
- Usa etiquetas (`severity`, `team`, etc.) para clasificar y enrutar alertas según su importancia.
- Silencia alertas durante mantenimientos programados para evitar notificaciones innecesarias.
- Integra Alertmanager con los canales de comunicación de tu equipo (Slack, email, PagerDuty, etc.).
- Documenta las alertas y su significado para facilitar la respuesta y el troubleshooting.


== Módulo 10: Helm: Gestor de Paquetes

Helm es el gestor de paquetes de facto para Kubernetes. Permite definir, instalar y actualizar aplicaciones complejas mediante "charts", que son plantillas reutilizables de recursos Kubernetes. Helm simplifica la gestión del ciclo de vida de aplicaciones, fomenta la reutilización y facilita la integración con pipelines de CI/CD.

=== Introducción a Helm

Helm es el gestor de paquetes más utilizado en Kubernetes. Permite definir, instalar, actualizar y gestionar aplicaciones complejas mediante "charts", que son plantillas reutilizables de recursos Kubernetes. Helm simplifica la gestión del ciclo de vida de las aplicaciones, fomenta la reutilización y facilita la integración con pipelines de CI/CD.

==== ¿Qué es un chart de Helm?

Un *chart* es un paquete que contiene:
- Plantillas YAML parametrizables para recursos de Kubernetes (Deployments, Services, ConfigMaps, etc.).
- Un archivo `values.yaml` con los valores por defecto y configurables.
- Metadatos (`Chart.yaml`) y documentación opcional (`README.md`).

Esto permite desplegar aplicaciones con diferentes configuraciones de forma sencilla y repetible.

==== Ventajas de usar Helm

- Instalación y actualización sencilla de aplicaciones complejas.
- Parametrización de despliegues mediante archivos de valores.
- Gestión de dependencias entre aplicaciones.
- Rollbacks automáticos en caso de errores.
- Reutilización y compartición de charts a través de repositorios públicos o privados.

==== Instalación de Helm

Puedes instalar Helm en tu máquina local con:

[source,bash]
----
curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
# o usando un gestor de paquetes:
sudo apt-get install helm
brew install helm
----

Verifica la instalación con:

[source,bash]
----
helm version
----

==== Primeros pasos con Helm

.1. Añade un repositorio de charts (por ejemplo, Bitnami):

[source,bash]
----
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update
----

.2. Busca un chart disponible:

[source,bash]
----
helm search repo bitnami/nginx
----

.3. Instala una aplicación usando un chart:

[source,bash]
----
helm install mi-nginx bitnami/nginx
----

.4. Lista los releases instalados:

[source,bash]
----
helm list
----

.5. Actualiza la configuración de un release:

[source,bash]
----
helm upgrade mi-nginx bitnami/nginx --set service.type=LoadBalancer
----

.6. Elimina un release:

[source,bash]
----
helm uninstall mi-nginx
----

==== Estructura básica de un chart

Al crear un chart personalizado, la estructura típica es:

----
mi-chart/
  Chart.yaml
  values.yaml
  templates/
    deployment.yaml
    service.yaml
    ...
----

Puedes crear un chart base con:

[source,bash]
----
helm create mi-chart
----

Esto genera una plantilla lista para personalizar.

==== Buenas prácticas

.Algunas recomendaciones para trabajar con Helm y charts:
- Versiona tus charts y almacénalos en un repositorio (por ejemplo, Git).
- Usa archivos `values.yaml` para separar la configuración del código.
- Documenta los parámetros configurables en el README del chart.
- Prueba los charts en entornos de desarrollo antes de usarlos en producción.
- Automatiza despliegues y actualizaciones usando Helm en tus pipelines de CI/CD.

=== Charts y repositorios

En Helm, un *chart* es un paquete que contiene todos los recursos necesarios para desplegar una aplicación en Kubernetes. Los charts pueden ser reutilizados, versionados y compartidos a través de repositorios públicos o privados, facilitando la gestión y el despliegue de aplicaciones complejas.

==== Estructura de un chart

Un chart típico tiene la siguiente estructura de directorios y archivos:

----
mi-chart/
  Chart.yaml         # Metadatos del chart (nombre, versión, descripción)
  values.yaml        # Valores por defecto y configurables
  templates/         # Plantillas YAML de recursos Kubernetes (Deployment, Service, etc.)
    deployment.yaml
    service.yaml
    ...
  charts/            # Dependencias de otros charts (opcional)
  README.md          # Documentación del chart (opcional)
----

- `Chart.yaml`: Define el nombre, versión, descripción y dependencias del chart.
- `values.yaml`: Permite parametrizar el despliegue, facilitando la reutilización y personalización.
- `templates/`: Contiene los manifiestos de Kubernetes como plantillas Go, que se rellenan con los valores definidos.

==== Repositorios de charts

Un *repositorio de charts* es una colección de charts accesibles mediante una URL. Existen repositorios públicos (como ArtifactHub, Bitnami, prometheus-community) y puedes crear repositorios privados para tu organización.

.Para añadir y listar repositorios:
[source,bash]
----
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo list
----

.Para buscar charts en los repositorios añadidos:
[source,bash]
----
helm search repo nginx
----

.Para actualizar la lista de charts disponibles:
[source,bash]
----
helm repo update
----

==== Instalación de aplicaciones desde un repositorio

Puedes instalar cualquier chart disponible en un repositorio con un solo comando:

[source,bash]
----
helm install mi-release bitnami/nginx
----

Puedes personalizar la instalación usando el parámetro `--set` o un archivo de valores propio:

[source,bash]
----
helm install mi-release bitnami/nginx --set service.type=LoadBalancer
# o
helm install mi-release bitnami/nginx -f valores-personalizados.yaml
----

==== Versionado y actualización de charts

Cada chart y cada release tiene una versión. Puedes instalar una versión específica de un chart:

[source,bash]
----
helm install mi-release bitnami/nginx --version 15.0.0
----

Para actualizar una aplicación instalada:

[source,bash]
----
helm upgrade mi-release bitnami/nginx --set replicaCount=3
----

==== Creación de repositorios privados

Puedes crear tu propio repositorio de charts usando un servidor web estático, GitHub Pages o herramientas como ChartMuseum.

.Ejemplo básico con ChartMuseum:
[source,bash]
----
docker run -d -p 8080:8080 \
  -v /ruta/a/mis/charts:/charts \
  chartmuseum/chartmuseum:latest \
  --storage-local-rootdir=/charts
----

Luego añade tu repositorio privado a Helm:

[source,bash]
----
helm repo add mi-repo http://localhost:8080
----

==== Buenas prácticas

.A continuación, algunas recomendaciones para trabajar con Helm y repositorios de charts:
- Versiona y documenta todos tus charts.
- Usa repositorios privados para aplicaciones internas o charts personalizados.
- Actualiza regularmente los repositorios para obtener mejoras y parches de seguridad.
- Revisa las dependencias y valores por defecto antes de instalar charts de terceros.
- Automatiza la publicación de charts en tus pipelines de CI/CD.

=== Creación de charts personalizados

La creación de charts personalizados en Helm te permite empaquetar, versionar y reutilizar la configuración de tus aplicaciones para Kubernetes, facilitando despliegues consistentes y automatizados en diferentes entornos.

==== Estructura básica de un chart

Al crear un chart, Helm genera una estructura de directorios estándar:

----
mi-chart/
  Chart.yaml         # Metadatos del chart (nombre, versión, descripción)
  values.yaml        # Valores por defecto y configurables
  templates/         # Plantillas YAML de recursos Kubernetes
    deployment.yaml
    service.yaml
    ...
  charts/            # Dependencias de otros charts (opcional)
  README.md          # Documentación del chart (opcional)
----

==== Creación de un chart desde cero

Puedes crear un nuevo chart con el comando:

[source,bash]
----
helm create mi-chart
----

Esto genera una plantilla básica con ejemplos de Deployment, Service y otros recursos en el directorio `templates/`.

==== Personalización de plantillas

Las plantillas en el directorio `templates/` usan la sintaxis de Go templating, lo que permite parametrizar cualquier campo usando los valores definidos en `values.yaml`.

.Ejemplo de fragmento de `deployment.yaml`:
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "mi-chart.fullname" . }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      app: {{ include "mi-chart.name" . }}
  template:
    metadata:
      labels:
        app: {{ include "mi-chart.name" . }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          ports:
            - containerPort: {{ .Values.service.port }}
----

.Ejemplo de `values.yaml`:
[source,yaml]
----
replicaCount: 2

image:
  repository: nginx
  tag: "1.25"

service:
  type: ClusterIP
  port: 80
----

Puedes añadir o modificar parámetros según las necesidades de tu aplicación.

==== Instalación y prueba del chart personalizado

Para instalar tu chart en el clúster:

[source,bash]
----
helm install mi-release ./mi-chart
----

Para probar cambios en los valores:

[source,bash]
----
helm install mi-release ./mi-chart --set replicaCount=3,image.tag=1.26
# o usando un archivo de valores personalizado
helm install mi-release ./mi-chart -f valores-dev.yaml
----

Para actualizar el chart tras cambios:

[source,bash]
----
helm upgrade mi-release ./mi-chart
----

==== Uso de helpers y funciones

En el archivo `_helpers.tpl` puedes definir funciones reutilizables para nombres, etiquetas, anotaciones, etc., mejorando la consistencia y evitando duplicación.

.Ejemplo de helper:
[source,gotemplate]
----
{{- define "mi-chart.fullname" -}}
{{ .Release.Name }}-{{ .Chart.Name }}
{{- end -}}
----

==== Buenas prácticas

.Para crear charts personalizados de calidad, sigue estas recomendaciones:
- Mantén los valores configurables en `values.yaml` y documenta cada parámetro.
- Usa helpers para nombres y etiquetas consistentes.
- Versiona y almacena tus charts en un repositorio (Git o ChartMuseum).
- Añade pruebas de linting con `helm lint` y pruebas de despliegue en entornos de staging.
- Documenta el uso y los parámetros en el `README.md` del chart.
- Reutiliza plantillas y subcharts para componentes comunes.

=== Gestión del ciclo de vida de aplicaciones

La gestión del ciclo de vida de aplicaciones en Kubernetes implica controlar todas las fases por las que pasa una aplicación: despliegue, actualización, escalado, monitorización, mantenimiento y eliminación. Kubernetes, junto con herramientas como Helm y sistemas de CI/CD, permite automatizar y simplificar estas tareas, asegurando despliegues consistentes, actualizaciones seguras y una operación eficiente.

==== Fases del ciclo de vida de una aplicación en Kubernetes

1. **Despliegue inicial**: 
   - Se crea el manifiesto YAML o el chart de Helm con los recursos necesarios (Deployments, Services, ConfigMaps, etc.).
   - Se aplica al clúster usando `kubectl apply -f` o `helm install`.
2. **Actualización**:
   - Se modifican los manifiestos o los valores de Helm para reflejar nuevas versiones, cambios de configuración o escalado.
   - Se aplican los cambios con `kubectl apply` o `helm upgrade`, aprovechando mecanismos como rolling updates para evitar downtime.
3. **Escalado**:
   - Se ajusta el número de réplicas manualmente (`kubectl scale`) o automáticamente mediante HPA/VPA.
   - Se puede escalar horizontalmente (más pods) o verticalmente (más recursos por pod).
4. **Monitorización y observabilidad**:
   - Se recolectan métricas, logs y trazas usando Prometheus, Grafana, EFK/ELK, Jaeger, etc.
   - Se configuran alertas y dashboards para anticipar y detectar problemas.
5. **Mantenimiento**:
   - Se gestionan configuraciones y secretos de forma segura.
   - Se aplican parches de seguridad y actualizaciones de imágenes.
   - Se realiza limpieza de recursos obsoletos y ajuste de cuotas.
6. **Rollback y recuperación**:
   - Si una actualización falla, se puede hacer rollback a una versión anterior con `kubectl rollout undo` o `helm rollback`.
   - Se restauran backups de datos y configuraciones si es necesario.
7. **Eliminación**:
   - Se eliminan los recursos con `kubectl delete` o `helm uninstall`.
   - Se limpian volúmenes, configuraciones y otros artefactos asociados.

==== Ejemplo de ciclo de vida con Helm

.1. Despliegue inicial:
[source,bash]
----
helm install mi-app ./mi-chart
----

.2. Actualización de la aplicación (por ejemplo, nueva imagen o configuración):
[source,bash]
----
helm upgrade mi-app ./mi-chart --set image.tag=2.0.0
----

.3. Escalado manual:
[source,bash]
----
kubectl scale deployment mi-app --replicas=5
----

.4. Rollback en caso de error:
[source,bash]
----
helm rollback mi-app 1
----

.5. Eliminación de la aplicación:
[source,bash]
----
helm uninstall mi-app
----

==== Integración con CI/CD

La gestión eficiente del ciclo de vida se potencia integrando Kubernetes con pipelines de CI/CD (por ejemplo, Jenkins, GitLab CI, GitHub Actions, ArgoCD, Flux):

- Automatiza el build, test y despliegue de aplicaciones.
- Aplica despliegues progresivos (canary, blue/green) y validaciones automáticas.
- Realiza rollbacks automáticos ante fallos detectados en producción.
- Versiona y audita todos los cambios en la infraestructura y la configuración.

.Ejemplo de pipeline básico en GitLab CI:
[source,yaml]
----
stages:
  - build
  - deploy

build:
  stage: build
  script:
    - docker build -t registry.example.com/mi-app:$CI_COMMIT_SHA .
    - docker push registry.example.com/mi-app:$CI_COMMIT_SHA

deploy:
  stage: deploy
  script:
    - helm upgrade --install mi-app ./mi-chart --set image.tag=$CI_COMMIT_SHA
  only:
    - main
----

==== Buenas prácticas

.Formas de gestionar el ciclo de vida de aplicaciones en Kubernetes:
- Versiona todos los manifiestos y charts en un sistema de control de versiones (Git).
- Automatiza despliegues y actualizaciones usando pipelines de CI/CD.
- Usa estrategias de despliegue seguras (rolling update, canary, blue/green).
- Supervisa el estado de las aplicaciones y configura alertas proactivas.
- Documenta los procedimientos de despliegue, actualización y rollback.
- Realiza pruebas de rollback y recuperación periódicamente.
- Elimina recursos obsoletos para evitar consumo innecesario de recursos.

=== Integraciones con CI/CD

La integración de Kubernetes con sistemas de CI/CD (Integración Continua y Despliegue Continuo) es fundamental para automatizar el ciclo de vida de las aplicaciones, garantizar despliegues repetibles y reducir errores humanos. Las pipelines de CI/CD permiten construir, testear, empaquetar y desplegar aplicaciones en Kubernetes de forma automática tras cada cambio en el código fuente.

==== Beneficios de la integración CI/CD con Kubernetes

- Automatización de builds, tests y despliegues.
- Despliegues consistentes y repetibles en diferentes entornos.
- Rollbacks automáticos ante fallos.
- Validación de cambios antes de llegar a producción.
- Auditoría y trazabilidad de todos los cambios.

==== Herramientas populares de CI/CD para Kubernetes

- **Jenkins**: Ampliamente usado, con plugins para Kubernetes y Helm.
- **GitLab CI/CD**: Integración nativa con Kubernetes, despliegue automático de aplicaciones y entornos review.
- **GitHub Actions**: Workflows YAML para construir y desplegar en Kubernetes.
- **ArgoCD** y **Flux**: Herramientas GitOps para despliegue continuo basado en cambios en el repositorio Git.
- **Tekton**: Framework nativo de Kubernetes para pipelines declarativas.

==== Ejemplo de pipeline de CI/CD con GitLab CI

.Ejemplo básico de `.gitlab-ci.yml` para construir una imagen y desplegar con Helm:
[source,yaml]
----
stages:
  - build
  - deploy

build:
  stage: build
  script:
    - docker build -t registry.example.com/mi-app:$CI_COMMIT_SHA .
    - docker push registry.example.com/mi-app:$CI_COMMIT_SHA

deploy:
  stage: deploy
  script:
    - helm upgrade --install mi-app ./chart --set image.tag=$CI_COMMIT_SHA
  only:
    - main
----

==== Ejemplo de pipeline con GitHub Actions

[source,yaml]
----
name: CI/CD

on:
  push:
    branches: [ "main" ]

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Build Docker image
        run: |
          docker build -t registry.example.com/mi-app:${{ github.sha }} .
          docker push registry.example.com/mi-app:${{ github.sha }}
      - name: Set up Kubeconfig
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.27.0'
      - name: Deploy with Helm
        run: |
          helm upgrade --install mi-app ./chart --set image.tag=${{ github.sha }}
----

==== GitOps: despliegue continuo basado en Git

Con herramientas como **ArgoCD** o **Flux**, el estado deseado del clúster se define en un repositorio Git. Los cambios en el repositorio (manifiestos YAML, charts de Helm, etc.) son detectados y aplicados automáticamente al clúster.

.Ventajas de GitOps:
- Auditoría y trazabilidad total de los cambios.
- Rollbacks sencillos mediante revert en Git.
- Sincronización automática entre Git y el clúster.

==== Buenas prácticas

.Aquí algunos consejos para integrar CI/CD con Kubernetes:
- Versiona todos los manifiestos y charts en el repositorio.
- Usa entornos separados (dev, staging, prod) y pipelines independientes.
- Automatiza pruebas antes del despliegue (lint, tests, validaciones de manifiestos).
- Protege los secretos y credenciales usando herramientas como Sealed Secrets, HashiCorp Vault o Kubernetes Secrets cifrados.
- Documenta el flujo de CI/CD y los procedimientos de rollback.

== Módulo 11: Operadores de Kubernetes

Los *Operadores* de Kubernetes son extensiones que automatizan la gestión de aplicaciones complejas y recursos personalizados dentro del clúster. Permiten encapsular el conocimiento operativo de un experto humano en software, facilitando tareas como despliegue, actualización, backup, escalado y recuperación de aplicaciones stateful o con lógica avanzada.

=== Concepto de operadores

Un *operador* de Kubernetes es una extensión que automatiza la gestión de aplicaciones complejas y recursos personalizados dentro del clúster. Los operadores encapsulan el conocimiento operativo de un experto humano en software, permitiendo que tareas como despliegue, actualización, backup, escalado y recuperación se realicen de forma automática y declarativa.

==== ¿Cómo funcionan los operadores?

- Los operadores se basan en el patrón controlador de Kubernetes: observan el estado de recursos personalizados (CRDs, Custom Resource Definitions) y ejecutan acciones para mantener el estado deseado.
- Un operador consta de:
  - Una o varias CRDs que definen nuevos tipos de recursos (por ejemplo, `PostgresCluster`, `KafkaTopic`).
  - Un controlador (controller) que observa estos recursos y ejecuta lógica personalizada (por ejemplo, crear pods, configurar backups, escalar instancias).
- El operador compara el estado actual del clúster con el estado deseado definido en los recursos personalizados y realiza los cambios necesarios para reconciliarlos.

==== Ventajas de usar operadores

- Automatización de tareas repetitivas y complejas.
- Gestión declarativa de aplicaciones stateful y servicios avanzados.
- Reducción de errores humanos y mejora de la consistencia operativa.
- Integración nativa con el ciclo de vida de Kubernetes (aplicar, actualizar, eliminar).
- Facilita la adopción de buenas prácticas y la estandarización de despliegues.

==== Ejemplo de uso de un operador

Supón que quieres gestionar una base de datos PostgreSQL de forma automatizada. Puedes instalar el *Postgres Operator* y crear un recurso personalizado como este:

[source,yaml]
----
apiVersion: postgres-operator.crunchydata.com/v1beta1
kind: PostgresCluster
metadata:
  name: mi-postgres
spec:
  instances:
    - name: instancia1
      replicas: 2
  backups:
    pgbackrest:
      repos:
        - name: repo1
          volume:
            size: 10Gi
----

El operador se encargará de crear los pods, servicios, backups y restauraciones según lo definido en el recurso `PostgresCluster`.

==== Buenas prácticas

.Los operadores son herramientas útiles, pero deben usarse con cuidado:
- Usa operadores oficiales o de la comunidad para aplicaciones complejas (bases de datos, colas, sistemas distribuidos).
- Versiona y documenta los recursos personalizados que definas.
- Supervisa el estado de los operadores y revisa los logs ante problemas.
- Limita los permisos de los operadores usando RBAC.
- Prueba los operadores en entornos de desarrollo antes de usarlos en producción.

=== Operator Framework

El *Operator Framework* es un conjunto de herramientas y librerías open source diseñado para facilitar la creación, prueba y gestión de operadores en Kubernetes. Proporciona una base estandarizada para desarrollar operadores robustos, escalables y mantenibles, acelerando el ciclo de vida de desarrollo y simplificando la integración con el ecosistema Kubernetes.

==== Componentes principales del Operator Framework

- **Operator SDK**: Herramienta CLI y librerías para crear, construir y empaquetar operadores en Go, Ansible o Helm. Permite generar la estructura básica del proyecto, gestionar CRDs y escribir la lógica de reconciliación.
- **Operator Lifecycle Manager (OLM)**: Gestiona la instalación, actualización y ciclo de vida de los operadores en el clúster. Facilita la publicación y el consumo de operadores a través de catálogos.
- **Operator Registry**: Almacena y distribuye catálogos de operadores, permitiendo a OLM descubrir e instalar operadores desde diferentes fuentes.

==== ¿Por qué usar Operator Framework?

- Estandariza el desarrollo y despliegue de operadores.
- Reduce la complejidad al abstraer detalles de bajo nivel.
- Permite elegir el lenguaje y enfoque más adecuado (Go, Ansible, Helm).
- Facilita la integración con OLM para la gestión del ciclo de vida y dependencias.

==== Ejemplo de creación de un operador con Operator SDK

.1. Instala Operator SDK:
[source,bash]
----
brew install operator-sdk
# o descarga desde https://github.com/operator-framework/operator-sdk
----

.2. Crea un nuevo proyecto de operador en Go:
[source,bash]
----
operator-sdk init --domain=miempresa.com --repo=github.com/miempresa/mi-operator
operator-sdk create api --group=apps --version=v1alpha1 --kind=DemoApp --resource --controller
----

.3. Implementa la lógica de reconciliación en el controlador generado (`controllers/demoapp_controller.go`).

.4. Genera y aplica la CRD:
[source,bash]
----
make install
----

.5. Despliega el operador en el clúster:
[source,bash]
----
make deploy
----

.6. Crea un recurso personalizado para probar el operador:
[source,yaml]
----
apiVersion: apps.miempresa.com/v1alpha1
kind: DemoApp
metadata:
  name: demoapp-ejemplo
spec:
  replicas: 2
----

==== Operator Lifecycle Manager (OLM)

OLM facilita la instalación, actualización y gestión de operadores y sus dependencias en el clúster. Permite publicar operadores en catálogos y gestionar su ciclo de vida de forma declarativa.

.Para instalar OLM:
[source,bash]
----
kubectl apply -f https://github.com/operator-framework/operator-lifecycle-manager/releases/latest/download/install.yaml
----

Luego puedes instalar operadores desde catálogos públicos (como OperatorHub.io) o privados.

==== Buenas prácticas
.Las siguientes recomendaciones te ayudarán a crear y gestionar operadores de forma efectiva:
- Usa Operator SDK para acelerar el desarrollo y seguir las mejores prácticas.
- Documenta las CRDs y la lógica de reconciliación.
- Prueba los operadores en entornos de staging antes de producción.
- Usa OLM para gestionar el ciclo de vida y actualizaciones de los operadores.
- Limita los permisos de los operadores mediante RBAC.

=== Casos de uso y ejemplos comunes

Los operadores de Kubernetes se utilizan para automatizar la gestión de aplicaciones y recursos complejos que requieren lógica operativa avanzada. A continuación se presentan algunos de los casos de uso más habituales y ejemplos prácticos:

==== 1. Gestión automatizada de bases de datos

Operadores como el *Postgres Operator*, *MongoDB Operator* o *MySQL Operator* permiten:

- Desplegar instancias de bases de datos de forma declarativa.
- Gestionar backups automáticos y restauraciones.
- Realizar actualizaciones y escalado sin intervención manual.
- Configurar alta disponibilidad y replicación.

.Ejemplo de recurso personalizado para PostgreSQL:
[source,yaml]
----
apiVersion: postgres-operator.crunchydata.com/v1beta1
kind: PostgresCluster
metadata:
  name: mi-postgres
spec:
  instances:
    - name: instancia1
      replicas: 2
  backups:
    pgbackrest:
      repos:
        - name: repo1
          volume:
            size: 10Gi
----

==== 2. Automatización de sistemas de mensajería y colas

Operadores como *Kafka Operator* o *RabbitMQ Cluster Operator* permiten:

- Crear y escalar clústeres de mensajería.
- Gestionar topics, usuarios y políticas de acceso.
- Automatizar actualizaciones y recuperación ante fallos.

.Ejemplo de recurso para Kafka:
[source,yaml]
----
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: mi-kafka
spec:
  kafka:
    replicas: 3
    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
  zookeeper:
    replicas: 3
  entityOperator:
    topicOperator: {}
    userOperator: {}
----

==== 3. Aplicaciones stateful y almacenamiento

Operadores como *Cassandra Operator*, *Elasticsearch Operator* o *Redis Operator* facilitan:

- Despliegue y escalado de clústeres stateful.
- Gestión de nodos, sharding y replicación.
- Automatización de backups y restauraciones.

==== 4. Automatización de certificados y seguridad

El *cert-manager* Operator automatiza la provisión y renovación de certificados TLS usando ACME (Let's Encrypt) u otros proveedores.

.Ejemplo de recurso Certificate:
[source,yaml]
----
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: mi-certificado
spec:
  secretName: mi-cert-tls
  dnsNames:
    - miapp.ejemplo.com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
----

==== 5. Integración con servicios cloud y SaaS

Operadores como *AWS Service Operator* o *Azure Service Operator* permiten gestionar recursos cloud (RDS, S3, CosmosDB, etc.) directamente desde Kubernetes, usando manifiestos YAML.

==== 6. Ejemplo básico: operador personalizado para reinicio de pods

Un operador sencillo puede observar un recurso personalizado (por ejemplo, `RestartPod`) y reiniciar un pod cuando se crea dicho recurso.

.Ejemplo de CRD:
[source,yaml]
----
apiVersion: myorg.com/v1
kind: RestartPod
metadata:
  name: reiniciar-mi-pod
spec:
  podName: mi-pod
----

El operador implementa la lógica para detectar este recurso y ejecutar `kubectl delete pod mi-pod` automáticamente.

==== Buenas prácticas

.Para trabajar con operadores de forma efectiva, considera las siguientes recomendaciones:
- Usa operadores oficiales o de la comunidad para aplicaciones críticas.
- Versiona y documenta los recursos personalizados y su uso.
- Supervisa el estado y logs de los operadores.
- Limita los permisos de los operadores mediante RBAC.
- Prueba los operadores en entornos de desarrollo antes de producción.

=== Creación de operadores básicos

Crear un operador básico en Kubernetes implica definir un recurso personalizado (CRD) y un controlador que observe y gestione ese recurso. Los operadores pueden desarrollarse en varios lenguajes y frameworks, pero el Operator SDK es la opción más popular y sencilla para empezar.

==== Pasos para crear un operador básico con Operator SDK

.1. Instala Operator SDK:
[source,bash]
----
brew install operator-sdk
# o descarga desde https://github.com/operator-framework/operator-sdk
----

.2. Inicializa un nuevo proyecto de operador (en Go, Ansible o Helm; aquí usamos Go):
[source,bash]
----
operator-sdk init --domain=miempresa.com --repo=github.com/miempresa/mi-operator
----

.3. Crea una API y un controlador para tu recurso personalizado:
[source,bash]
----
operator-sdk create api --group=apps --version=v1alpha1 --kind=DemoApp --resource --controller
----

Esto genera la definición de la CRD y la estructura del controlador.

.4. Implementa la lógica de reconciliación en el archivo `controllers/demoapp_controller.go`. Por ejemplo, puedes hacer que el operador cree un Deployment cuando detecte un recurso `DemoApp`.

.5. Genera y aplica la CRD en el clúster:
[source,bash]
----
make install
----

.6. Despliega el operador en el clúster:
[source,bash]
----
make deploy
----

.7. Crea un recurso personalizado para probar el operador:
[source,yaml]
----
apiVersion: apps.miempresa.com/v1alpha1
kind: DemoApp
metadata:
  name: demoapp-ejemplo
spec:
  replicas: 2
----

El operador observará este recurso y ejecutará la lógica definida (por ejemplo, crear un Deployment con 2 réplicas).

==== Ejemplo mínimo de CRD y lógica de reconciliación

.CRD (`config/crd/bases/apps.miempresa.com_demoapps.yaml`):
[source,yaml]
----
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: demoapps.apps.miempresa.com
spec:
  group: apps.miempresa.com
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                replicas:
                  type: integer
  scope: Namespaced
  names:
    plural: demoapps
    singular: demoapp
    kind: DemoApp
    shortNames:
      - da
----

.Lógica de reconciliación (Go, fragmento simplificado):
[source,go]
----
// controllers/demoapp_controller.go
func (r *DemoAppReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    var demoApp appsv1alpha1.DemoApp
    if err := r.Get(ctx, req.NamespacedName, &demoApp); err != nil {
        return ctrl.Result{}, client.IgnoreNotFound(err)
    }
    // Crear o actualizar un Deployment según el spec.replicas
    // ...
    return ctrl.Result{}, nil
}
----

==== Buenas prácticas

.Si quieres crear un operador básico, considera las siguientes recomendaciones:
- Empieza con un operador sencillo y ve añadiendo lógica según las necesidades.
- Documenta la CRD y el comportamiento esperado del operador.
- Usa pruebas unitarias y de integración para validar la lógica.
- Limita los permisos del operador mediante RBAC.
- Supervisa los logs y el estado del operador en el clúster.

=== Automatización de tareas complejas

Los operadores de Kubernetes permiten automatizar tareas operativas avanzadas que tradicionalmente requerían intervención manual o scripts externos. Gracias a su capacidad para observar el estado del clúster y ejecutar lógica personalizada, los operadores pueden gestionar procesos complejos de forma declarativa y repetible.

==== Ejemplos de automatización con operadores

- **Backups y restauraciones automáticas**: Operadores de bases de datos (Postgres, MongoDB, MySQL) pueden programar y ejecutar backups periódicos, así como restauraciones automáticas ante fallos.
- **Actualizaciones y migraciones sin downtime**: Operadores pueden gestionar actualizaciones de versiones de aplicaciones o bases de datos, aplicando migraciones de esquema y validando el estado antes y después del cambio.
- **Escalado avanzado y auto-recuperación**: Operadores pueden implementar lógica de escalado basada en métricas personalizadas, o reiniciar/reemplazar recursos ante fallos detectados.
- **Rotación y provisión de certificados**: Operadores como cert-manager automatizan la solicitud, renovación y distribución de certificados TLS para aplicaciones y servicios.
- **Gestión de dependencias entre recursos**: Operadores pueden orquestar la creación y configuración de recursos dependientes (por ejemplo, crear un PVC antes de desplegar una base de datos, o esperar a que un servicio esté disponible antes de lanzar una aplicación).

==== Ejemplo: backup automático con un operador de base de datos

[source,yaml]
----
apiVersion: postgres-operator.crunchydata.com/v1beta1
kind: PostgresCluster
metadata:
  name: mi-postgres
spec:
  backups:
    pgbackrest:
      repos:
        - name: repo1
          schedule: "0 2 * * *" # Backup diario a las 2 AM
          volume:
            size: 10Gi
----

El operador se encarga de ejecutar el backup según la programación definida y gestionar la retención y restauración de los datos.

==== Ejemplo: rotación automática de certificados con cert-manager

[source,yaml]
----
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: mi-certificado
spec:
  secretName: mi-cert-tls
  dnsNames:
    - miapp.ejemplo.com
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
----

cert-manager solicitará, renovará y almacenará automáticamente el certificado TLS en el Secret especificado.

==== Buenas prácticas

.Las siguientes recomendaciones te ayudarán a automatizar tareas complejas con operadores de forma efectiva:
- Define claramente los flujos de automatización y documenta los recursos personalizados utilizados.
- Supervisa el estado y los logs de los operadores para detectar errores o comportamientos inesperados.
- Limita los permisos de los operadores mediante RBAC para reducir riesgos de seguridad.
- Prueba exhaustivamente la lógica de automatización en entornos de desarrollo antes de aplicarla en producción.
- Mantén los operadores y sus dependencias actualizados para aprovechar mejoras y parches de seguridad.

== Módulo 12: CI/CD con Kubernetes

La integración de CI/CD (Integración Continua y Despliegue Continuo) con Kubernetes es esencial para automatizar el ciclo de vida de las aplicaciones, garantizar despliegues repetibles y acelerar la entrega de valor. Este módulo aborda las mejores prácticas, herramientas y patrones para implementar pipelines de CI/CD modernos sobre Kubernetes.

=== Pipelines de CI/CD para Kubernetes

Los pipelines de CI/CD (Integración Continua y Despliegue Continuo) son fundamentales para automatizar la construcción, prueba y despliegue de aplicaciones en Kubernetes. Permiten entregar cambios de forma rápida, segura y repetible, reduciendo errores humanos y acelerando la entrega de valor.

==== ¿Cómo funciona un pipeline de CI/CD para Kubernetes?

1. **Build**: El código fuente se construye y se empaqueta en una imagen de contenedor.
2. **Test**: Se ejecutan pruebas unitarias, de integración y de seguridad sobre la imagen.
3. **Push**: La imagen se sube a un registro de contenedores (Docker Hub, GitLab Registry, ECR, etc.).
4. **Deploy**: Se actualizan los manifiestos YAML o charts de Helm y se despliega la nueva versión en el clúster Kubernetes.
5. **Validación**: Se realizan pruebas post-despliegue y validaciones automáticas.
6. **Rollback**: Si se detectan errores, el pipeline puede revertir automáticamente a la versión anterior.

==== Herramientas populares para pipelines de CI/CD en Kubernetes

- **Jenkins**: Con plugins para Kubernetes y Helm.
- **GitLab CI/CD**: Integración nativa con clústeres Kubernetes y despliegue automático.
- **GitHub Actions**: Workflows YAML para construir y desplegar en Kubernetes.
- **Tekton**: Framework nativo de Kubernetes para pipelines declarativas.
- **Argo Workflows**: Pipelines nativos para Kubernetes, ideales para flujos complejos.

==== Ejemplo básico de pipeline con GitLab CI

[source,yaml]
----
stages:
  - build
  - deploy

build:
  stage: build
  script:
    - docker build -t registry.example.com/mi-app:$CI_COMMIT_SHA .
    - docker push registry.example.com/mi-app:$CI_COMMIT_SHA

deploy:
  stage: deploy
  script:
    - helm upgrade --install mi-app ./chart --set image.tag=$CI_COMMIT_SHA
  only:
    - main
----

==== Ejemplo básico de pipeline con GitHub Actions

[source,yaml]
----
name: CI/CD

on:
  push:
    branches: [ "main" ]

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Build Docker image
        run: |
          docker build -t registry.example.com/mi-app:${{ github.sha }} .
          docker push registry.example.com/mi-app:${{ github.sha }}
      - name: Set up Kubeconfig
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.27.0'
      - name: Deploy with Helm
        run: |
          helm upgrade --install mi-app ./chart --set image.tag=${{ github.sha }}
----

==== Buenas prácticas

.En la implementación de pipelines de CI/CD para Kubernetes, considera las siguientes recomendaciones:
- Versiona todos los manifiestos y charts en el repositorio.
- Usa pipelines independientes para cada entorno (dev, staging, prod).
- Automatiza pruebas antes del despliegue (lint, tests, validaciones de manifiestos).
- Protege los secretos y credenciales usando herramientas como Sealed Secrets, HashiCorp Vault o Kubernetes Secrets cifrados.
- Documenta el flujo de CI/CD y los procedimientos de rollback.
- Supervisa y audita los despliegues y cambios aplicados.

=== GitOps con ArgoCD/Flux

*GitOps* es un enfoque de gestión de infraestructura y aplicaciones donde el estado deseado del clúster se define y versiona en un repositorio Git. Herramientas como **ArgoCD** y **Flux** sincronizan automáticamente los cambios en el repositorio con el clúster Kubernetes, permitiendo despliegues automáticos, auditables y reversibles.

==== ¿Cómo funciona GitOps?

1. El estado deseado (manifiestos YAML, charts de Helm, Kustomize, etc.) se almacena en un repositorio Git.
2. ArgoCD o Flux monitorizan el repositorio y aplican los cambios detectados al clúster.
3. Si el estado real del clúster difiere del definido en Git, la herramienta lo reconcilia automáticamente.
4. Los rollbacks se realizan simplemente revirtiendo cambios en Git.

==== Ventajas de GitOps

- **Auditoría y trazabilidad total**: Todo cambio queda registrado en el historial de Git.
- **Despliegues automáticos y consistentes**: El clúster siempre refleja el estado definido en Git.
- **Rollbacks sencillos**: Basta con revertir un commit para volver a un estado anterior.
- **Separación de responsabilidades**: Los equipos de desarrollo y operaciones pueden colaborar usando flujos de trabajo Git estándar.
- **Automatización y seguridad**: Menos intervención manual y mejor control de cambios.

==== Ejemplo de flujo GitOps con ArgoCD

1. Instala ArgoCD en el clúster:
+
[source,bash]
----
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
----

2. Accede a la interfaz web de ArgoCD:
+
[source,bash]
----
kubectl port-forward svc/argocd-server -n argocd 8080:443
# Luego abre http://localhost:8080
----

3. Crea una aplicación ArgoCD que apunte a tu repositorio Git:
+
[source,yaml]
----
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: mi-app
  namespace: argocd
spec:
  project: default
  source:
    repoURL: 'https://github.com/mi-org/mi-repo-k8s'
    targetRevision: main
    path: charts/mi-app
  destination:
    server: 'https://kubernetes.default.svc'
    namespace: produccion
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
----

4. ArgoCD sincroniza automáticamente el estado del clúster con el repositorio Git.

==== Ejemplo de flujo GitOps con Flux

1. Instala Flux usando el CLI:
+
[source,bash]
----
curl -s https://fluxcd.io/install.sh | sudo bash
flux install
----

2. Conecta Flux a tu repositorio Git:
+
[source,bash]
----
flux create source git mi-repo \
  --url=https://github.com/mi-org/mi-repo-k8s \
  --branch=main
----

3. Aplica los recursos definidos en el repositorio:
+
[source,bash]
----
flux create kustomization mi-app \
  --source=mi-repo \
  --path="./k8s/mi-app" \
  --prune=true \
  --interval=1m
----

==== Buenas prácticas

.Para implementar GitOps de forma efectiva, considera las siguientes recomendaciones:
- Versiona todos los manifiestos y charts en Git.
- Usa ramas y pull requests para gestionar cambios y revisiones.
- Protege el acceso al repositorio y usa revisiones de código.
- Documenta el flujo de trabajo GitOps y los procedimientos de rollback.
- Supervisa el estado de sincronización y los logs de ArgoCD/Flux.

=== Estrategias de despliegue (blue/green, canary)

Las estrategias de despliegue avanzadas permiten minimizar riesgos y validar nuevas versiones de aplicaciones en Kubernetes de forma progresiva y controlada. Las más comunes son *blue/green* y *canary*, y pueden implementarse combinando Deployments, Services, Ingress y, en escenarios avanzados, Service Mesh.

==== Blue/Green Deployment

Consiste en tener dos entornos idénticos: uno activo (blue) y otro inactivo (green). El entorno green se actualiza con la nueva versión y, tras las pruebas, el tráfico se redirige completamente del entorno blue al green.

.Ejemplo de implementación:
[source,yaml]
----
# Deployment Blue (versión actual)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: blue
  template:
    metadata:
      labels:
        app: webapp
        version: blue
    spec:
      containers:
        - name: webapp
          image: mi-app:v1
----
[source,yaml]
----
# Deployment Green (nueva versión)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-green
spec:
  replicas: 3
  selector:
    matchLabels:
      app: webapp
      version: green
  template:
    metadata:
      labels:
        app: webapp
        version: green
    spec:
      containers:
        - name: webapp
          image: mi-app:v2
----
[source,yaml]
----
# Service apuntando a la versión activa
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
    version: blue # Cambia a 'green' para hacer el switch
  ports:
    - port: 80
      targetPort: 80
----

.Para cambiar el tráfico a la nueva versión, solo actualiza el selector del Service a `version: green`.

==== Canary Deployment

Permite liberar la nueva versión a un pequeño porcentaje de usuarios/pods y, si no hay problemas, aumentar progresivamente el tráfico.

.Ejemplo de implementación:
[source,yaml]
----
# Deployment estable (90% del tráfico)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-stable
spec:
  replicas: 9
  selector:
    matchLabels:
      app: webapp
      track: stable
  template:
    metadata:
      labels:
        app: webapp
        track: stable
    spec:
      containers:
        - name: webapp
          image: mi-app:v1
----
[source,yaml]
----
# Deployment canary (10% del tráfico)
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webapp-canary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webapp
      track: canary
  template:
    metadata:
      labels:
        app: webapp
        track: canary
    spec:
      containers:
        - name: webapp
          image: mi-app:v2
----
[source,yaml]
----
# Service balanceando entre ambas versiones
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  selector:
    app: webapp
  ports:
    - port: 80
      targetPort: 80
----

.El Service balanceará el tráfico entre los pods de ambas versiones según el número de réplicas. Puedes ajustar la proporción modificando el número de réplicas de cada Deployment.

.Para un control más granular (por ejemplo, por porcentaje real de tráfico HTTP), utiliza un Ingress Controller avanzado (NGINX, Traefik) o un Service Mesh como Istio.

==== A/B Testing

Similar al canary, pero el enrutamiento se basa en características del usuario, cabeceras HTTP, cookies, etc. Requiere un Ingress Controller avanzado o Service Mesh.

.Ejemplo de regla de Ingress para A/B Testing con NGINX:
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ab-ingress
  annotations:
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-by-header: "X-User-Type"
    nginx.ingress.kubernetes.io/canary-by-header-value: "beta"
spec:
  rules:
    - host: miapp.ejemplo.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: webapp-canary
                port:
                  number: 80
----

.Los usuarios que envíen la cabecera `X-User-Type: beta` serán dirigidos a la versión canary.

==== Buenas prácticas

.Con las estrategias de despliegue, considera las siguientes recomendaciones:
- Automatiza los despliegues y cambios de tráfico usando herramientas de CI/CD.
- Supervisa métricas y logs durante el despliegue para detectar problemas rápidamente.
- Define criterios claros de rollback y monitorea la experiencia del usuario.
- Documenta cada estrategia y los pasos para revertir cambios si es necesario.

==== Resumen gráfico

[plantuml, advanced-deploy-strategies, png]
----
@startuml
actor Usuario
cloud "Internet"
cloud "Ingress/Service Mesh"
node "Kubernetes Cluster" {
  [Pods v1] as v1
  [Pods v2] as v2
}
Usuario --> "Internet" --> "Ingress/Service Mesh"
"Ingress/Service Mesh" --> v1 : Tráfico estable/canary
"Ingress/Service Mesh" --> v2 : Tráfico canary/green
@enduml
----

=== Integración con herramientas populares

Kubernetes y sus pipelines de CI/CD se integran fácilmente con una amplia variedad de herramientas del ecosistema cloud-native, permitiendo automatizar, asegurar y optimizar todo el ciclo de vida de las aplicaciones.

==== Herramientas de integración más utilizadas

- **Helm**: Para empaquetar, versionar y desplegar aplicaciones complejas mediante charts reutilizables.
- **Kustomize**: Para gestionar overlays y personalizaciones de manifiestos YAML sin duplicar archivos.
- **Prometheus & Grafana**: Monitorización y visualización de métricas del clúster y las aplicaciones.
- **EFK/ELK (Elasticsearch, Fluentd, Kibana)**: Logging centralizado y análisis de logs.
- **Jaeger/Zipkin**: Trazabilidad distribuida para microservicios.
- **SonarQube**: Análisis de calidad y seguridad del código en pipelines de CI.
- **Trivy, Snyk, Grype**: Escaneo de vulnerabilidades en imágenes de contenedor.
- **Sealed Secrets, HashiCorp Vault**: Gestión segura de secretos y credenciales en pipelines y clústeres.
- **ArgoCD, Flux**: GitOps para despliegue continuo y sincronización automática desde Git.
- **Tekton, Argo Workflows**: Pipelines nativos y flujos de trabajo declarativos en Kubernetes.
- **Service Mesh (Istio, Linkerd)**: Gestión avanzada del tráfico, seguridad y observabilidad entre microservicios.

==== Ejemplo de integración en un pipeline de CI/CD

.Un pipeline típico puede incluir:
- Build y push de imagen Docker.
- Escaneo de vulnerabilidades con Trivy.
- Análisis de código con SonarQube.
- Despliegue con Helm o Kustomize.
- Sincronización GitOps con ArgoCD o Flux.
- Notificaciones a Slack o email en caso de éxito o fallo.

.Ejemplo de pasos en un pipeline (GitLab CI):
[source,yaml]
----
stages:
  - build
  - scan
  - test
  - deploy

build:
  stage: build
  script:
    - docker build -t registry.example.com/mi-app:$CI_COMMIT_SHA .
    - docker push registry.example.com/mi-app:$CI_COMMIT_SHA

scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image registry.example.com/mi-app:$CI_COMMIT_SHA

test:
  stage: test
  script:
    - sonar-scanner

deploy:
  stage: deploy
  script:
    - helm upgrade --install mi-app ./chart --set image.tag=$CI_COMMIT_SHA
----

==== Buenas prácticas

.Para integrar herramientas en tus pipelines de CI/CD, considera las siguientes recomendaciones:
- Integra herramientas de análisis y seguridad en todas las etapas del pipeline.
- Usa Helm o Kustomize para gestionar la configuración y el despliegue.
- Centraliza logs y métricas para facilitar el troubleshooting.
- Protege los secretos y credenciales usando soluciones especializadas.
- Automatiza notificaciones y alertas para mantener informado al equipo.


=== Automatización de pruebas y despliegues

La automatización de pruebas y despliegues es un pilar fundamental en la entrega continua de aplicaciones sobre Kubernetes. Permite validar cambios de forma temprana, reducir errores humanos y acelerar la entrega de nuevas funcionalidades a producción.

==== Fases típicas de automatización

1. **Construcción de la imagen**: El código fuente se construye y empaqueta en una imagen de contenedor.
2. **Pruebas automáticas**: Se ejecutan pruebas unitarias, de integración, de seguridad y de calidad de código.
3. **Escaneo de vulnerabilidades**: Se analiza la imagen con herramientas como Trivy o Snyk.
4. **Despliegue automatizado**: Se actualizan los manifiestos o charts de Helm y se despliega la nueva versión en el clúster.
5. **Pruebas post-despliegue**: Se validan endpoints, health checks y se ejecutan pruebas end-to-end.
6. **Rollback automático**: Si se detectan fallos, el pipeline revierte a la versión anterior.

==== Ejemplo de pipeline automatizado (GitLab CI)

[source,yaml]
----
stages:
  - build
  - test
  - scan
  - deploy
  - post-deploy

build:
  stage: build
  script:
    - docker build -t registry.example.com/mi-app:$CI_COMMIT_SHA .
    - docker push registry.example.com/mi-app:$CI_COMMIT_SHA

test:
  stage: test
  script:
    - pytest tests/

scan:
  stage: scan
  image: aquasec/trivy:latest
  script:
    - trivy image registry.example.com/mi-app:$CI_COMMIT_SHA

deploy:
  stage: deploy
  script:
    - helm upgrade --install mi-app ./chart --set image.tag=$CI_COMMIT_SHA

post-deploy:
  stage: post-deploy
  script:
    - curl --fail http://mi-app.svc.cluster.local/health
    - pytest tests_e2e/
----

==== Herramientas recomendadas

- **pytest, JUnit, Mocha**: Para pruebas unitarias e integración.
- **Trivy, Snyk, Grype**: Para escaneo de vulnerabilidades en imágenes.
- **Helm, Kustomize**: Para despliegue automatizado.
- **Argo Rollouts, Flagger**: Para despliegues progresivos y validación automática.
- **SonarQube**: Para análisis de calidad de código.
- **K6, Locust**: Para pruebas de carga y rendimiento.

==== Buenas prácticas

.Los siguientes consejos te ayudarán a automatizar pruebas y despliegues de forma efectiva:
- Automatiza todas las pruebas posibles antes y después del despliegue.
- Falla el pipeline si se detectan vulnerabilidades críticas o pruebas fallidas.
- Usa entornos efímeros (review apps) para validar cambios antes de fusionar a main.
- Versiona y documenta los scripts y manifiestos de pruebas y despliegue.
- Integra notificaciones automáticas para informar al equipo de los resultados.

== Módulo 13: Kubernetes en Producción
En este módulo aprenderás cómo desplegar, gestionar y mantener aplicaciones en un entorno de Kubernetes de producción. Se abordarán buenas prácticas, configuraciones recomendadas, ejemplos de archivos YAML y comandos útiles para la administración eficiente de clústeres.

=== Arquitecturas multi-cluster

Las arquitecturas multi-cluster son una estrategia avanzada en la administración de contenedores y orquestadores como Kubernetes. Permiten desplegar y gestionar múltiples clústeres de forma coordinada, lo que aporta ventajas en términos de alta disponibilidad, resiliencia, escalabilidad y cumplimiento de normativas.

==== ¿Qué es un clúster?

Un clúster es un conjunto de nodos (máquinas físicas o virtuales) que trabajan juntos para ejecutar aplicaciones y servicios de manera orquestada. En Kubernetes, un clúster está compuesto por un nodo maestro (control plane) y varios nodos de trabajo (workers).

==== ¿Por qué usar una arquitectura multi-cluster?

Las arquitecturas multi-cluster se utilizan para:

- Mejorar la disponibilidad y tolerancia a fallos, distribuyendo cargas entre diferentes clústeres.
- Cumplir requisitos legales o de soberanía de datos, desplegando clústeres en distintas regiones o países.
- Facilitar la migración y actualización de aplicaciones sin afectar la disponibilidad.
- Optimizar el uso de recursos y la escalabilidad horizontal.

==== Ejemplo de escenarios multi-cluster

*Alta disponibilidad geográfica*: Empresas globales pueden desplegar clústeres en diferentes regiones (por ejemplo, Europa y América) para asegurar que los usuarios accedan al clúster más cercano.

*Entornos de desarrollo y producción aislados*: Se pueden tener clústeres separados para desarrollo, pruebas y producción, evitando interferencias y mejorando la seguridad.

==== Herramientas para gestionar arquitecturas multi-cluster

- `kubefed`: Permite federar múltiples clústeres de Kubernetes, gestionando recursos de manera centralizada.
- `Rancher`: Plataforma de gestión de múltiples clústeres Kubernetes con interfaz gráfica.
- `Google Anthos`, `Azure Arc`, `Amazon EKS Anywhere`: Soluciones de nube híbrida para administrar clústeres en diferentes entornos.

==== Ejemplo de configuración multi-cluster con kubeconfig

Para interactuar con varios clústeres desde la misma máquina, se puede configurar el archivo `~/.kube/config` con múltiples contextos:

[source,yaml]
----
apiVersion: v1
kind: Config
clusters:
- name: cluster-europa
  cluster:
    server: https://europa.example.com
    certificate-authority: /path/to/ca.crt
- name: cluster-america
  cluster:
    server: https://america.example.com
    certificate-authority: /path/to/ca.crt
users:
- name: usuario-europa
  user:
    client-certificate: /path/to/europa.crt
    client-key: /path/to/europa.key
- name: usuario-america
  user:
    client-certificate: /path/to/america.crt
    client-key: /path/to/america.key
contexts:
- name: europa-context
  context:
    cluster: cluster-europa
    user: usuario-europa
- name: america-context
  context:
    cluster: cluster-america
    user: usuario-america
current-context: europa-context
----

Con este archivo, puedes cambiar de clúster fácilmente usando el comando:

[source,bash]
----
kubectl config use-context america-context
----

==== Ejemplo de despliegue federado con KubeFed

KubeFed permite desplegar recursos en varios clústeres federados. Un ejemplo de manifiesto federado sería:

[source,yaml]
----
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: nginx
  namespace: default
spec:
  template:
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx
            image: nginx:latest
----

Este recurso se replicará en todos los clústeres federados.

==== Ventajas y retos de las arquitecturas multi-cluster

.*Ventajas*:
- Alta disponibilidad y tolerancia a fallos.
- Cumplimiento de normativas y soberanía de datos.
- Escalabilidad global.

.*Retos*:
- Complejidad en la gestión y monitoreo.
- Sincronización de configuraciones y políticas.
- Seguridad y control de acceso entre clústeres.


=== Gestión de certificados y TLS

La gestión de certificados y TLS (Transport Layer Security) es fundamental para garantizar la seguridad en la comunicación entre servicios, usuarios y aplicaciones dentro de entornos de contenedores y orquestadores como Kubernetes o Docker. TLS proporciona confidencialidad, integridad y autenticación en las conexiones de red.

==== Conceptos básicos de TLS y certificados

TLS es un protocolo criptográfico que protege los datos transmitidos entre dos puntos. Para establecer una conexión segura, se utilizan certificados digitales, que son archivos que contienen una clave pública y la identidad del propietario, firmados por una Autoridad Certificadora (CA).

.Un certificado típico incluye:
- Clave pública
- Información del propietario (CN, organización, etc.)
- Firma digital de la CA
- Fecha de validez

==== Generación de certificados autofirmados

Para entornos de desarrollo o pruebas, es común generar certificados autofirmados. Esto se puede hacer con `openssl`:

[source,bash]
----
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout mi_certificado.key -out mi_certificado.crt \
  -subj "/CN=miapp.local/O=MiEmpresa"
----

Esto genera un certificado (`mi_certificado.crt`) y una clave privada (`mi_certificado.key`).

==== Uso de certificados en Docker

Docker permite proteger el daemon y las conexiones entre clientes y servidores usando TLS. Para habilitar TLS en Docker, se deben proporcionar los certificados al iniciar el daemon:

[source,bash]
----
dockerd --tlsverify \
  --tlscacert=ca.pem \
  --tlscert=server-cert.pem \
  --tlskey=server-key.pem \
  -H=0.0.0.0:2376
----

El cliente Docker también debe usar los certificados para conectarse de forma segura:

[source,bash]
----
docker --tlsverify \
  --tlscacert=ca.pem \
  --tlscert=cert.pem \
  --tlskey=key.pem \
  -H=hostname:2376 info
----

==== Gestión de certificados en Kubernetes con cert-manager

En Kubernetes, la gestión de certificados puede automatizarse usando `cert-manager`, un controlador que emite y renueva certificados automáticamente, integrándose con ACME (Let's Encrypt) u otras CAs.

Ejemplo de manifiesto para instalar cert-manager usando Helm:

[source,bash]
----
helm repo add jetstack https://charts.jetstack.io
helm repo update
helm install cert-manager jetstack/cert-manager \
  --namespace cert-manager \
  --create-namespace \
  --set installCRDs=true
----

Ejemplo de recurso `Certificate` para solicitar un certificado TLS:

[source,yaml]
----
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: mi-certificado-tls
  namespace: default
spec:
  secretName: mi-certificado-tls
  issuerRef:
    name: letsencrypt-prod
    kind: ClusterIssuer
  commonName: miapp.example.com
  dnsNames:
    - miapp.example.com
----

El certificado generado se almacena como un secreto de Kubernetes (`mi-certificado-tls`), que puede ser usado en un Ingress para habilitar HTTPS:

[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: miapp-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
spec:
  tls:
    - hosts:
        - miapp.example.com
      secretName: mi-certificado-tls
  rules:
    - host: miapp.example.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: miapp-service
                port:
                  number: 80
----

==== Renovación y revocación de certificados

La renovación automática es esencial para evitar interrupciones. `cert-manager` gestiona la renovación antes de que caduquen los certificados. Para revocar un certificado, se debe notificar a la CA y actualizar los recursos afectados.

==== Buenas prácticas en la gestión de certificados

.Las siguientes recomendaciones te ayudarán a gestionar certificados y TLS de forma segura y eficiente:
- Usar certificados de una CA confiable en producción.
- Proteger las claves privadas y restringir su acceso.
- Automatizar la emisión y renovación de certificados.
- Monitorizar la caducidad de los certificados.
- Rotar certificados y claves periódicamente.

=== Estrategias de backups y recuperación

La protección de los datos y la capacidad de restaurarlos ante fallos, errores humanos o ataques es esencial en cualquier infraestructura moderna, especialmente en entornos de contenedores y orquestadores como Docker y Kubernetes. Las estrategias de backup y recuperación permiten garantizar la continuidad del negocio y minimizar la pérdida de información.

==== Principios básicos de backup y recuperación

Un backup es una copia de los datos o configuraciones críticas que se almacena en un lugar seguro para poder restaurarla en caso de pérdida o corrupción. La recuperación (restore) es el proceso de devolver los datos a su estado original a partir de una copia de seguridad.

Las estrategias efectivas deben considerar:
- Qué datos respaldar (volúmenes, bases de datos, configuraciones, manifiestos).
- Frecuencia de los backups (diaria, semanal, incremental, diferencial).
- Dónde almacenar los backups (local, remoto, nube).
- Cómo automatizar y verificar los backups.
- Procedimientos claros de restauración y pruebas periódicas.

==== Estrategias de backup en Docker

En Docker, los datos suelen almacenarse en volúmenes. Para respaldar un volumen, se puede usar el comando `docker cp` o crear un contenedor temporal que acceda al volumen y copie los datos.

Ejemplo de backup de un volumen llamado `mi_volumen`:

[source,bash]
----
docker run --rm \
  -v mi_volumen:/datos \
  -v $(pwd):/backup \
  busybox \
  tar czf /backup/backup_mi_volumen.tar.gz -C /datos .
----

Para restaurar el backup:

[source,bash]
----
docker run --rm \
  -v mi_volumen:/datos \
  -v $(pwd):/backup \
  busybox \
  tar xzf /backup/backup_mi_volumen.tar.gz -C /datos
----

==== Estrategias de backup en Kubernetes

En Kubernetes, además de los volúmenes persistentes (PV/PVC), es importante respaldar los recursos del clúster (manifiestos, secretos, configuraciones).

Herramientas populares:
- `Velero`: Solución open source para backup y recuperación de clústeres Kubernetes.
- `Kasten K10`, `Stash`, `TrilioVault`: Otras soluciones comerciales y open source.

Ejemplo de backup de recursos del clúster con Velero:

[source,bash]
----
velero install --provider aws --bucket mi-bucket-backup --secret-file ./credentials-velero
velero backup create backup-diario --include-namespaces mi-namespace
----

Para restaurar:

[source,bash]
----
velero restore create --from-backup backup-diario
----

Ejemplo de backup de un volumen persistente con Velero:

[source,bash]
----
velero backup create backup-volumen --include-resources persistentvolumes,persistentvolumeclaims
----

==== Backup de bases de datos en contenedores

Las bases de datos requieren estrategias específicas, ya que suelen estar en uso constante. Es recomendable usar herramientas propias de cada base de datos para realizar backups consistentes.

Ejemplo de backup de una base de datos MySQL en un contenedor:

[source,bash]
----
docker exec mi-mysql-container \
  mysqldump -u usuario -pcontraseña basededatos > backup_basededatos.sql
----

Para restaurar:

[source,bash]
----
docker exec -i mi-mysql-container \
  mysql -u usuario -pcontraseña basededatos < backup_basededatos.sql
----

==== Automatización y almacenamiento seguro

Automatizar los backups es fundamental. Se pueden usar cron jobs en el host o dentro de contenedores para ejecutar los comandos de backup periódicamente.

Ejemplo de cron job en Linux para backup diario:

[source,cron]
----
0 2 * * * docker run --rm -v mi_volumen:/datos -v /backups:/backup busybox tar czf /backup/backup_$(date +\%F).tar.gz -C /datos .
----

Almacenar los backups en ubicaciones seguras y redundantes (por ejemplo, en la nube usando S3, Azure Blob o Google Cloud Storage) es una buena práctica para evitar pérdidas por fallos locales.

==== Pruebas de recuperación

No basta con hacer backups; es imprescindible probar periódicamente la restauración para asegurar que los datos pueden recuperarse correctamente y que los procedimientos funcionan.

==== Buenas prácticas

.Las siguientes recomendaciones te ayudarán a implementar estrategias de backup y recuperación efectivas:
- Documentar los procedimientos de backup y recuperación.
- Automatizar y monitorizar los procesos.
- Proteger los backups con cifrado y control de acceso.
- Realizar pruebas de restauración regularmente.
- Mantener varias copias y versiones de los backups.

=== Optimización de recursos y costes

La optimización de recursos y costes es un aspecto clave en la administración de infraestructuras basadas en contenedores y orquestadores como Docker y Kubernetes. Una gestión eficiente permite reducir gastos operativos, mejorar el rendimiento y aprovechar al máximo la capacidad disponible, evitando el desperdicio de recursos y el sobredimensionamiento.

==== Principios de optimización

Optimizar recursos implica ajustar el uso de CPU, memoria, almacenamiento y red para que las aplicaciones funcionen correctamente sin consumir más de lo necesario. Esto se traduce en menores costes de infraestructura, especialmente en entornos cloud donde se paga por uso.

Los principios básicos incluyen:
- Dimensionar correctamente los recursos asignados a cada servicio o contenedor.
- Escalar dinámicamente según la demanda.
- Automatizar la gestión de recursos.
- Monitorizar y analizar el uso real para detectar ineficiencias.

==== Optimización en Docker

En Docker, se pueden limitar los recursos que consume cada contenedor usando opciones al ejecutar los contenedores:

[source,bash]
----
docker run --name miapp \
  --memory="512m" \
  --cpus="1.0" \
  miimagen:latest
----

Esto limita el contenedor a 512 MB de RAM y 1 CPU. Así se evita que un contenedor consuma todos los recursos del host.

Para ver el uso de recursos de los contenedores en tiempo real:

[source,bash]
----
docker stats
----

==== Optimización en Kubernetes

Kubernetes ofrece mecanismos avanzados para gestionar y optimizar recursos a nivel de clúster.

===== Requests y Limits

.En los manifiestos de pods y deployments, se pueden definir requests (recursos mínimos garantizados) y limits (máximos permitidos):

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: miapp
spec:
  containers:
    - name: miapp
      image: miimagen:latest
      resources:
        requests:
          memory: "256Mi"
          cpu: "250m"
        limits:
          memory: "512Mi"
          cpu: "500m"
----

Esto ayuda a que el scheduler de Kubernetes distribuya los pods de manera eficiente y evita el sobreconsumo.

===== Autoscaling

.Kubernetes permite escalar automáticamente los pods según la carga, usando el Horizontal Pod Autoscaler (HPA):
[source,bash]
----
kubectl autoscale deployment miapp --cpu-percent=50 --min=2 --max=10
----

Esto ajusta el número de réplicas de acuerdo al uso de CPU.

===== Node Autoscaling

En entornos cloud, se puede habilitar el escalado automático de nodos (Cluster Autoscaler), que añade o elimina nodos según la demanda, optimizando costes.

===== Monitorización y análisis

Herramientas como Prometheus, Grafana y Kubernetes Metrics Server permiten monitorizar el uso de recursos y detectar cuellos de botella o recursos infrautilizados.

.Ejemplo de consulta PromQL para ver el uso de CPU por pod:
[source,promql]
----
sum(rate(container_cpu_usage_seconds_total{image!=""}[5m])) by (pod)
----

==== Optimización de almacenamiento

.Algunas recomendaciones para optimizar el uso de almacenamiento en Kubernetes son:
- Usar volúmenes persistentes adecuados al perfil de uso (rápidos para bases de datos, económicos para backups).
- Eliminar volúmenes y datos huérfanos.
- Comprimir y deduplicar datos cuando sea posible.

==== Optimización de red

.Los siguientes consejos te ayudarán a optimizar el uso de red en Kubernetes:
- Configurar políticas de red para evitar tráfico innecesario.
- Usar servicios internos (ClusterIP) en lugar de exponer servicios externos si no es necesario.
- Monitorizar el tráfico para detectar anomalías o sobrecostes.

==== Optimización de costes en la nube

.Para optimizar costes en entornos cloud, considera:
- Elegir instancias adecuadas al perfil de carga.
- Usar instancias reservadas o spot/preemptibles para cargas no críticas.
- Apagar entornos de desarrollo fuera del horario laboral.
- Automatizar el escalado y la eliminación de recursos no utilizados.

==== Buenas prácticas

.Para optimizar recursos y costes, considera las siguientes recomendaciones:
- Revisar periódicamente el uso de recursos y ajustar configuraciones.
- Automatizar el escalado y la limpieza de recursos.
- Monitorizar y alertar sobre consumos anómalos.
- Fomentar la cultura de eficiencia entre los equipos de desarrollo y operaciones.

=== Troubleshooting y resolución de problemas

El troubleshooting o resolución de problemas es una habilidad esencial para mantener la estabilidad y disponibilidad de aplicaciones y servicios en entornos de contenedores como Docker y Kubernetes. Consiste en identificar, diagnosticar y solucionar incidencias que afectan al funcionamiento esperado de los sistemas.

==== Principios del troubleshooting

.Un enfoque sistemático ayuda a resolver problemas de manera eficiente:
- Recopilar información relevante (logs, métricas, eventos).
- Reproducir el problema si es posible.
- Aislar el alcance (afecta a un contenedor, pod, nodo, red, almacenamiento, etc.).
- Formular hipótesis y probar soluciones.
- Documentar el proceso y la solución aplicada.

==== Troubleshooting en Docker

===== Inspección de contenedores

.Para obtener información detallada sobre un contenedor:
[source,bash]
----
docker inspect mi_contenedor
----

===== Visualización de logs

.Para ver los logs de un contenedor:
[source,bash]
----
docker logs mi_contenedor
----

Agregar `-f` para seguir los logs en tiempo real.

===== Estado de los contenedores

.Para ver el estado de todos los contenedores:
[source,bash]
----
docker ps -a
----

===== Acceso a un contenedor en ejecución

.Para abrir una shell dentro de un contenedor:
[source,bash]
----
docker exec -it mi_contenedor /bin/sh
----

Esto permite inspeccionar archivos, procesos y configuraciones desde dentro.

===== Ejemplo de troubleshooting

.Si un contenedor se detiene inesperadamente, puedes ver el motivo con:
[source,bash]
----
docker inspect --format='{{.State.ExitCode}}' mi_contenedor
docker logs mi_contenedor
----

==== Troubleshooting en Kubernetes

===== Estado de los pods

.Para ver el estado de los pods en un namespace:
[source,bash]
----
kubectl get pods -n mi-namespace
----

.Para obtener detalles de un pod específico:
[source,bash]
----
kubectl describe pod mi-pod -n mi-namespace
----

Esto muestra eventos, razones de fallos y condiciones del pod.

===== Logs de pods

.Para ver los logs de un pod:
[source,bash]
----
kubectl logs mi-pod -n mi-namespace
----

.Si el pod tiene varios contenedores:
[source,bash]
----
kubectl logs mi-pod -c nombre-contenedor -n mi-namespace
----

===== Acceso interactivo a un pod

.Para abrir una shell dentro de un contenedor en un pod:
[source,bash]
----
kubectl exec -it mi-pod -- /bin/sh
----

===== Eventos del clúster

.Para ver eventos recientes que pueden indicar problemas:
[source,bash]
----
kubectl get events -n mi-namespace --sort-by=.metadata.creationTimestamp
----

===== Ejemplo de troubleshooting

.Si un pod está en estado `CrashLoopBackOff`, puedes investigar con:
[source,bash]
----
kubectl describe pod mi-pod
kubectl logs mi-pod
----

Busca errores de configuración, variables de entorno faltantes o problemas de conectividad.

==== Troubleshooting de red

.Para problemas de red en Kubernetes:
- Verifica la conectividad entre pods usando herramientas como `ping` o `curl` desde dentro de los contenedores.
- Revisa las políticas de red (`NetworkPolicy`) que puedan estar bloqueando el tráfico.
- Usa utilidades como `kubectl port-forward` para exponer servicios temporalmente y probar accesos.

==== Troubleshooting de almacenamiento

.Para problemas de almacenamiento:
- Verifica el estado de los volúmenes persistentes (PV/PVC) con `kubectl get pv,pvc`.
- Revisa los eventos y logs relacionados con el almacenamiento.
- Asegúrate de que los permisos y rutas de montaje sean correctos.

==== Herramientas útiles

.Para facilitar el troubleshooting, considera usar herramientas como:
- `docker-compose logs` para aplicaciones multi-contenedor.
- `stern` o `kubetail` para ver logs de múltiples pods en Kubernetes.
- `kubectl top` para monitorizar recursos en tiempo real.
- Dashboards como Grafana y Kibana para análisis de métricas y logs.

==== Buenas prácticas

.Las siguientes recomendaciones te ayudarán a mejorar el proceso de troubleshooting:
- Automatizar la recolección de logs y métricas.
- Documentar los problemas y soluciones encontradas.
- Mantener procedimientos de troubleshooting actualizados.
- Capacitar al equipo en el uso de herramientas de diagnóstico.


== Módulo 14: Extensiones y Ecosistema

El ecosistema de contenedores y orquestadores como Docker y Kubernetes se ha expandido enormemente gracias a la comunidad y a la disponibilidad de extensiones, plugins y herramientas complementarias. Estas extensiones permiten ampliar funcionalidades, integrar nuevas capacidades y facilitar la administración, seguridad, monitorización y automatización de los entornos de contenedores.

=== Custom Resource Definitions (CRDs)

Las Custom Resource Definitions (CRDs) son una de las principales formas de extender la funcionalidad nativa de Kubernetes, permitiendo a los usuarios definir y gestionar recursos personalizados que se comportan como los recursos estándar del clúster (Pods, Services, Deployments, etc.).

==== ¿Qué es una CRD?

Una CRD es un recurso de Kubernetes que permite registrar un nuevo tipo de objeto en la API del clúster. Una vez creada, los usuarios pueden crear, leer, actualizar y eliminar instancias de ese recurso personalizado usando las mismas herramientas y flujos que para los recursos nativos.

Esto habilita la creación de operadores y controladores que gestionan el ciclo de vida de aplicaciones complejas o integraciones específicas.

==== Ejemplo básico de CRD

.A continuación, se muestra un ejemplo de definición de una CRD llamada `CronTab`, que permite crear recursos personalizados para programar tareas:

[source,yaml]
----
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: crontabs.stable.example.com
spec:
  group: stable.example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cronSpec:
                  type: string
                image:
                  type: string
                replicas:
                  type: integer
  scope: Namespaced
  names:
    plural: crontabs
    singular: crontab
    kind: CronTab
    shortNames:
      - ct
----

.Una vez aplicada esta CRD, puedes crear recursos del tipo `CronTab`:

[source,yaml]
----
apiVersion: stable.example.com/v1
kind: CronTab
metadata:
  name: ejemplo-crontab
spec:
  cronSpec: "* * * * */5"
  image: my-cron-image
  replicas: 1
----

==== Ventajas de usar CRDs

.Las CRDs ofrecen varias ventajas:
- Permiten modelar recursos y flujos de trabajo específicos de la organización.
- Facilitan la automatización avanzada mediante operadores.
- Integran recursos personalizados en el ciclo de vida y la API de Kubernetes.

==== Operadores y CRDs

Los operadores son controladores que gestionan recursos personalizados definidos por CRDs. Por ejemplo, un operador puede observar instancias de `CronTab` y crear o eliminar pods según la programación definida.

Herramientas como Operator SDK y Kubebuilder facilitan la creación de operadores y CRDs.

==== Gestión y actualización de CRDs

Las CRDs pueden evolucionar con nuevas versiones de la API. Es importante definir correctamente los esquemas y validar los datos para evitar inconsistencias.

.Para listar todas las CRDs instaladas en un clúster:

[source,bash]
----
kubectl get crds
----

.Para eliminar una CRD (y todos sus recursos asociados):

[source,bash]
----
kubectl delete crd crontabs.stable.example.com
----

==== Buenas prácticas

.Al trabajar con CRDs, considera las siguientes recomendaciones:
- Definir esquemas claros y validar los datos con OpenAPI v3.
- Versionar las CRDs para facilitar la evolución y compatibilidad.
- Documentar el uso y los campos de los recursos personalizados.
- Proteger el acceso a los recursos personalizados mediante RBAC.

=== API Aggregation

La API Aggregation es una característica avanzada de Kubernetes que permite extender la API nativa del clúster integrando APIs externas como si fueran parte del propio servidor de Kubernetes. Esto facilita la incorporación de nuevos servicios y recursos personalizados sin modificar el código fuente del API server principal.

==== ¿Qué es API Aggregation?

API Aggregation consiste en desplegar un servidor de API externo (llamado Aggregated API Server) que implementa nuevos recursos o funcionalidades. Este servidor se registra en el API server principal de Kubernetes, que actúa como proxy y enruta las solicitudes a la API agregada. Así, los usuarios pueden interactuar con los nuevos recursos usando las mismas herramientas (`kubectl`, API REST) y mecanismos de autenticación/autorización que para los recursos nativos.

==== Diferencias entre CRDs y API Aggregation

- **CRDs**: Permiten definir nuevos tipos de recursos directamente en el API server principal, gestionados por el propio clúster.
- **API Aggregation**: Permite añadir APIs completas y lógicas de negocio externas, útiles cuando se requiere lógica de validación, mutación o procesamiento avanzado que no es posible solo con CRDs.

==== Arquitectura de API Aggregation

[plantuml, api-aggregation, svg]
----
@startuml
actor User
User -> "Kubernetes API Server": Solicitud para recurso extendido
"Kubernetes API Server" -> "Aggregated API Server": Proxy de la solicitud
"Aggregated API Server" --> "Kubernetes API Server": Respuesta
"Kubernetes API Server" --> User: Respuesta final
@enduml
----

==== Ejemplo de uso: Metrics Server

Un ejemplo común de API Aggregation es el Metrics Server, que expone métricas de recursos (CPU, memoria) a través de la API `/apis/metrics.k8s.io/v1beta1`. El Metrics Server se registra como API agregada y Kubernetes enruta las solicitudes a este servicio.

Para ver las APIs agregadas en tu clúster:

[source,bash]
----
kubectl get apiservices
----

Esto mostrará recursos como `v1beta1.metrics.k8s.io` gestionados por servidores externos.

==== Ejemplo de manifiesto de APIService

Para registrar una API agregada, se crea un recurso `APIService`:

[source,yaml]
----
apiVersion: apiregistration.k8s.io/v1
kind: APIService
metadata:
  name: v1alpha1.ejemplo.miempresa.com
spec:
  service:
    name: mi-api-aggregated
    namespace: default
  group: ejemplo.miempresa.com
  version: v1alpha1
  insecureSkipTLSVerify: true
  groupPriorityMinimum: 1000
  versionPriority: 15
----

Esto indica al API server que enrute las solicitudes para el grupo y versión especificados al servicio `mi-api-aggregated` en el namespace `default`.

==== Ventajas y casos de uso

.API Aggregation ofrece varias ventajas:
- Permite implementar lógica avanzada de validación, mutación y procesamiento.
- Facilita la integración de servicios externos o APIs de terceros.
- Útil para exponer recursos complejos o APIs que requieren control total sobre el backend.

==== Consideraciones de seguridad

.Al implementar API Aggregation, es importante considerar la seguridad:
- Es recomendable usar TLS y autenticación mutua entre el API server y los servidores agregados.
- Controlar el acceso a las APIs agregadas mediante RBAC.

==== Buenas prácticas

.Las siguientes recomendaciones te ayudarán a implementar API Aggregation de forma efectiva:
- Documentar claramente los recursos y endpoints expuestos por la API agregada.
- Monitorizar el rendimiento y la disponibilidad del servidor de API agregado.
- Versionar las APIs agregadas para facilitar la evolución y compatibilidad.

=== Servicios en la nube basados en Kubernetes

El auge de Kubernetes como estándar de orquestación de contenedores ha impulsado a los principales proveedores de nube a ofrecer servicios gestionados que simplifican el despliegue, administración y escalado de clústeres. Estos servicios permiten a las organizaciones centrarse en el desarrollo y operación de aplicaciones, delegando la gestión de la infraestructura subyacente al proveedor.

==== ¿Qué es un servicio gestionado de Kubernetes?

Un servicio gestionado de Kubernetes es una oferta en la nube donde el proveedor se encarga de instalar, actualizar, escalar y mantener el plano de control (control plane) y, en muchos casos, los nodos de trabajo (worker nodes). El usuario puede crear y administrar clústeres de Kubernetes sin preocuparse por la complejidad de la instalación, la alta disponibilidad o los parches de seguridad.

==== Principales servicios en la nube

===== Google Kubernetes Engine (GKE)

- Proveedor: Google Cloud Platform
- Características: 
  - Gestión automática del control plane y nodos.
  - Integración con servicios de Google (IAM, Stackdriver, Cloud Build).
  - Escalado automático de nodos y pods.
  - Actualizaciones automáticas y opciones de seguridad avanzadas.

.Ejemplo de creación de clúster con `gcloud`:

[source,bash]
----
gcloud container clusters create mi-cluster \
  --zone us-central1-a \
  --num-nodes 3
----

===== Amazon Elastic Kubernetes Service (EKS)

- Proveedor: Amazon Web Services
- Características:
  - Control plane gestionado y escalable.
  - Integración con IAM, CloudWatch, ALB, ECR.
  - Soporte para nodos EC2, Fargate (serverless) y nodos autogestionados.
  - Seguridad y cumplimiento con políticas de AWS.

.Ejemplo de creación de clúster con `eksctl`:

[source,bash]
----
eksctl create cluster --name mi-cluster --region us-west-2 --nodes 3
----

===== Azure Kubernetes Service (AKS)

- Proveedor: Microsoft Azure
- Características:
  - Control plane gratuito y gestionado.
  - Integración con Azure Active Directory, Monitor, DevOps.
  - Escalado automático, actualizaciones y soporte para nodos spot.
  - Seguridad reforzada y opciones de red avanzadas.

.Ejemplo de creación de clúster con `az`:

[source,bash]
----
az aks create --resource-group mi-grupo --name mi-cluster --node-count 3 --generate-ssh-keys
----

==== Ventajas de los servicios gestionados

.Los servicios gestionados de Kubernetes ofrecen varias ventajas:
- **Simplicidad**: Eliminan la necesidad de instalar y mantener manualmente Kubernetes.
- **Escalabilidad**: Permiten escalar nodos y recursos de forma automática y bajo demanda.
- **Seguridad**: Aplican parches y actualizaciones de seguridad de manera proactiva.
- **Integración**: Se integran con otros servicios de la nube (almacenamiento, redes, monitorización, CI/CD).
- **Alta disponibilidad**: Ofrecen opciones de despliegue multi-zona y recuperación ante fallos.

==== Consideraciones y buenas prácticas

.Al usar servicios gestionados de Kubernetes, es importante considerar:
- **Costes**: Analizar el modelo de precios, ya que se paga por los recursos consumidos y, en algunos casos, por el control plane.
- **Seguridad**: Configurar roles y políticas de acceso (RBAC, IAM) y proteger los endpoints de la API.
- **Backup y recuperación**: Utilizar herramientas compatibles (Velero, snapshots de volúmenes) para proteger los datos y la configuración.
- **Monitorización**: Integrar con las soluciones de logging y métricas del proveedor para detectar incidencias y optimizar recursos.
- **Actualizaciones**: Planificar las actualizaciones de versión y probar la compatibilidad de las aplicaciones.

==== Ejemplo de archivo de configuración kubeconfig

.Los servicios gestionados proporcionan un archivo `kubeconfig` para interactuar con el clúster desde `kubectl`:

[source,yaml]
----
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: <CA_DATA>
    server: https://<CLUSTER_ENDPOINT>
  name: cloud-cluster
contexts:
- context:
    cluster: cloud-cluster
    user: cloud-user
  name: cloud-context
current-context: cloud-context
users:
- name: cloud-user
  user:
    token: <ACCESS_TOKEN>
----

==== Otros servicios y ecosistema

El ecosistema de Kubernetes en la nube es amplio y diverso, con múltiples proveedores y herramientas que complementan y amplían las capacidades de los servicios gestionados. Además de los principales servicios de Google, Amazon y Azure, existen otras opciones relevantes y soluciones especializadas que pueden adaptarse a diferentes necesidades empresariales y técnicas.

===== IBM Cloud Kubernetes Service (IKS)

- Proveedor: IBM Cloud
- Características:
  - Control plane gestionado y nodos desplegados en la infraestructura de IBM.
  - Integración con servicios de IA, Watson, y herramientas de seguridad de IBM.
  - Opciones de despliegue multizona y soporte para bare metal.
  - Integración con Red Hat OpenShift y soporte empresarial.

===== Oracle Container Engine for Kubernetes (OKE)

- Proveedor: Oracle Cloud Infrastructure (OCI)
- Características:
  - Control plane gratuito y gestionado.
  - Integración con servicios de base de datos y almacenamiento de Oracle.
  - Opciones de nodos flexibles y soporte para redes avanzadas.
  - Seguridad reforzada y cumplimiento normativo.

===== DigitalOcean Kubernetes (DOKS)

- Proveedor: DigitalOcean
- Características:
  - Sencillez y rapidez en la creación de clústeres.
  - Facturación simple y transparente.
  - Integración con block storage, load balancers y espacios de DigitalOcean.
  - Ideal para startups, proyectos pequeños y entornos de desarrollo.

===== Alibaba Cloud Container Service for Kubernetes (ACK)

- Proveedor: Alibaba Cloud
- Características:
  - Integración con el ecosistema de Alibaba (bases de datos, almacenamiento, red).
  - Opciones de escalado automático y despliegue multirregión.
  - Soporte para workloads híbridos y edge computing.
  - Seguridad y cumplimiento para el mercado asiático.

===== Red Hat OpenShift (ROSA, ARO, OpenShift Dedicated)

- Proveedor: Red Hat (disponible como servicio gestionado en AWS, Azure y en la nube de IBM)
- Características:
  - Basado en Kubernetes, pero con herramientas adicionales para desarrolladores y operaciones.
  - Gestión avanzada de ciclo de vida, CI/CD integrado, y catálogo de aplicaciones.
  - Seguridad reforzada, soporte empresarial y cumplimiento.
  - Opciones de despliegue híbrido y multicloud.

===== Herramientas y servicios complementarios

- **Rancher**: Plataforma de gestión multi-cluster y multi-cloud para Kubernetes, con interfaz gráfica y control centralizado.
- **Portainer**: Interfaz web para la gestión de clústeres Docker y Kubernetes.
- **Lens**: IDE para Kubernetes que facilita la visualización y administración de recursos.
- **Velero**: Solución de backup y recuperación de clústeres Kubernetes, compatible con la mayoría de servicios gestionados.
- **ArgoCD y Flux**: Herramientas de GitOps para despliegue continuo y gestión declarativa de aplicaciones en Kubernetes.
- **Prometheus y Grafana**: Monitorización y visualización de métricas en clústeres Kubernetes.
- **Istio, Linkerd**: Service Mesh para gestión avanzada de tráfico, seguridad y observabilidad.

===== Consideraciones para elegir un servicio

- **Ubicación geográfica y cumplimiento**: Elegir proveedores que ofrezcan regiones y certificaciones adecuadas a los requisitos legales y de negocio.
- **Integración con otros servicios**: Analizar la compatibilidad con bases de datos, almacenamiento, redes y herramientas de CI/CD.
- **Soporte y comunidad**: Valorar el soporte técnico, la documentación y la comunidad activa alrededor del servicio.
- **Coste y escalabilidad**: Comparar modelos de precios, opciones de escalado y flexibilidad de recursos.

=== Integraciones con proveedores cloud (AWS, GCP, Azure)

La integración de Kubernetes con los principales proveedores cloud —Amazon Web Services (AWS), Google Cloud Platform (GCP) y Microsoft Azure— permite aprovechar servicios nativos de cada plataforma, automatizar operaciones y mejorar la seguridad, el almacenamiento y la conectividad de los clústeres. Estas integraciones son clave para sacar el máximo partido a los servicios gestionados y a la infraestructura cloud.

==== Integración con AWS

AWS ofrece Amazon Elastic Kubernetes Service (EKS), pero además existen integraciones profundas con otros servicios:

- **Almacenamiento**: Uso de volúmenes EBS como Persistent Volumes (PV) mediante el EBS CSI Driver.
- **Balanceadores de carga**: Integración automática con Elastic Load Balancer (ELB) para exponer servicios de Kubernetes.
- **IAM Roles for Service Accounts (IRSA)**: Permite que los pods asuman roles de IAM para acceder de forma segura a servicios de AWS (S3, DynamoDB, etc.).
- **Autoscaling**: Cluster Autoscaler y soporte para nodos EC2 y Fargate.
- **Monitorización y logs**: Integración con CloudWatch para métricas y logs de aplicaciones y clúster.

Ejemplo de StorageClass para EBS:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: ebs-sc
provisioner: ebs.csi.aws.com
parameters:
  type: gp3
----

==== Integración con GCP

Google Cloud Platform proporciona Google Kubernetes Engine (GKE) y una integración nativa con sus servicios:

- **Almacenamiento**: Uso de Persistent Disks como volúmenes persistentes.
- **Balanceadores de carga**: Integración con Google Cloud Load Balancer para exponer servicios.
- **IAM y Workload Identity**: Permite a los pods autenticarse con servicios de GCP usando identidades de Google.
- **Autoscaling**: Node Autoscaler y Horizontal Pod Autoscaler gestionados.
- **Monitorización y logs**: Stackdriver (ahora Cloud Operations) para métricas, logs y alertas.

Ejemplo de StorageClass para Persistent Disk:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: pd-standard
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
----

==== Integración con Azure

Azure Kubernetes Service (AKS) facilita la integración con servicios de Azure:

- **Almacenamiento**: Uso de Azure Disks y Azure Files como volúmenes persistentes.
- **Balanceadores de carga**: Integración con Azure Load Balancer y Application Gateway.
- **Managed Identities**: Permite a los pods acceder de forma segura a servicios de Azure (Blob Storage, Key Vault, etc.).
- **Autoscaling**: Cluster Autoscaler y Virtual Node para escalar con Azure Container Instances.
- **Monitorización y logs**: Azure Monitor y Log Analytics para observabilidad y alertas.

Ejemplo de StorageClass para Azure Disk:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: azure-disk
provisioner: disk.csi.azure.com
parameters:
  skuName: Standard_LRS
----

==== Buenas prácticas de integración

- Utilizar controladores CSI (Container Storage Interface) oficiales para almacenamiento.
- Configurar roles y permisos mínimos necesarios para los pods (principio de menor privilegio).
- Aprovechar las herramientas de monitorización y logging nativas del proveedor.
- Automatizar la provisión de recursos cloud mediante controladores y operadores.
- Mantener actualizados los controladores y plugins de integración.

=== Herramientas complementarias

El ecosistema de Kubernetes y Docker se enriquece con una amplia variedad de herramientas complementarias que facilitan la gestión, automatización, monitorización, seguridad y despliegue de aplicaciones en entornos de contenedores. Estas herramientas ayudan a cubrir necesidades específicas y a mejorar la eficiencia operativa.

==== Monitorización y observabilidad

- **Prometheus**: Sistema de monitorización y alerta ampliamente adoptado en Kubernetes. Permite recolectar métricas de clústeres, nodos, pods y aplicaciones.
- **Grafana**: Plataforma de visualización de métricas que se integra con Prometheus y otras fuentes de datos para crear dashboards interactivos.
- **Kube-state-metrics**: Exporta métricas sobre el estado de los recursos de Kubernetes, útil para monitorización avanzada.
- **ELK Stack (Elasticsearch, Logstash, Kibana)**: Solución para la gestión y visualización de logs de aplicaciones y clústeres.
- **Loki**: Sistema de logs diseñado para integrarse con Grafana y Kubernetes.

==== Seguridad y cumplimiento

- **Kube-bench**: Evalúa la seguridad del clúster según las recomendaciones de CIS Kubernetes Benchmark.
- **Kube-hunter**: Herramienta de auditoría que detecta vulnerabilidades en clústeres Kubernetes.
- **Trivy**: Escáner de vulnerabilidades para imágenes de contenedor y recursos de Kubernetes.
- **OPA (Open Policy Agent)**: Motor de políticas para controlar el acceso y la configuración de recursos en Kubernetes.
- **Kyverno**: Herramienta de políticas nativa de Kubernetes para validar, mutar y generar recursos.

==== Gestión y automatización

- **Helm**: Gestor de paquetes para Kubernetes que facilita el despliegue y la actualización de aplicaciones mediante charts.
- **Kustomize**: Permite personalizar manifiestos de Kubernetes sin duplicar archivos.
- **ArgoCD** y **Flux**: Herramientas de GitOps para despliegue continuo y gestión declarativa de aplicaciones.
- **Velero**: Solución para backup y recuperación de clústeres y volúmenes persistentes.
- **Rancher**: Plataforma de gestión multi-cluster y multi-cloud con interfaz gráfica.

==== Service Mesh y networking

- **Istio**: Service Mesh que proporciona control avanzado de tráfico, seguridad, observabilidad y gestión de servicios.
- **Linkerd**: Service Mesh ligero y fácil de instalar, enfocado en la simplicidad y el rendimiento.
- **Cilium**: Solución de networking y seguridad basada en eBPF para Kubernetes.
- **MetalLB**: Proporciona soporte de LoadBalancer en clústeres on-premises.

==== Desarrollo y productividad

- **Skaffold**: Automatiza el ciclo de desarrollo local a clúster, facilitando el build, push y deploy de aplicaciones.
- **Tilt**: Herramienta para desarrollo local de aplicaciones en Kubernetes con feedback rápido.
- **Telepresence**: Permite el desarrollo y depuración remota de servicios en clústeres Kubernetes desde el entorno local.
- **Lens**: IDE gráfico para la administración y visualización de clústeres Kubernetes.
- **Portainer**: Interfaz web para la gestión de contenedores Docker y clústeres Kubernetes.

==== Ejemplo de integración: Helm

Desplegar una aplicación con Helm:

[source,bash]
----
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install mi-mysql bitnami/mysql
----

==== Buenas prácticas

- Seleccionar herramientas que se integren bien con el stack tecnológico y los flujos de trabajo existentes.
- Mantener las herramientas actualizadas y monitorizar su seguridad.
- Automatizar tareas repetitivas y documentar los procesos.
- Evaluar el impacto de cada herramienta en la seguridad y el rendimiento del clúster.

== Recursos Adicionales

=== Documentación oficial y CNCF

- https://kubernetes.io/es/docs/[Documentación oficial de Kubernetes]: La fuente principal y más actualizada sobre todos los conceptos, recursos y comandos de Kubernetes.
- https://github.com/kubernetes/kubernetes[Repositorio oficial de Kubernetes en GitHub]: Para explorar el código fuente, reportar issues y contribuir.
- https://www.cncf.io/[Cloud Native Computing Foundation (CNCF)]: Organización que impulsa Kubernetes y otros proyectos cloud-native. Ofrece recursos, webinars y eventos.

=== Certificaciones disponibles (CKA, CKAD, CKS)

- *Certified Kubernetes Administrator (CKA)*: Certificación orientada a la administración y operación de clústeres Kubernetes.
- *Certified Kubernetes Application Developer (CKAD)*: Certificación enfocada en el desarrollo y despliegue de aplicaciones sobre Kubernetes.
- *Certified Kubernetes Security Specialist (CKS)*: Certificación avanzada sobre seguridad en entornos Kubernetes.
- Más información y registro: https://training.linuxfoundation.org/certification/

=== Comunidad y eventos

- https://discuss.kubernetes.io/[Foro oficial de Kubernetes]: Espacio para resolver dudas y compartir experiencias.
- https://slack.k8s.io/[Slack de Kubernetes]: Comunidad activa con canales temáticos.
- https://www.meetup.com/topics/kubernetes/[Meetups de Kubernetes]: Encuentra eventos y grupos locales para networking y aprendizaje.
- https://www.cncf.io/events/[Eventos CNCF]: KubeCon, CloudNativeCon y otros eventos internacionales.

=== Recursos de aprendizaje continuo

- https://katacoda.com/courses/kubernetes[Katacoda]: Laboratorios interactivos gratuitos para practicar Kubernetes.
- https://play.instruqt.com/public/topics/kubernetes[Instruqt]: Escenarios prácticos y retos de Kubernetes.
- https://www.udemy.com/topic/kubernetes/[Cursos en Udemy]: Amplia oferta de cursos en español e inglés.
- https://www.edx.org/learn/kubernetes[edX]: Cursos oficiales de la CNCF y Linux Foundation.
- https://www.youtube.com/c/KubernetesCommunity[YouTube Kubernetes Community]: Charlas, tutoriales y grabaciones de eventos.

=== Tendencias futuras en el ecosistema de Kubernetes

- *Serverless sobre Kubernetes*: Uso de frameworks como Knative para ejecutar funciones sin gestionar la infraestructura.
- *GitOps*: Automatización de despliegues y gestión de clústeres basada en Git (ArgoCD, Flux).
- *Service Mesh*: Adopción creciente de Istio, Linkerd y otros para observabilidad, seguridad y control de tráfico.
- *Edge Computing*: Kubernetes ligero (K3s, MicroK8s) para IoT y entornos distribuidos.
- *Inteligencia Artificial y ML Ops*: Integración de flujos de trabajo de machine learning sobre Kubernetes.
- *Seguridad avanzada*: Políticas de seguridad, escaneo de imágenes y Zero Trust en el ciclo de vida de aplicaciones.
